[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EODH Training Materials",
    "section": "",
    "text": "Welcome\nThis site is in heavy development.\nWelcome to the eodh-training repository! This repository aims to provide a live set of documents to demonstrate how to use the EODH (Earth Observation Data Hub) and associated tools such as the pyeodh Python API client, workflow generator and QGIS plugin. Other training materials may be added to this repository in future.\nWhether you’re a user looking to explore the project or a developer wanting to contribute, you’ll find all the information you need here.\n\n\nContent\nYou’ll be able to find the content you require by navidating through this website using the sidebar to the left.\n\n\nAdditional Support\nWe have set up an accessible Discussion Group as a location that users can interact with other users, as well as the Hub owners and developers. Please use this resource to build a vibrant community around the EODH platform. It is also accessible via the GitHub icon at the top of every page.\nIf you require specific development information on the API client then please check out the ReadTheDocs page for the client.\n\nThis website is made using Quarto.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "workflows/1_workflows.html",
    "href": "workflows/1_workflows.html",
    "title": "What is a Workflow",
    "section": "",
    "text": "What is the Workflow Runner?\n\n\nWhat is CWL?\n\n\nWhat is the Workflow generator?",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "title": "Demonstration for DEFRA",
    "section": "a) Viewing images with specific band configurations",
    "text": "a) Viewing images with specific band configurations\nOnce you have the URL to the Cloud Optimised Geotiff from the STAC records held in the resource catalogue it is possible to view different band combinations. The following code uses the URL to the Sentinel 2 ARD image discovered in the Data Discovery notebook and displays an interactive false colour composite.\n\n# Path to raster (URL or local path)\ndata_url = 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif'\n\n# Check that the dataset is a valid COG. If invalid, returns False\nvalidate_cog(data_url)\n\nTrue\n\n\n\n\n# First, create TileClient using example file\ndclient = TileClient(data_url)\n\n# Create 2 tile layers from same raster viewing different bands\nl = get_leaflet_tile_layer(dclient, indexes=[6, 2, 1])\n\n# Make the ipyleaflet map\nm = Map(center=dclient.center(), zoom=dclient.default_zoom)\nm.add(l)\nm.add_control(ScaleControl(position='bottomleft'))\nm.add_control(FullScreenControl())\nm\n\n\n\n\nimage.png\n\n\n\n# we can use the same tool to view rendered data stored locally e.g. in a user's workspace.\nmap = TileClient('data/S2A_clip_rend.tif')\nmap\n\n\n\n\nimage.png",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#b-towards-a-data-cube",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#b-towards-a-data-cube",
    "title": "Demonstration for DEFRA",
    "section": "b) Towards a data cube",
    "text": "b) Towards a data cube\nThis is very much work in progress, and follows the outline of the tutorial supplied here. The tutorial at that link is designed to create a data cube in the USA using data held in an Element 84 STAC catalogue. Here we alter the processing to make use of the CEDA STAC catalogue. Ultimately both of these datasets will be held within the EODH resource catalogue and this code will be rewritten to utilise pyeodh and the EODH data holdings.\n\n# Set up\n# A helper method for changing bounding box representation to leaflet notation\n# (lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))\n\ndef convert_bounds(bbox, invert_y=False):\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\nThe next thing the user needs to do is find some data in a STAC catalogue that intersects with the site of interest for a given time period. Therefore, we need to know what datasets are available, over what period and for what locations.\n\n# Find some data using STAC\n# note: here we are using `pystac`. This will be replaced by `pyeodh` in future. \n\nurl = \"https://api.stac.ceda.ac.uk/\"\n\nclient = Client.open(url)\nfor coll in client.get_collections():\n    print(f\"{coll.id}: {coll.description}\")\n\nsentinel2_ard = client.get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\n# check the spatial and temporal extent of the collection\nprint('')\nprint(\"spatial extent:\", sentinel2_ard.extent.spatial.bboxes)\nprint(\"Temporal range:\", [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\ncmip6: CMIP6\ncordex: CORDEX\nland_cover: land_cover\nsentinel1: Sentinel 1\nsentinel2_ard: sentinel 2 ARD\nsst-cdrv3-collection: collection of EOCIS SST CDR V3\nukcp: UKCP\n\nspatial extent: [[-9.00034454651177, 49.48562028352171, 3.1494256015866995, 61.33444247301668]]\nTemporal range: ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# Find imagery that intersects with the site in Rutland \n# Uncomment max_items to limit the seatch to a set number of items\n\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-04-01',\n        'end_datetime&lt;=2023-10-01',\n        'eo:cloud_cover&lt;=75.0'\n      ],\n    # max_items=10,\n)\n\nitems = list(item_search.items())\nlen(items)\n\n30\n\n\nNow that we know we have available data we can start to build out a data cube.\n\n# First we create a dask client\n\nclientd = dask.distributed.Client()\nconfigure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client) # sets up gdal for cloud use\ndisplay(clientd)\n\n\nprint(f\"Found: {len(items):d} datasets\")\n\n# Convert STAC items into a GeoJSON FeatureCollection\nstac_json = item_search.item_collection_as_dict()\n\nFound: 30 datasets\n\n\n\ngdf = gpd.GeoDataFrame.from_features(stac_json, \"epsg:4326\")\ngdf.columns\n\nIndex(['geometry', 'file_count', 'start_datetime', 'end_datetime',\n       'NSSDC Identifier', 'created', 'Instrument Family Name',\n       'Platform Number', 'Datatake Type', 'esa_file_name',\n       'Ground Tracking Direction', 'datetime', 'instance_id', 'size',\n       'Product Type', 'Instrument Family Name Abbreviation',\n       'Start Orbit Number', 'eo:cloud_cover', 'Start Relative Orbit Number',\n       'updated', 'Instrument Mode', 'EPSG'],\n      dtype='object')\n\n\n\n# Plot an outline of the data items as a sense check\nf = folium.Figure(width=600, height=300)\nm = folium.Map(location=[52, 2], zoom_start=5).add_to(f)\n\ngdf.explore(\n    \"esa_file_name\",\n    categorical=True,\n    tooltip=[\n        \"esa_file_name\",\n        \"datetime\",\n        \"eo:cloud_cover\",\n    ],\n    popup=False,\n    legend=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"STAC\",\n    m=m,\n)\n\n\n\n\n\n# Construct the dask dataset\n\n# Since we will plot it on a map we need to use `EPSG:3857` projection\ncrs = \"epsg:3857\"\nzoom = 2**5  # overview level 5\n\nxx = stac_load(\n    items,\n    crs=crs,\n    resolution=10 * zoom,\n    chunks={\"x\": 2048, \"y\": 2048},  # &lt;-- use Dask\n    groupby=\"solar_day\",\n)\n\nprint(f\"Bands: {','.join(list(xx.data_vars))}\")\n#display(xx)\n\nBands: cloud,cloud_probability,thumbnail,topographic_shadow,cog,valid_pixels,saturated_pixels\n\n\nWe are currently looking into workarounds for a known conflict of using the eo STAC extension. Once a suitable way forward is identiified a full description of how to create and manipulate the CEDA Sentinel 2 ARD in a data cube (generated from STAC records) will be posted in the training materials repository.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/1_202409_Defra_DataDiscovery.html",
    "href": "presentations/DEFRA/1_202409_Defra_DataDiscovery.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment\n\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh geopandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s data discovery\nThere are a number of API endpoints that are exposed by the EODH. Oxidian have developed a Python API Client, pyeodh, that makes the Hub’s API endpoints available to Python users. pyeodh is available on PyPi (https://pypi.org/project/pyeodh/) and can be installed using pip. Documentation for the API Client is available at: https://pyeodh.readthedocs.io/en/latest/api.html\nWe will use pyeodh throughout this presentation.\n\n# Imports\nimport pyeodh\n\nimport os\n\nimport shapely \nimport geopandas as gpd\nimport folium\n\nimport urllib.request\nfrom io import BytesIO \nfrom PIL import Image\n\nHaving imported the necessary libraries the next task is to set up the locations of the areas of interest. Having created the AOI points the user needs to connect to the Resource Catalogue so that they can start to find some data.\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\n\n# Optional cell\n# If you want to see these points on a map run this cell\n\n# Create a map (m) centered between the two points\ncenter_lat = (rut_pnt.y + thet_pnt.y) / 2\ncenter_lon = (rut_pnt.x + thet_pnt.x) / 2\n\nm = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n\n# Add markers for each point\nfolium.Marker([rut_pnt.y, rut_pnt.x], popup=\"Rutland Site\", icon=folium.Icon(color=\"blue\")).add_to(m)\nfolium.Marker([thet_pnt.y, thet_pnt.x], popup=\"Thetford Site\", icon=folium.Icon(color=\"green\")).add_to(m)\n\n# Step 4: Display the map\nm\n\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"{collect.id}: {collect.description}\")\n\ncmip6: CMIP6\ncordex: CORDEX\nukcp: UKCP\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\nairbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\nairbus_data_example: Airbus data\nsentinel2_ard: sentinel 2 ARD\nsentinel1: Sentinel 1\nnaip: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n\n\n\n# The next thing to do is find some open data\n# For this presentation we want to find Sentinel-2 analysis ready (ARD) imagery near Rutland\n\n# First we just want to understand some of the parameters linked to the data collection\n# We will just print the first 5 records and the dataset temporal extent   \nsentinel2_ard = client.get_catalog(\"supported-datasets/ceda-stac-fastapi\").get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\nlim = 5\ni = 0\n\nfor item in sentinel2_ard.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('DATASET TEMPORAL EXTENT: ', [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb\nDATASET TEMPORAL EXTENT:  ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# To find out information about all the imagery in the collection then use this cell\n# It undertakes a search for specific date ranges (November 2023) and limits the pagination return to 10\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n    ],\n    limit=10,\n)\n\n# The item id and start time of image capture can be printed\n# If end time is also required, add the following code to the print statement: item.properties[\"end_datetime\"]  \nfor item in item_search:\n    print(item.id, item.properties[\"start_datetime\"])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65 2023-11-21T11:43:49+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn527lonw0007_T30UXD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0037_T30UVC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0022_T30UWC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn518lonw0008_T30UXC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn510lonw0036_T30UVB_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.18.S2B_20231118_latn554lonw0053_T30UUG_ORB080_20231118122250_utm30n_osgb 2023-11-18T11:33:19+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn519lone0023_T31UDT_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn509lone0009_T31UCS_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\n\n\n\n# To find specific imagery for the Rutland site we need to add the intersects parameter. We set this to be our AOI point.\n# We can also filter the search by cloud cover, in this case limiting our search to images with less than 50% cloud in them\n\nitems = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n        'Cloud Coverage Assessment&lt;=50.0'\n    ],\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of items found: ', items.total_count)\n\nNumber of items found:  2\n\n\n\n# For the purposes of this presentation we will look at the second record ([1]) in more detail\n# First we need to understand what information we can access\n\na = items[1].to_dict()\nprint(a.keys())\n\ndict_keys(['type', 'stac_version', 'id', 'properties', 'geometry', 'links', 'assets', 'bbox', 'stac_extensions', 'collection'])\n\n\n\n# The data we want to access is stored under the 'assets' key. But what information is held in that? \nfor key in (a['assets']):\n    print(key)\n\ncloud\ncloud_probability\ncog\nmetadata\nsaturated_pixels\nthumbnail\ntopographic_shadow\nvalid_pixels\n\n\n\n# Now we can get the link to each of the different assets\nfor key, value in items[1].assets.items():\n    print(key, value.href)\n\ncloud https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds.tif\ncloud_probability https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds_prob.tif\ncog https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif\nmetadata https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml\nsaturated_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_sat.tif\nthumbnail https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_thumbnail.jpg\ntopographic_shadow https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_toposhad.tif\nvalid_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_valid.tif\n\n\n\n# We can use this information to view the image thumbnail\n\nasset_dict = items[1].assets\n\n# Get the url as a string\nthumbnail_assets = [v for k, v in asset_dict.items() if 'thumbnail' in k]\nthumbnail_url = thumbnail_assets[0].href\n\n# Here we open the remote URL, read the data and dislay the thumbnail \nwith urllib.request.urlopen(thumbnail_url) as url:\n    img = Image.open(BytesIO(url.read()))\n\ndisplay(img)\n\n\n\n\n\n\n\n\nThis shows that we can relatively easily interrogate the Resource Catalogue and filter the results so that we can find the data we require in the EODH. With a bit of tweaking of the code the user could also generate a list of assets and accompanying URLs to the datasets (for this and other datasets).\nNow our user wants to see what commercial data exists for the Thetford site.\n\n# Find some commercial data\n\nfor collect in client.get_collections():\n    if 'defra' in collect.id: \n        print(f\"{collect.id}: {collect.description}\")\n\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\n\n\n\n# Let's search for information on the Planet holdings  \nplanet = client.get_catalog(\"supported-datasets/defra\").get_collection('defra-planet')\nplanet.get_items()\n\nlim = 5\ni = 0\n\nfor item in planet.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('PLANET DATASET TEMPORAL EXTENT: ', [str(d) for d in planet.extent.temporal.intervals[0]])\n\n2024-08-23_strip_7527622_composite\n2024-08-23_strip_7527462_composite\nPLANET DATASET TEMPORAL EXTENT:  ['2024-08-23 11:09:19.358417+00:00', '2024-08-23 11:24:40.991786+00:00']\n\n\n\n# To find specific imagery for the Thetford site we need to add the intersects parameter. We set this to be our AOI point.\nitems1 = client.search(\n    collections=['defra-planet'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\nitems2 = client.search(\n    collections=['defra-airbus'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of Planet items found: ', items1.total_count)\nprint('Number of Airbus items found: ', items2.total_count)\n\nNumber of Planet items found:  1\nNumber of Airbus items found:  2\n\n\n\nfor key, value in items1[0].assets.items():\n    print('Planet: ', key, value)\n\nPlanet:  data &lt;Asset href=2024-08-23_strip_7527462_composite_file_format.tif&gt;\nPlanet:  udm2 &lt;Asset href=2024-08-23_strip_7527462_composite_udm2_file_format.tif&gt;\n\n\nThe final step would be to use the ordering service integrated into the EODH resource catalogue to purchase the required commercial imagery. This would be stored in a users workspace and could then be used in specific workflows or for data analytics (depending on licence restrictions).\nFor the purposes of this presentation we looked at the different commercial datasets offline in QGIS.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Discovery"
    ]
  },
  {
    "objectID": "api-client/2_ResourceCatalog.html",
    "href": "api-client/2_ResourceCatalog.html",
    "title": "Resource Catalog",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nimport pyeodh\n\nFirst we need to create an instance of the Client, which is our entrypoint to EODH APIs.\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"{collect.id}: {collect.description}\")\n\ncmip6: CMIP6\ncordex: CORDEX\nukcp: UKCP\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\nairbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\nairbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\nairbus_data_example: Airbus data\nsentinel2_ard: sentinel 2 ARD\nsentinel1: Sentinel 1\n\n\nAll attributes are mapped to properties, e.g.\n\nceda_cat = service.get_catalog(\"supported-datasets/ceda-stac-fastapi\")\nprint(\"id: \", ceda_cat.id)\nprint(\"title: \", ceda_cat.title)\nprint(\"description: \", ceda_cat.description)\n\nid:  ceda-stac-fastapi\ntitle:  Supported Datasets\ndescription:  Catalogue containing supported datasets\n\n\nAPI endpoints are wrapped in methods and are structured into classes following the same logic as the API. E.g. to fetch a collection item, I first need to get the collection from the resource catalog.\n\n# GET /stac-fastapi/collections/{collection_id}/items/{item_id}\ncmip6 = client.get_catalog(\"supported-datasets/ceda-stac-fastapi\").get_collection('cmip6')\n\nSome API responses are paginated (e.g. collection items), and you can simply iterate over them.\n\n# GET /stac-fastapi/collections/cmip6/items\nitems = cmip6.get_items()\nfor item in items:\n    print(item.id)\n\nCMIP6.ScenarioMIP.THU.CIESM.ssp585.r1i1p1f1.Amon.rsus.gr.v20200806\nCMIP6.ScenarioMIP.THU.CIESM.ssp585.r1i1p1f1.Amon.rlus.gr.v20200806\nCMIP6.ScenarioMIP.CSIRO.ACCESS-ESM1-5.ssp126.r1i1p1f1.day.uas.gn.v20210318\nCMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r1i1p1f1.day.pr.gn.v20210317\nCMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317\nCMIP6.ScenarioMIP.CSIRO.ACCESS-ESM1-5.ssp585.r1i1p1f1.day.rsds.gn.v20210318\nCMIP6.ScenarioMIP.CSIRO.ACCESS-ESM1-5.ssp585.r1i1p1f1.day.hurs.gn.v20210318\nCMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r1i1p1f1.day.tas.gn.v20210317\nCMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r1i1p1f1.day.psl.gn.v20210317\nCMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.tasmin.gn.v20210317\n\n\n\nNote: Requires reworking from here on\nAttempting to create a collection with id that already exists will result in 409 error code. To see the example in action delete the test collection first by running the cell below.\nDelete a colletion\n\nrc.get_collection(\"test1\").delete()\n\nCreate new collection example\n\ntest1 = rc.create_collection(id=\"test1\", title=\"Test\", description=\"Test collection\")\nprint(test1.description)\n\nUpdate a collection\n\ntest1.update(description=\"Different description\")\nprint(test1.description)\n\nCreate an item\n\ntestitem1 = test1.create_item(id=\"test1.testitem1\")\nprint(f\"Created {testitem1.id} in collection {testitem1.collection}\")\n\nUpdate an item\n\ntestitem1.update(properties={\"foo\": \"bar\"})\nprint(testitem1.properties)\n\nDelete an item\n\ntestitem1.delete()\n\nFind out more about the Resource Catalog\n\nprint(f\"Livecheck: PING-{rc.ping()}\")\nprint(\"\\nAPI conforms to:\", *rc.get_conformance(), sep=\"\\n\")\n\nSearch the Catalog\n\nfor result in rc.search(collections=['cmip6']):\n    print(result.id)",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Searching the Resource Catalogue"
    ]
  },
  {
    "objectID": "plugin/2_RunningWorkflows.html",
    "href": "plugin/2_RunningWorkflows.html",
    "title": "Running a workflow",
    "section": "",
    "text": "Running a workflow",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "Running A Workflow"
    ]
  },
  {
    "objectID": "api-client/planet-stac-proxy.html",
    "href": "api-client/planet-stac-proxy.html",
    "title": "Remove in production release",
    "section": "",
    "text": "This notebook demostrates usage of the EODH resource catalog API using pyeodh\n\nEnsure\n\na clear description of purpose,\nintended audience and/or use case\nlinkages between notebooks and other training resources (if required)\n\n\n\nRecord\n\nTechnical dependencies,\nPlatform and Service Dependencies,\nPython Language versions,\nlibraries, additional scripts and files.\n\n\n\nRemember\n“When Jupyter notebooks are used in an educational context, they should not only be conceptualized to teach a specific topic but should also set a good example by following and implementing best practices for scientific computing”\n\nNeed in-order execution of notebook cells\nGood-quality code\nNo code duplication\nImports at the beginning of a notebook\nConsistent code style and formatting\nMeaningful names for variables\nLicence for code and training resources\n\n\n\n\nPlanet STAC Example\nDemo of retrieving data from Planet via their STAC proxy api using the PySTAC Client. Requires an api-key saved in as .planet.json which has the content {\"key\": \"API_KEY\"}.\n\nimport base64\nfrom pystac_client import Client\nimport requests\nfrom shapely import Point\nimport planet\nfrom IPython.display import Image\n\napi_key = planet.Auth.from_file().value\n\nuserpass = f\"{api_key}:\"\n\nheaders = {\n    \"Authorization\": f\"Basic {base64.b64encode(userpass.encode()).decode()}\"\n}\n\n\nclient = Client.open(\n    url=\"https://api.planet.com/x/data/\",\n    headers=headers\n)\n\nitem_search = client.search(\n    collections=['PSScene'],\n    datetime='2024-06-01/2024-08-01',\n    intersects=Point(-111, 45.68),\n    filter={\n        \"op\": \"&gt;=\",\n        \"args\": [{\"property\": \"clear_percent\"}, 50]\n    }\n)\n\nitem = next(item_search.items())\nthumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\nImage(data=thumbnail.content)\n\n\n\n\n\n\n\n\n\nimport pyeodh\nimport base64\nfrom io import BytesIO\nfrom pystac_client import Client\nimport requests\nfrom shapely import Point\nimport planet\nfrom IPython.display import Image\n\napi_key = planet.Auth.from_file().value\n\nuserpass = f\"{api_key}:\"\n\nheaders = {\n    \"Authorization\": f\"Basic {base64.b64encode(userpass.encode()).decode()}\"\n}\n\nclient = pyeodh.Client(username=api_key).get_catalog_service()\nitem_search = client.search(\n    collections=['PSScene'],\n    datetime='2024-06-01/2024-08-01',\n    intersects=Point(-111, 45.68),\n    filter={\n        \"op\": \"&gt;=\",\n        \"args\": [{\"property\": \"clear_percent\"}, 50]\n    }\n)\n\nitem = item_search[0]\nthumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\nImage(data=thumbnail.content)\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[10], line 29\n     18 client = pyeodh.Client(username=api_key).get_catalog_service()\n     19 item_search = client.search(\n     20     collections=['PSScene'],\n     21     datetime='2024-06-01/2024-08-01',\n   (...)\n     26     }\n     27 )\n---&gt; 29 item = item_search[0]\n     30 thumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\n     31 Image(data=thumbnail.content)\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:82, in PaginatedList.__getitem__(self, index)\n     80 def __getitem__(self, index: int) -&gt; T:\n     81     assert isinstance(index, int)\n---&gt; 82     self._fetch_to_index(index)\n     83     return self._elements[index]\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:87, in PaginatedList._fetch_to_index(self, index)\n     85 def _fetch_to_index(self, index: int) -&gt; None:\n     86     while len(self._elements) &lt;= index and self._has_next():\n---&gt; 87         new_elements = self._fetch_next()\n     88         self._elements += new_elements\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:96, in PaginatedList._fetch_next(self)\n     94 if not self._next_url:\n     95     raise RuntimeError(\"Next url not specified!\")\n---&gt; 96 headers, resp_data = self._client._request_json(\n     97     self._method,\n     98     self._next_url,\n     99     headers=self._headers,\n    100     params=self._params,\n    101     data=self._data,\n    102 )\n    103 next_link = next(\n    104     filter(lambda ln: ln.get(\"rel\") == \"next\", resp_data.get(\"links\", {})), {}\n    105 )\n    106 self._next_url = next_link.get(\"href\")\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/client.py:102, in Client._request_json(self, method, url, headers, params, data, encode)\n     93 def _request_json(\n     94     self,\n     95     method: RequestMethod,\n   (...)\n    100     encode: Callable[[Any], tuple[str, Any]] = _encode_json,\n    101 ) -&gt; tuple[Headers, Any]:\n--&gt; 102     status, resp_headers, resp_data = self._request_json_raw(\n    103         method, url, headers, params, data, encode\n    104     )\n    106     if not len(resp_data):\n    107         return resp_headers, None\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/client.py:89, in Client._request_json_raw(self, method, url, headers, params, data, encode)\n     82 logger.debug(\n     83     f\"Received response {response.status_code}\\nheaders: {response.headers}\"\n     84     f\"\\ncontent: {response.text}\"\n     85 )\n     86 # TODO consider moving this to _requst_json() and raise own exceptions\n     87 # so that we can user _raw in e.g. delete methods where we expect a 409 and\n     88 # want to recover\n---&gt; 89 response.raise_for_status()\n     91 return response.status_code, response.headers, response.text\n\nFile /opt/jaspy/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n   1019     http_error_msg = (\n   1020         f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n   1021     )\n   1023 if http_error_msg:\n-&gt; 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 400 Client Error: Bad Request for url: https://test.eodatahub.org.uk/api/catalogue/stac/search"
  },
  {
    "objectID": "presentations/DEFRA/2_202409_Defra_DataProcessing.html",
    "href": "presentations/DEFRA/2_202409_Defra_DataProcessing.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include: * A Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more * A Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements * A Web Presence - an intuitive user interface to allow account management, data discovery and mapping * An App Hub - a science portal providing access to a Jupyter lab environment\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh pandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s massive compute\nThe EODH compute architecture is built around a new OGC standard called EO Application Packages (EOAP). These are complex constructions of code and data, and at their core is the concept of a Common Workflow Language (CWL) workflow. To run CWL workflows you need a CWL runner, and the EODH Workflow Runner provides that. The EOAPs require a workflow description in CWL, a Docker container, bespoke scripts and links to the data. In the case of EODH, the data inputs and outputs are to be provided as STAC catalogues. Oxidian, as part of our work developing integrations for the Hub, have created a generator tool eoap-gen that abstracts away much of the complexity (check out the training materials repository and website for more details).\nOxidian have also developed a QGIS plugin to allow desktop users to discover, parameterise and execute workflows on the Hub.\n\n# Imports\nimport pyeodh\n\nimport os\nfrom requests import HTTPError\n\n\n# First the user needs to connect to the Workflow Runner\n# Currently this is done by obtaining a user account and API key from the core development team. Here, those details have been saved in secrets.txt\n# secrets.txt should contain the two lines below where USERNAME and API_KEY are specific to the user:\n# USER=\"USERNAME\"\n# PSWD=\"API_KEY\" \n\nwith open('secrets.txt', 'r') as file:\n    lines = file.readlines()\n    username = lines[0].strip().split('=')[1].strip('\"')\n    token = lines[1].strip().split('=')[1].strip('\"')\n\nclientwfr = pyeodh.Client(username=username, token=token, s3_token=token)\nwfr = clientwfr.get_ades()\n\nOur user wants to know what workflows they have access to in their workspace on the Hub.\n\n# List the workflows available in the user workspace\nfor p in wfr.get_processes():\n    print(p.id)\n\ndisplay\necho\nconvert-img\n\n\nAs development continues the number of demonstration workflows available to users will increase. With uptake of the generator tool users will also be able to create their own bespoke workflows. In time, the Hub will contain organisational accounts and the ability to share workflow files.\nOur user wants to deploy a workflow that they have found online. They can do so for compliant CWL files by submitting the URl.\n\n# Deploy a workflow using a URL to a .cwl file hosted online\nconvert_url_proc = wfr.deploy_process(\n    cwl_url=\"https://raw.githubusercontent.com/EOEPCA/deployment-guide/main/deploy/samples/requests/processing/convert-url-app.cwl\"\n)\nprint(convert_url_proc.id, convert_url_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nconvert-url Convert URL : Deployed\n\ndisplay\necho\nconvert-img\nconvert-url\n\n\n\n# If a user wants to tidy their workspace or no longer wants access to a workflow they can remove it\ntry:\n    wfr.get_process(\"convert-url\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\nProcess removed\n\n\nThe user is interested in the ARD files, but they are too large for the task that they want to undertake. The workflow file linked to here takes a series of data from the Sentinel 2 ARD collection and resizes the imagery using gdal_translate. The CWL file needs to be submitted to the Workflow Runner and parameterised, before it can then be run.\n\n# Deploy the workflow\n# Remove an existing nstance of the workflow\ntry:\n    wfr.get_process(\"convert-img\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\n# Deploy from a URL to a .cwl file hosted online\n#img_proc = wfr.deploy_process(cwl_yaml=cwl_yaml)\nimg_proc = wfr.deploy_process(cwl_url='https://raw.githubusercontent.com/ajgwords/eodh-tests/main/resize-col.cwl')\n\nprint(img_proc.id, img_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nProcess not found\nresize-col Resize collection cogs : Deployed\n\ndisplay\necho\nresize-col\n\n\nAs part of the presentation we looked at the plugin offline. Screenshots are provided here for those who do not have the plugin installed.\nOnce the CWL file has been submitted then it is possible for users with suitable permissions to also view the list of available workflows through the QGIS plugin.\n\n\n\nimage.png\n\n\nThe user can choose and parameterise a workflow using the QGIS plugin as shown in this screenshot. A series of defaults are provided with all workflow files so a user can run the workflow straight away using those. To do so in code, the user provides an empty dictionary.\n\n\n\nimage.png\n\n\n\n# Run the workflow using the defaults\npyeodh.set_log_level(10) # set the logging to be verbose\n\nresize_job = img_proc.execute(\n    {\n    }\n)\n\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'POST', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution', 'headers': {'Prefer': 'respond-async'}, 'params': None, 'data': {'inputs': {'workspace': 'ajgwords'}}, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: POST https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution\n       headers: {'Prefer': 'respond-async', 'Content-Type': 'application/json'}\n       params: None\n       body: {\"inputs\": {\"workspace\": \"ajgwords\"}}\nDEBUG: Received response 201\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:46 GMT', 'x-powered-by': 'ZOO-Project-DRU', 'x-also-powered-by': 'jwt.securityIn', 'x-also-also-powered-by': 'dru.securityIn', 'preference-applied': 'respond-async', 'x-also-also-also-powered-by': 'dru.securityOut', 'location': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'RtavU9ppINxjxyCvlYOnhxdNqGnDtX-HNJmyLlCbtYPf3Dcd5MgLsg=='}\n       content: {\n         \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\",\n         \"type\": \"process\",\n         \"processID\": \"resize-col\",\n         \"created\": \"2024-09-18T14:05:46.805Z\",\n         \"started\": \"2024-09-18T14:05:46.805Z\",\n         \"updated\": \"2024-09-18T14:05:46.805Z\",\n         \"status\": \"running\",\n         \"message\": \"ZOO-Kernel accepted to run your service!\",\n         \"links\": [\n           {\n             \"title\": \"Status location\",\n             \"rel\": \"monitor\",\n             \"type\": \"application/json\",\n             \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"\n           }\n         ]\n       }\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running ZOO-Kernel accepted to run your service!\n\n\n\nresize_job.refresh()\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'GET', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'headers': None, 'params': None, 'data': None, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: GET https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\n       headers: {}\n       params: None\n       body: None\nDEBUG: Received response 200\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '507', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:54 GMT', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'IakHOzKo8AnbmuIW_lFytmYjofpvygKjKotlpOjxhGiQ-a5G6RH22A=='}\n       content: {\"progress\": 15, \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\", \"type\": \"process\", \"processID\": \"resize-col\", \"created\": \"2024-09-18T14:05:46.805Z\", \"started\": \"2024-09-18T14:05:46.805Z\", \"updated\": \"2024-09-18T14:05:48.804Z\", \"status\": \"running\", \"message\": \"processing environment created, preparing execution\", \"links\": [{\"title\": \"Status location\", \"rel\": \"monitor\", \"type\": \"application/json\", \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"}]}\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running processing environment created, preparing execution\n\n\nThe outputs above show the status of the running job: \"status\": \"running\". The job status can also be monitored using the QGIS plugin.\n\n\n\nimage.png\n\n\n\n\n\nimage-2.png\n\n\nThe outputs are accessible and loadable from the QGIS plugin.\nOriginal image: \nResampled image: \nThe workflow outputs are available in a users’s workspace and are attached to the job id presented through the QGIS plugin. A user can open any output from within the plugin to be displayed in the QGIS map window.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Processing"
    ]
  },
  {
    "objectID": "website/about.html",
    "href": "website/about.html",
    "title": "About",
    "section": "",
    "text": "What is the EODH\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment\n\n\n\nResources on this website\nThere are a number of resources on this website. The site is a reflection of the training materials repository on GitHub so all Notebooks can be discovered there. Notebooks can then be run locally or on the AppHub.",
    "crumbs": [
      "About"
    ]
  }
]