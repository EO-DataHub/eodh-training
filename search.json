[
  {
    "objectID": "workflows/5_UserExample2.html",
    "href": "workflows/5_UserExample2.html",
    "title": "User Example 2: temporal “best-pixel” mosaics",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nBackground\nThe problem to be solved in this use case is whether or not a data cube could be constructed for temporal cloud-free mosaics\n\n\n\nCloudfree Workflow\n\n\n\n\nGenerating the workflow",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: temporal mosaics"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html",
    "href": "workflows/3_eoapgen.html",
    "title": "EOAP Generator",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html#requirements",
    "href": "workflows/3_eoapgen.html#requirements",
    "title": "EOAP Generator",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements that are needed for the tool to create a working EOAP. These are: * Python scripts. These must use argparse or click and the parameters will be mapped to the CWL CommandLineTool inputs * A pip requirements file for each script being wrapped into the EOAP TO DO: check this can now also take conda * A compliant eoap-gen configuration file",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html#steps",
    "href": "workflows/3_eoapgen.html#steps",
    "title": "EOAP Generator",
    "section": "Steps",
    "text": "Steps\nA full tutorial is provided with the repository (see https://github.com/EO-DataHub/eoap-gen/blob/main/ades_guide.md). Here, we will outline the main steps required in using the eoap-gen tool.\nThe first thing a user is required to do is understand the workflow that they want to wrap. At it’s most simple the steps of a workflow are threefold: * find your input data, * process your input data, and * create a STAC output of the processed data.\nFor the eoap-gen tool these steps will always be required and when using the workflow runner (WR) (aka ADES) on the EODH the output will always need to be a directory output containing a STAC catalog. When using the EODH it is recommended that the Python API client pyeodh is used to access the API endpoints on the Hub.\nThe following directory structure is recommended when using the eoap-gen tool:\n\n.github\n└── workflows\n    └── build.yml\nget_urls\n├── get_urls.py\n└── get_urls_reqs.txt\nmake_stac\n├── make_stac.py\n└── make_stac_reqs.txt\nconfig.yml",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html#other-tools",
    "href": "workflows/3_eoapgen.html#other-tools",
    "title": "EOAP Generator",
    "section": "Other tools",
    "text": "Other tools\nOther useful tools that you may want to try include:\n\ncwltool\nThe cwltool is “the reference implementation of the Common Workflow Language open standards. It is intended to be feature complete and provide comprehensive validation of CWL files as well as provide other tools related to working with CWL”. It is a commandline tool designed to run locally and is an excellent piece of software to help check that CWL is compliant. It is designed for use on Linux and will also run on a Mac or Windows (through WSL - windows Subsystem for Linux). It can implement Docker, Podman, Singularity and others for the containerisatoion of commandline components.\n\n\nscriptcwl\nScriptcwl is a Python package for creating CWL workflows and the latest doscumentation gives an indepth explanation of its use. Be aware that this tool has not been developed on or updated for many years.\n\n\ncwl-utils\nStill actively developed, cwl-utils provides Python utilities and autogenerated classes for loading and parsing CWL documents. Although not specific to EOAPs this set of tools may be helpful when developing your workflows. Documentation is relatively sparse.\nhttps://github.com/EO-DataHub/eoap-gen/blob/main/ades_guide.md",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html",
    "href": "workflows/1_Workflows.html",
    "title": "What is a Workflow",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#scripting-workflows",
    "href": "workflows/1_Workflows.html#scripting-workflows",
    "title": "What is a Workflow",
    "section": "Scripting workflows",
    "text": "Scripting workflows\n\nWhy CWL?\nWhile shell scripts or other code scripts (e.g. Python) can meet the need of data processing workflows, using a formal workflow language (such as CWL) brings additional benefits such as abstraction and improved scalability and portability.\nComputational workflows explicitly create a divide between a user’s dataflow and the computational details which underpin the chain of tools.\nThe dataflow is described by the workflow and the tool implementation is specified by descriptors that remove the workflow complexity.\nWorkflow managers such as cwltool help with the automation, monitoring and tracking of a dataflow. By producing computational workflows in a standardised format, and publishing them (alongside any data) with open access, the workflows become more FAIR (Findable, Accessible, Interoperable, and Reusable). The Common Workflow Language (CWL) standard has been developed to standardise workflow needs across different thematic areas.\ncwltool is a great way to test workflows locally: https://cwltool.readthedocs.io/en/latest/cli.html\nGood tutorial here: https://andrewjesaitis.com/posts/2017-02-06-cwl-tutorial/\nCheck your workflow: https://view.commonwl.org/\nExamples - link to Lot 1 & 2 Other tools - link to Lot 2 tool",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#context",
    "href": "workflows/1_Workflows.html#context",
    "title": "What is a Workflow",
    "section": "Context",
    "text": "Context\nThe first thing to do when designing a workflow is understand the context of what is desired, and how that may need to be referred to in the workflow. For this example workflow, we will take a list of Sentinel-2 ARD images, clip them to an area of interest, and stack them. The flow will look like the following:\ngraph LR;\n  get_data -- S2_ARD --&gt; clip -- clipped --&gt; stack -- stacked--&gt; Output ;\n\nThe next thing to do is access the data to be used in the Workflow. In this case we will download two bands of a Sentinel 2 image held on AWS. We will use the curl tool to do this, saving the accessed image as B0$.tif (where $ is the band number):\ncurl -o B04.tif https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/53/H/PA/2021/7/S2B_53HPA_20210723_0_L2A/B04.tif\n\ncurl -o B03.tif https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/53/H/PA/2021/7/S2B_53HPA_20210723_0_L2A/B03.tif \nThe commands that we will use in the workflow are all available through gdal.\n\nClip the image\nWe will use gdal_-_translate to clip the larger image to a smaller more manageable dataset. The gdal command that we can test and that we will need to replicate in CWL is:\ngdal_translate -projwin ULX ULY LRX LRY  -projwin_srs EPSG:4326 BO4.tif B04_clipped.tif\nwhere the coordinates UL refer to upper left and LR to lower right X and Y.\n\n\nStack the clips\nSimilarly, we will use gdal_merge.py to construct the stacked images from the clipped image. This can be tested using the following command:\ngdal_merge.py -separate B04_clipped.tif B03_clipped.tif -o stacked.tif",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#building-the-workflow",
    "href": "workflows/1_Workflows.html#building-the-workflow",
    "title": "What is a Workflow",
    "section": "Building the Workflow",
    "text": "Building the Workflow\n\nRequired files\nThere are three main files that are required to construct a CWL Workflow. These are: * DockerFile or existing online container * CWL file * YAML file It may be that other files e.g. a .sh script or a Python script are also needed, depending on how bespoke and/or complex the desired workflow is.\n\n\ncwltool\nTo run CWL workflows you will need a CWL runner. The most commonly used (locally) is cwltool which is maintained by the CWL community. It will support everything in the current CWL specification. cwltool can be installed using pip or variants of conda. More information can be found here and here.\n\n\nContainers\nFor the purposes of this example, we will be pulling the GDAL container from the OSgeo repository (see here).\nNOTE: There are a number of different images that can be accessed. To use the .py tools available through GDAL then ‘GDAL Python’ is required.\nIf we wanted to we could also build our own bespoke image using a DockerFile and then run that. This is often used when data processing scripts need to be copied into the container.\nWe will also be using Podman as our container software. ‘podman’ is a drop in replacement for Docker but does require the --podman arguement in the cwltool command. If using Windows, or if you are more familiar with Docker, then using Docker is the default containerisation method.\n\n\nCWL files\nFor this example we require a CWL CommandLine file for both the clipping and stacking components of the workflow. We will also need a CWL Workflow file to bring these together and run the entire process. The next block of code outlines the overall Workflow file. Note: This example is based on the example found here. Some errors were found in the original CWL files and the version presented here has been tested and is known to work on a local Linux (Debian based) system.\nclass: Workflow\nlabel: Sentinel-2 clipping and stacking\ndoc:  This workflow creates a stacked composite. File name: composite.cwl\nid: main\n\nrequirements:\n- class: ScatterFeatureRequirement\n\ninputs:\n\n  geotiff:\n    doc: list of geotifs\n    type: File[]\n\n  bbox: \n    doc: area of interest as a bounding box\n    type: string\n\n  epsg:\n    doc: EPSG code \n    type: string\n    default: \"EPSG:4326\"\n\noutputs:\n  rgb:\n    outputSource:\n    - node_concatenate/composite\n    type: File\n\nsteps:\n\n  node_translate:\n\n    run: gdal-translate.cwl\n\n    in:\n\n      geotiff: geotiff  \n      bbox: bbox\n      epsg: epsg\n\n    out:\n    - clipped_tif\n\n    scatter: geotiff\n    scatterMethod: dotproduct\n\n  node_concatenate:\n\n    run: concatenate2.cwl\n\n    in: \n      tifs:\n        source: node_translate/clipped_tif\n\n    out:\n    - composite\n\n\ncwlVersion: v1.0\n\nFrom this example, we can see that we require two CommandLine CWL files: gdal-translate.cwl and concatenate2.cwl. Let’s deal with these in order.\nclass: CommandLineTool\n\ncwlVersion: v1.0\ndoc:  This runs GDAL Translate to clip an image to bbox corner coordinates.\n\nrequirements: \n  InlineJavascriptRequirement: {}\n  DockerRequirement: \n    dockerPull: ghcr.io/osgeo/gdal:ubuntu-small-latest\n\nbaseCommand: gdal_translate\n\narguments:\n- -projwin \n- valueFrom: ${ return inputs.bbox.split(\",\")[0]; }\n- valueFrom: ${ return inputs.bbox.split(\",\")[3]; }\n- valueFrom: ${ return inputs.bbox.split(\",\")[2]; }\n- valueFrom: ${ return inputs.bbox.split(\",\")[1]; }\n- valueFrom: ${ return inputs.geotiff.basename.replace(\".tif\", \"\") + \"_clipped.tif\"; }\n  position: 8\n\ninputs:\n  geotiff: \n    type: File\n    inputBinding:\n      position: 7\n  bbox: \n    type: string\n  epsg:\n    type: string\n    default: \"EPSG:4326\" \n    inputBinding:\n      position: 6\n      prefix: -projwin_srs\n      separate: true\n\noutputs:\n  clipped_tif:\n    outputBinding:\n      glob: '*_clipped.tif'\n    type: File\n\n\nclass: CommandLineTool\n\ncwlVersion: v1.0\ndoc: This runs GDAL Merge to stack images together.\n\nrequirements:\n  InlineJavascriptRequirement: {}\n  DockerRequirement: \n    dockerPull: ghcr.io/osgeo/gdal:ubuntu-small-latest\n\nbaseCommand: gdal_merge.py\n\narguments: \n- -separate \n- valueFrom: ${ return inputs.tifs; }\n- -o\n- composite.tif\n# gdal_merge.py -separate 1.tif 2.tif 3.tif -o rgb.tif\n\ninputs:\n\n  tifs:\n    type: File[]\n\noutputs:\n\n  composite:\n    outputBinding:\n      glob: '*.tif'\n    type: File\n\nNOTE: YAML generally doesn’t play well with tabs as whitespace so it’s best practice to use spaces for indentations",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#running-the-workflow",
    "href": "workflows/1_Workflows.html#running-the-workflow",
    "title": "What is a Workflow",
    "section": "Running the Workflow",
    "text": "Running the Workflow\nNow that we have our commandline CWL component files, and the Workflow CWL file that brings the tools together, we need to specify the input parameters. This is done using a parameters.yml file, where the name of the file can be anything that you want. The contents should follow the layout that we will be using:\nbbox: \"136.659,-35.96,136.923,-35.791\"\ngeotiff: \n- { \"class\": \"File\", \"path\": \"../B04.tif\" }\n- { \"class\": \"File\", \"path\": \"../B03.tif\" }\nepsg: \"EPSG:4326\"\nYou will need to change the path parameter to match the location of your input files.\nNow we run it with the command:\ncwltool --podman composite.cwl composite-params.yml\nNote: remember that if you are using Docker then you do not need the --podman arguement.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#outputs",
    "href": "workflows/1_Workflows.html#outputs",
    "title": "What is a Workflow",
    "section": "Outputs",
    "text": "Outputs\nThis workflow takes a couple of minutes to run, during which time the executed commandsand their runtime messages are displayed on the commandline. Once the workflow completes, the output file will be found in the directory from where the workflow was run. Intermediate files that are not specified in the out block in the workflow are automatically deleted.\nThe output .tif file can now be opened in QGIS or a similar software application to check that the output is as expected (in this case a 2-layer image of a clipped area of the extent of the input files).",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#tips",
    "href": "workflows/1_Workflows.html#tips",
    "title": "What is a Workflow",
    "section": "Tips",
    "text": "Tips\nYou can pass --leave-tmpdirs to the cwltool command. This is often helpful to figure out if the outputs from a step are what you think they should be.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "science/1_DataCubes.html",
    "href": "science/1_DataCubes.html",
    "title": "Code Snippets",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase different tools and functions that are of scientific interest/use and implement the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of November 2024.\nAuthor(s): Alastair Graham\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nCreate data cube using sentinel2_ard STAC records\nUsers may have a need to generate a data cube directly from the CEDA Sentinel 2 Analysis Ready Data (ARD) (sentinel2_ard). As the STAC catalogue for that dataset has been created in a specific way, users will need to be aware of this and make alterations from the default. The following code was supplied by Pete Gadomski of Development Seed and enables the generation of a data cube from the ARD catalogue (as structured in November 2024). The STAC catalogue may be reprocessed in future to allow the default settings for odc-stac to be implemented.\n\n# Note that this uses pystac and pystac_client\n\nimport os\n\nimport dask.distributed\nimport odc.stac\nimport rasterio\nfrom pystac.extensions.raster import RasterBand\nfrom pystac_client import Client\n\n\nclient = dask.distributed.Client()\n\n\nclient = Client.open(\"https://api.stac.ceda.ac.uk/\")\nitem_collection = client.search(\n    collections=[\"sentinel2_ard\"],\n    intersects={\"type\": \"Point\", \"coordinates\": [-1.3144, 51.5755]},\n    sortby=\"-properties.datetime\",\n    max_items=10,\n).item_collection()\n\n# Commented out for clarity when submitted to training materials website.\n# Uncomment the next line to view the Feature Collection\n\n#item_collection\n\nAs of November 2024, the STAC items in sentinel2_ard are missing information because the projection and raster extensions are not installed. This means that odc-stac will refuse to compute the “geobox” and hence the subsequent components needed to generate and view the data cube. The code in the following cell adds those extensions.\n\nfor item in item_collection.items:\n    asset = item.assets[\"cog\"]\n    cog = rasterio.open(asset.href)\n    epsg = cog.crs.to_epsg()\n    dtypes = cog.dtypes\n    shape = cog.shape\n    transform = list(cog.transform)\n\n    item.ext.add(\"proj\")\n    item.ext.add(\"raster\")\n\n    item.ext.proj.epsg = epsg\n\n    cog = item.assets[\"cog\"]\n    cog.ext.raster.bands = [RasterBand.create(data_type=dtype) for dtype in dtypes]\n    cog.ext.proj.shape = shape\n    cog.ext.proj.transform = transform\n\n\nbbox = [-1.2, 51.6, -1.1, 51.7]\ndataset = odc.stac.load(item_collection, bands=(\"B03\", \"B04\", \"B08\"), chunks={}, bbox=bbox)\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 48MB\nDimensions:      (y: 1121, x: 706, time: 10)\nCoordinates:\n  * y            (y) float64 9kB 2.005e+05 2.005e+05 ... 1.893e+05 1.893e+05\n  * x            (x) float64 6kB 4.554e+05 4.554e+05 ... 4.624e+05 4.624e+05\n    spatial_ref  int32 4B 27700\n  * time         (time) datetime64[ns] 80B 2023-10-03T11:08:09 ... 2023-11-20...\nData variables:\n    B03          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n    B04          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n    B08          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;xarray.DatasetDimensions:y: 1121x: 706time: 10Coordinates: (4)y(y)float642.005e+05 2.005e+05 ... 1.893e+05units :metreresolution :-10.0crs :EPSG:27700array([200525., 200515., 200505., ..., 189345., 189335., 189325.])x(x)float644.554e+05 4.554e+05 ... 4.624e+05units :metreresolution :10.0crs :EPSG:27700array([455385., 455395., 455405., ..., 462415., 462425., 462435.])spatial_ref()int3227700spatial_ref :PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9.01,61.01,2.01]],ID[\"EPSG\",27700]]crs_wkt :PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9.01,61.01,2.01]],ID[\"EPSG\",27700]]semi_major_axis :6377563.396semi_minor_axis :6356256.909237285inverse_flattening :299.3249646reference_ellipsoid_name :Airy 1830longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :OSGB36horizontal_datum_name :Ordnance Survey of Great Britain 1936projected_crs_name :OSGB36 / British National Gridgrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :49.0longitude_of_central_meridian :-2.0false_easting :400000.0false_northing :-100000.0scale_factor_at_central_meridian :0.9996012717GeoTransform :455380 10 0 200530 0 -10array(27700, dtype=int32)time(time)datetime64[ns]2023-10-03T11:08:09 ... 2023-11-...array(['2023-10-03T11:08:09.000000000', '2023-10-06T11:21:19.000000000',\n       '2023-10-08T11:09:41.000000000', '2023-10-13T11:09:19.000000000',\n       '2023-10-26T11:21:19.000000000', '2023-10-28T11:11:51.000000000',\n       '2023-11-10T11:23:11.000000000', '2023-11-15T11:22:39.000000000',\n       '2023-11-17T11:13:31.000000000', '2023-11-20T11:23:51.000000000'],\n      dtype='datetime64[ns]')Data variables: (3)B03(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n15.10 MiB\n1.51 MiB\n\n\nShape\n(10, 1121, 706)\n(1, 1121, 706)\n\n\nDask graph\n10 chunks in 3 graph layers\n\n\nData type\nuint16 numpy.ndarray\n\n\n\n\n                                           706 1121 10\n\n\n\n\nB04(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n15.10 MiB\n1.51 MiB\n\n\nShape\n(10, 1121, 706)\n(1, 1121, 706)\n\n\nDask graph\n10 chunks in 3 graph layers\n\n\nData type\nuint16 numpy.ndarray\n\n\n\n\n                                           706 1121 10\n\n\n\n\nB08(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n15.10 MiB\n1.51 MiB\n\n\nShape\n(10, 1121, 706)\n(1, 1121, 706)\n\n\nDask graph\n10 chunks in 3 graph layers\n\n\nData type\nuint16 numpy.ndarray\n\n\n\n\n                                           706 1121 10\n\n\n\n\nIndexes: (3)yPandasIndexPandasIndex(Index([200525.0, 200515.0, 200505.0, 200495.0, 200485.0, 200475.0, 200465.0,\n       200455.0, 200445.0, 200435.0,\n       ...\n       189415.0, 189405.0, 189395.0, 189385.0, 189375.0, 189365.0, 189355.0,\n       189345.0, 189335.0, 189325.0],\n      dtype='float64', name='y', length=1121))xPandasIndexPandasIndex(Index([455385.0, 455395.0, 455405.0, 455415.0, 455425.0, 455435.0, 455445.0,\n       455455.0, 455465.0, 455475.0,\n       ...\n       462345.0, 462355.0, 462365.0, 462375.0, 462385.0, 462395.0, 462405.0,\n       462415.0, 462425.0, 462435.0],\n      dtype='float64', name='x', length=706))timePandasIndexPandasIndex(DatetimeIndex(['2023-10-03 11:08:09', '2023-10-06 11:21:19',\n               '2023-10-08 11:09:41', '2023-10-13 11:09:19',\n               '2023-10-26 11:21:19', '2023-10-28 11:11:51',\n               '2023-11-10 11:23:11', '2023-11-15 11:22:39',\n               '2023-11-17 11:13:31', '2023-11-20 11:23:51'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\ndataset.odc.geobox\ndataset = dataset.compute()\n\n\n_ = dataset.isel(time=0).to_array(\"band\").plot.imshow(vmin=0, vmax=256)",
    "crumbs": [
      "Science & code examples",
      "Data cubes"
    ]
  },
  {
    "objectID": "presentations/DEFRA2/1_202412_Defra_Technology.html",
    "href": "presentations/DEFRA2/1_202412_Defra_Technology.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Huband some of the integrations and applications around it as of December 2024. The Notebook “user” would like to ….\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-11-26\nDate last modified: 2024-12-12\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "presentations/DEFRA2/1_202412_Defra_Technology.html#presentation-set-up",
    "href": "presentations/DEFRA2/1_202412_Defra_Technology.html#presentation-set-up",
    "title": "Demonstration for DEFRA",
    "section": "Presentation set up",
    "text": "Presentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh geopandas matplotlib numpy pillow folium"
  },
  {
    "objectID": "presentations/DEFRA/2_202409_Defra_DataProcessing.html",
    "href": "presentations/DEFRA/2_202409_Defra_DataProcessing.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include: * A Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more * A Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements * A Web Presence - an intuitive user interface to allow account management, data discovery and mapping * An App Hub - a science portal providing access to a Jupyter lab environment\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh pandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s massive compute\nThe EODH compute architecture is built around a new OGC standard called EO Application Packages (EOAP). These are complex constructions of code and data, and at their core is the concept of a Common Workflow Language (CWL) workflow. To run CWL workflows you need a CWL runner, and the EODH Workflow Runner provides that. The EOAPs require a workflow description in CWL, a Docker container, bespoke scripts and links to the data. In the case of EODH, the data inputs and outputs are to be provided as STAC catalogues. Oxidian, as part of our work developing integrations for the Hub, have created a generator tool eoap-gen that abstracts away much of the complexity (check out the training materials repository and website for more details).\nOxidian have also developed a QGIS plugin to allow desktop users to discover, parameterise and execute workflows on the Hub.\n\n# Imports\nimport pyeodh\n\nimport os\nfrom requests import HTTPError\n\n\n# First the user needs to connect to the Workflow Runner\n# Currently this is done by obtaining a user account and API key from the core development team. Here, those details have been saved in secrets.txt\n# secrets.txt should contain the two lines below where USERNAME and API_KEY are specific to the user:\n# USER=\"USERNAME\"\n# PSWD=\"API_KEY\" \n\nwith open('secrets.txt', 'r') as file:\n    lines = file.readlines()\n    username = lines[0].strip().split('=')[1].strip('\"')\n    token = lines[1].strip().split('=')[1].strip('\"')\n\nclientwfr = pyeodh.Client(username=username, token=token, s3_token=token)\nwfr = clientwfr.get_ades()\n\nOur user wants to know what workflows they have access to in their workspace on the Hub.\n\n# List the workflows available in the user workspace\nfor p in wfr.get_processes():\n    print(p.id)\n\ndisplay\necho\nconvert-img\n\n\nAs development continues the number of demonstration workflows available to users will increase. With uptake of the generator tool users will also be able to create their own bespoke workflows. In time, the Hub will contain organisational accounts and the ability to share workflow files.\nOur user wants to deploy a workflow that they have found online. They can do so for compliant CWL files by submitting the URl.\n\n# Deploy a workflow using a URL to a .cwl file hosted online\nconvert_url_proc = wfr.deploy_process(\n    cwl_url=\"https://raw.githubusercontent.com/EOEPCA/deployment-guide/main/deploy/samples/requests/processing/convert-url-app.cwl\"\n)\nprint(convert_url_proc.id, convert_url_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nconvert-url Convert URL : Deployed\n\ndisplay\necho\nconvert-img\nconvert-url\n\n\n\n# If a user wants to tidy their workspace or no longer wants access to a workflow they can remove it\ntry:\n    wfr.get_process(\"convert-url\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\nProcess removed\n\n\nThe user is interested in the ARD files, but they are too large for the task that they want to undertake. The workflow file linked to here takes a series of data from the Sentinel 2 ARD collection and resizes the imagery using gdal_translate. The CWL file needs to be submitted to the Workflow Runner and parameterised, before it can then be run.\n\n# Deploy the workflow\n# Remove an existing nstance of the workflow\ntry:\n    wfr.get_process(\"convert-img\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\n# Deploy from a URL to a .cwl file hosted online\n#img_proc = wfr.deploy_process(cwl_yaml=cwl_yaml)\nimg_proc = wfr.deploy_process(cwl_url='https://raw.githubusercontent.com/ajgwords/eodh-tests/main/resize-col.cwl')\n\nprint(img_proc.id, img_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nProcess not found\nresize-col Resize collection cogs : Deployed\n\ndisplay\necho\nresize-col\n\n\nAs part of the presentation we looked at the plugin offline. Screenshots are provided here for those who do not have the plugin installed.\nOnce the CWL file has been submitted then it is possible for users with suitable permissions to also view the list of available workflows through the QGIS plugin.\n\n\n\nimage.png\n\n\nThe user can choose and parameterise a workflow using the QGIS plugin as shown in this screenshot. A series of defaults are provided with all workflow files so a user can run the workflow straight away using those. To do so in code, the user provides an empty dictionary.\n\n\n\nimage.png\n\n\n\n# Run the workflow using the defaults\npyeodh.set_log_level(10) # set the logging to be verbose\n\nresize_job = img_proc.execute(\n    {\n    }\n)\n\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'POST', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution', 'headers': {'Prefer': 'respond-async'}, 'params': None, 'data': {'inputs': {'workspace': 'ajgwords'}}, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: POST https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution\n       headers: {'Prefer': 'respond-async', 'Content-Type': 'application/json'}\n       params: None\n       body: {\"inputs\": {\"workspace\": \"ajgwords\"}}\nDEBUG: Received response 201\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:46 GMT', 'x-powered-by': 'ZOO-Project-DRU', 'x-also-powered-by': 'jwt.securityIn', 'x-also-also-powered-by': 'dru.securityIn', 'preference-applied': 'respond-async', 'x-also-also-also-powered-by': 'dru.securityOut', 'location': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'RtavU9ppINxjxyCvlYOnhxdNqGnDtX-HNJmyLlCbtYPf3Dcd5MgLsg=='}\n       content: {\n         \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\",\n         \"type\": \"process\",\n         \"processID\": \"resize-col\",\n         \"created\": \"2024-09-18T14:05:46.805Z\",\n         \"started\": \"2024-09-18T14:05:46.805Z\",\n         \"updated\": \"2024-09-18T14:05:46.805Z\",\n         \"status\": \"running\",\n         \"message\": \"ZOO-Kernel accepted to run your service!\",\n         \"links\": [\n           {\n             \"title\": \"Status location\",\n             \"rel\": \"monitor\",\n             \"type\": \"application/json\",\n             \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"\n           }\n         ]\n       }\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running ZOO-Kernel accepted to run your service!\n\n\n\nresize_job.refresh()\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'GET', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'headers': None, 'params': None, 'data': None, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: GET https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\n       headers: {}\n       params: None\n       body: None\nDEBUG: Received response 200\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '507', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:54 GMT', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'IakHOzKo8AnbmuIW_lFytmYjofpvygKjKotlpOjxhGiQ-a5G6RH22A=='}\n       content: {\"progress\": 15, \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\", \"type\": \"process\", \"processID\": \"resize-col\", \"created\": \"2024-09-18T14:05:46.805Z\", \"started\": \"2024-09-18T14:05:46.805Z\", \"updated\": \"2024-09-18T14:05:48.804Z\", \"status\": \"running\", \"message\": \"processing environment created, preparing execution\", \"links\": [{\"title\": \"Status location\", \"rel\": \"monitor\", \"type\": \"application/json\", \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"}]}\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running processing environment created, preparing execution\n\n\nThe outputs above show the status of the running job: \"status\": \"running\". The job status can also be monitored using the QGIS plugin.\n\n\n\nimage.png\n\n\n\n\n\nimage-2.png\n\n\nThe outputs are accessible and loadable from the QGIS plugin.\nOriginal image: \nResampled image: \nThe workflow outputs are available in a users’s workspace and are attached to the job id presented through the QGIS plugin. A user can open any output from within the plugin to be displayed in the QGIS map window.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Processing"
    ]
  },
  {
    "objectID": "plugin/3_OtherTools.html",
    "href": "plugin/3_OtherTools.html",
    "title": "Other QGIS Plugins",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-12-04\nDate last modified: 2024-12-05\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * EO Data Hub: https://eodatahub.org.uk/\n\nOther QGIS Plugins\nA major benefit derived from constructing the EODH platform from open components is that the technologies and tools can integrate with existing software. One useful pre-existing QGIS plugin is the STAC API Browser. Use this URL in the STAC API Browser plugin (https://test.eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue/) to find assets accessible through the EODH Resource Catalogue (RC).\nFirst you need to set up a connection as shown in the screenshot below.\n\n\n\nimg 1\n\n\nThen a search can be initiated. Depending on the quality of your connection and the number of returned items this could be slow.\n\n\n\nimg 2\n\n\nOne useful aspect of this plugin is to plot the returned item boundaries, as shown below. It is also possible to access specific item assets i.e. the data, but this can be very slow given the size of the assets themselves. It is recommended that a tool such as pyeodh or the EODH web interface is used to directly access the datasets referenced by the catalogues.\n\n\n\nimg 3",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "Other Tools"
    ]
  },
  {
    "objectID": "plugin/1_introduction.html",
    "href": "plugin/1_introduction.html",
    "title": "Introduction & Overview",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-10-23\nDate last modified: 2024-10-23\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * EO Data Hub: https://eodatahub.org.uk/",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "QGIS Plugin Introduction"
    ]
  },
  {
    "objectID": "plugin/1_introduction.html#why-do-we-need-this-plugin",
    "href": "plugin/1_introduction.html#why-do-we-need-this-plugin",
    "title": "Introduction & Overview",
    "section": "Why do we need this plugin?",
    "text": "Why do we need this plugin?\nA QGIS plugin is an extension or add-on for QGIS, a popular open-source Geographic Information System (GIS) software package. Although fully fnctioned, plugins enhance QGIS by adding specialised tools, workflows, or functionalities that extend its core capabilities. These plugins are written in Python and are highly customisable and diverse in functionality. They allow users to perform complex geospatial tasks more efficiently, integrate with external tools or datasets, and automate repetitive tasks, all within the QGIS environment. This adaptability makes QGIS a powerful and flexible tool for GIS professionals across various industries.\nThe EODH development team recognised that a tranche of potential users benefiting from the platform would be GIS specialists. As such, a QGIS plugin that automates connection to a user’s workspace on the Hub has been created. The plugin allows a user to connect to the EODH and access EO Application Packages (EOAP - for further information see pages relating to workflows). Once a required EOAP has been found, the user can parameterise and execute the workflow. Once complete the GIS user can then interact with the returned data assets and can view the processing logs.",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "QGIS Plugin Introduction"
    ]
  },
  {
    "objectID": "plugin/1_introduction.html#installation",
    "href": "plugin/1_introduction.html#installation",
    "title": "Introduction & Overview",
    "section": "Installation",
    "text": "Installation\nThere are two ways in which the plugin can be installed.\nInstall from zip\n\nGo to the plugin GitHub repository\nFind the latest release and download for your operating system\nOpen QGIS and click ….\nUse the “Install from Zip” function to install the plugin\n\nInstall from repository\n\nOpen QGIS and click ….\nSearch for EODH under the All or Uninstalled tabs\nSelect and click “Install”",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "QGIS Plugin Introduction"
    ]
  },
  {
    "objectID": "platform/1_Intro.html",
    "href": "platform/1_Intro.html",
    "title": "EODH Introduction",
    "section": "",
    "text": "Description & purpose: This webpage is designed to provide an introduction to the idea behind the Hub and borrows text from the EODH test server ‘About’ page. It also outlines the potential user journey that different practitioners might take.\nAuthor(s): Alastair Graham, EODH\nDate created: 2024-10-23\nDate last modified: 2024-12-05\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nEODH Introduction\nThe overall goal of the Earth Observation Data Hub (EODH) project is to develop and operate a new centralised software infrastructure – the Hub – to provide a ‘single point’ of access for UK Earth Observation (EO) data obtained from distributed sources, to include public and commercial centres.\nBy providing this single point of access, the objective is to provide a standard common set of services and APIs upon which new EO services and tools can be developed and accessed by the UK EO data community. The project is currently in a Pathfinder phase (2023-2025) that brings new thinking and experimental developments to bear, resulting in practical services for a variety of users in a short period of time.\nBy the end of the pathfinder phase (March 2025), it is expected that there will be a community of researchers, industry and government users working together to provide and intereact with EO data in new and innovative ways.\n\n\nExample User Journeys\nThe journey through the technology made accessbile by the Hub will be different depending on the needs of the user. The following diagram presents a simplified and high level route for four different user types: * a GIS user who spends most of their working time in QGIS * a web application developer, interested in hooking into the processing power of the EODH and the associated datasets * a data scientist, who will use code in a Jupyter notebook to process different spatial datasets * a dev-ops specialist, tasked with creating and maintaining data services hosted on the EODH\n\nThe concepts of the AppHub and its associated notebook service, workflows, data collections and the Resource Catalogue, and account access are all covered in other sections of these training materials.",
    "crumbs": [
      "Hub Platform",
      "EODH Platform Introduction"
    ]
  },
  {
    "objectID": "api-client/4_Services.html",
    "href": "api-client/4_Services.html",
    "title": "EODH Training Materials",
    "section": "",
    "text": "Description & purpose:\nAuthor(s): Alastair Graham\nDate created: 2024-12-05\nDate last modified: 2024-12-05\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# TO DO",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Using Mapping Services"
    ]
  },
  {
    "objectID": "api-client/2_ResourceCatalog.html",
    "href": "api-client/2_ResourceCatalog.html",
    "title": "Resource Catalog",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of December 2024.\nAuthor(s): Alastair Graham, Dusan Figala, EODH\nDate created: 2024-09-05\nDate last modified: 2024-12-05\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\nThe first thing to do is ensure that the most recent version of pyeodh is installed on your system. It is good practice to run the following cell if you have not installed pyeodh or have not used it in a while.\n\n# Run this cell if pyeodh is not installed, or needs updating\n!pip install --upgrade pyeodh\n\n\nExploring the Resource Catalogue\nNow we are ready to investigate the Resource Catalogue. First off, we need to import the pyeodh package.\n\n# Import the Python API Client\nimport pyeodh\n\nNext we need to create an instance of the Client, which is our entrypoint to EODH APIs. From there we can start to search the collections held within the platform.\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"- {collect.id}: {collect.description}\")\n\n- cmip6: CMIP6\n- cordex: CORDEX\n- ukcp: UKCP\n- airbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\n- col_f99e920a-a58e-11ef-aab0-7a33b70d8782: description\n- defra-airbus: A collection of Airbus data for the DEFRA use case.\n- defra-planet: A collection of Planet data for the DEFRA use case.\n- eocis-sst-cdrv3: This dataset provides daily estimates of global sea surface temperature (SST) based on observations from multiple satellite sensors. Resolution: 5km.  Available from 1980 onwards.\n- col_c1f6f668-b2f8-11ef-b6b1-ee3aaed8a789: description\n- sentinel2_ard: sentinel 2 ARD\n\n\nThe attributes of a catalogue are mapped to a series of properties. For instance, in the following cell we print the id, title and description for the supported-datasets catalogue.\n\nceda_cat = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\")\nprint(\"id: \", ceda_cat.id)\nprint(\"title: \", ceda_cat.title)\nprint(\"description: \", ceda_cat.description)\n\nid:  ceda-stac-catalogue\ntitle:  CEDA Catalogue\ndescription:  CEDA STAC Catalogue\n\n\nThe Hub API endpoints are wrapped in methods inside pyeodh and are structured into classes, following the same logic as the underlying APIs. This means that, for xample, to fetch a collection item we first need to get the collection from the resource catalogue. The following cell provedes a code example to do this.\n\n# GET /stac-fastapi/collections/{collection_id}/items/{item_id}\ncmip6 = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\").get_collection('cmip6')\ncmip6\n\n&lt;pyeodh.resource_catalog.Collection at 0x775d597a9010&gt;\n\n\nSome API responses are paginated (e.g. collection items), and you can simply iterate over them.\n\n# GET /stac-fastapi/collections/cmip6/items\nitems = cmip6.get_items()\n\n# Warning: this will take a long time for large catalogues such as cmip6\nfor item in items:\n    print(item.id)\n\n\n\nCreating, removing and updating collections\nAttempting to create a collection with id that already exists will result in 409 error code. To see the example in action delete the test collection first by running the cell below.\nDelete a colletion\n\nrc.get_collection(\"test1\").delete()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 rc.get_collection(\"test1\").delete()\n\nNameError: name 'rc' is not defined\n\n\n\nCreate new collection example\n\ntest1 = rc.create_collection(id=\"test1\", title=\"Test\", description=\"Test collection\")\nprint(test1.description)\n\nUpdate a collection\n\ntest1.update(description=\"Different description\")\nprint(test1.description)\n\n\n\nInteracting with items\nCreate an item\n\ntestitem1 = test1.create_item(id=\"test1.testitem1\")\nprint(f\"Created {testitem1.id} in collection {testitem1.collection}\")\n\nUpdate an item\n\ntestitem1.update(properties={\"foo\": \"bar\"})\nprint(testitem1.properties)\n\nDelete an item\n\ntestitem1.delete()\n\nFind out more about the Resource Catalog\n\nprint(f\"Livecheck: PING-{rc.ping()}\")\nprint(\"\\nAPI conforms to:\", *rc.get_conformance(), sep=\"\\n\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 print(f\"Livecheck: PING-{rc.ping()}\")\n      2 print(\"\\nAPI conforms to:\", *rc.get_conformance(), sep=\"\\n\")\n\nNameError: name 'rc' is not defined\n\n\n\nSearch the Catalog\n\nfor result in rc.search(collections=['cmip6']):\n    print(result.id)",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Using the Resource Catalogue"
    ]
  },
  {
    "objectID": "website/about.html",
    "href": "website/about.html",
    "title": "About",
    "section": "",
    "text": "The Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "website/about.html#using-this-website",
    "href": "website/about.html#using-this-website",
    "title": "About",
    "section": "Using this website",
    "text": "Using this website\nThe code in a given Jupyter notebook can either be copied and run on the Python command-line or in a script. The entire notebook can be downloaded and run wholly or stepwise through each cell. The code has been tested at the time of creation and will work with the required import packages.\nIf there are any issues running the code, please create an issue in the GitHub repository explaining the problem, post in the discussion forum or if you are able to fix the code then please create a pull request and add to these help pages.\nTo clone the repository you will git installed and to then use the following command in a suitable directory: git clone https://github.com/EO-DataHub/eodh-training.git\nThe notebooks can be run anywhere that a Jupyter server is available. To use on the AppHub, download the required notebooks locally and then upload them using the upload button at the top-left of the AppHub window.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "website/about.html#acronym-list",
    "href": "website/about.html#acronym-list",
    "title": "About",
    "section": "Acronym list",
    "text": "Acronym list\n\n\n\n\n\n\n\nAcronym\nMeaning\n\n\n\n\nADES\nApplication Deployment and Execution Service (the EODH workflow runner)\n\n\nARD\nAnalysis Ready Data (a pre-processed dataset)\n\n\nAWS\nAmazon Web Services\n\n\nCEDA\nCentre for Environmental Data Analysis\n\n\nCWL\nCommon Workflow Language (a specification for standardising workflows)\n\n\nEO\nEarth observation\n\n\nEODH\nEarth observation Data Hub\n\n\nWR\nWorkflow Runner\n\n\nYAML\nYet Another Markup Language (or YAML ain’t markup language)",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EODH Training Materials",
    "section": "",
    "text": "Welcome\nThis site is in heavy development.\nWelcome to the eodh-training repository! This repository aims to provide a live set of documents to demonstrate how to use the EODH (Earth Observation Data Hub) and associated tools such as the pyeodh Python API client, eoap-gen workflow generator and QGIS plugin. Other training materials may be added to this repository in future.\nWhether you’re a user looking to explore the project or a developer wanting to contribute, you’ll find all the information you need here.\n\n\nContent\nYou’ll be able to find the content you require by navigating through this website using the sidebar to the left.\n\n\nAccess to EODH\nThe EODH project is keen to hear from interested early adopters and potential users. If you would like to apply for a user account, or just find out more about the project, please use enquiries@eodatahub.org.uk to contact the core development team.\n\n\nAdditional Support\nWe have set up an accessible Discussion Group as a location that users can interact with other users, as well as the Hub owners and developers. Please use this resource to build a vibrant community around the EODH platform. It is also accessible via the GitHub icon at the top of every page.\nIf you require specific development information on the API client then please check out the ReadTheDocs page for the client.\n\nThis website is made using Quarto.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "api-client/1_ClientIntro.html",
    "href": "api-client/1_ClientIntro.html",
    "title": "What is pyeodh?",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, EODH\nDate created: 2024-11-08\nDate last modified: 2024-12-10\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWhat is pyeodh?\nAs part of the EODH project, pyeodh has been created. This is a lightweight Python client for easy access to EODH APIs.\nAn API client is a software tool or library designed to simplify interactions between a user’s application and an external API (Application Programming Interface). In the case of pyeodh, it is a Python-based tool tailored to facilitate communication with the specific API endpoints exposed by the EODH platform. Using pyeodh, developers and scientists can programmatically access the API’s features—such as sending requests, retrieving data, or executing commands—without needing to handle the underlying details such as crafting HTTP requests or managing authentication manually.\nBy abstracting these complexities, ‘pyeodh’ makes it easier to integrate the API into Python applications, enabling developers to focus on building features rather than managing low-level networking tasks.\n\n\nWhy is pyeodh needed?\nA key group of expected users are data scientists, and the key tools for this group tend to be written in Python. The pyeodh API client will simplify the interaction with the EODH platform allowing for more rapid, frictionless scientific development.\n\n\nBefore you start\nAccess to the EODH platform is largely free and open. However, in order to complete tasks on the workflow runner (WR) you will require an API token. To generate the token you will require a user account whch can be requested by contacting enquiries@eodatahub.org.uk. Once you have the account, login and navigate to ‘Workspaces’. Under the ‘Applications’ tab click on the ‘DataHub’ as shown in the screenshot below. From there you will be able to generate a new API token and manage other tokens that you have access to.",
    "crumbs": [
      "Simplifying access - pyeodh",
      "What is pyeodh?"
    ]
  },
  {
    "objectID": "api-client/3_WorkflowRunner.html",
    "href": "api-client/3_WorkflowRunner.html",
    "title": "ADD Oxidian link",
    "section": "",
    "text": "Description & purpose:\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-09-10\nDate last modified: 2024-12-05\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nADD Oxidian link\n\n\nWhat this does\nThis notebook demostrates usage of the EODH ADES API using pyeodh.\nEODH provides Application Deployment & Execution Service - ADES, to which you can deploy workflows and execute parametrised processing jobs. Pyeodh provides an interface to simplify interaction with ADES from python scripts.\nFirst we need to instantiate pyeodh client and get the ADES entrypoint.\nNote: This API requires authentication credentials to be provided by the user (in this case read from environment variables). This is a subject to change as the hub is implementing proper IAM solution.\n\nfrom requests import HTTPError\nimport os\nfrom pprint import pp\nfrom dotenv import load_dotenv\n\nimport pyeodh\n\nfrom pathlib import Path\n\ndotenv_path = Path('reqts/.env')\nload_dotenv(dotenv_path=dotenv_path)\n\n#load_dotenv()\nusername = os.getenv(\"ADES_USER\")\ntoken = os.getenv(\"ADES_TOKEN\")\n\n\nclient = pyeodh.Client(username=username, token=token)\nades = client.get_ades()\n\n\nProcesses (or workflows) are predefined applications which can be parametrised and executed by users. To get a list of currently available processes in our user workspace call Ades.get_processes() method:\n\n\nfor p in ades.get_processes():\n    print(p.id)\n\necho\nconvert-url\n\n\nYou can fetch a specific workflow if you know it’s ID using Ades.get_process() method. The Process object also contains metadata giving us more information about the process and how to execute it, for example the schema of inputs we can use to parametrise the process or output schema.\n\nconvert_url_proc = ades.get_process(\"convert-url\")\n\npp(convert_url_proc.inputs_schema)\npp(convert_url_proc.outputs_schema)\n\n{'fn': {'title': 'the operation to perform',\n        'description': 'the operation to perform',\n        'schema': {'type': 'string'}},\n 'size': {'title': 'the percentage for a resize operation',\n          'description': 'the percentage for a resize operation',\n          'schema': {'type': 'string'}},\n 'url': {'title': 'the image to convert',\n         'description': 'the image to convert',\n         'schema': {'type': 'string'}}}\n{'converted_image': {'title': 'converted_image',\n                     'description': 'None',\n                     'extended-schema': {'oneOf': [{'allOf': [{'$ref': 'http://zoo-project.org/dl/link.json'},\n                                                              {'type': 'object',\n                                                               'properties': {'type': {'enum': ['application/json']}}}]},\n                                                   {'type': 'object',\n                                                    'required': ['value'],\n                                                    'properties': {'value': {'oneOf': [{'type': 'object'}]}}}]},\n                     'schema': {'oneOf': [{'type': 'object'}]}}}\n\n\n\nfor j in ades.get_jobs():\n    print(j.id, j.process_id, j.status)\n\nOnly one process with the same ID can exist. To demonstrate deploying a process further down in this notebook, we first need to undeploy convert-url. Note that attempting to delete a non-existent process will result in 4xx http status code.\n\ntry:\n    ades.get_process(\"convert-url\").delete()\nexcept HTTPError:\n    print(\"Process not found, no need to undeploy.\")\n\nLet’s deploy the convert-url process again. There are 2 ways we can provide the CWL file - either referencing the file by URL or by passing the CWL file content directly. Note that Ades.deploy_process() will fail if we try to create a process with ID that already exists. If we want to update an existing process, we should use Process.update() method instead. Both methods can handle URL or CWL YAML. In this example we deploy a process referencing by URL and then update it by passing the new CWL YAML content directly. Also note that when updating a worklow you need to provide the entire workflow, the API does not support partial updates (e.g. to change the description we need to provide the entire workflow again).\n\nconvert_url_proc = ades.deploy_process(\n    cwl_url=\"https://raw.githubusercontent.com/EOEPCA/deployment-guide/main/deploy/samples/requests/processing/convert-url-app.cwl\"\n)\nprint(convert_url_proc.id, convert_url_proc.description)\n\nconvert-url Convert URL\n\n\n\ncwl_yaml = \"\"\"cwlVersion: v1.0\n$namespaces:\n  s: https://schema.org/\ns:softwareVersion: 0.1.2\nschemas:\n  - http://schema.org/version/9.0/schemaorg-current-http.rdf\n$graph:\n  # Workflow entrypoint\n  - class: Workflow\n    id: convert-url\n    label: convert url app\n    doc: Convert URL YAML\n    requirements:\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 1024\n    inputs:\n      fn:\n        label: the operation to perform\n        doc: the operation to perform\n        type: string\n      url:\n        label: the image to convert\n        doc: the image to convert\n        type: string\n      size:\n        label: the percentage for a resize operation\n        doc: the percentage for a resize operation\n        type: string\n    outputs:\n      - id: converted_image\n        type: Directory\n        outputSource:\n          - convert/results\n    steps:\n      convert:\n        run: \"#convert\"\n        in:\n          fn: fn\n          url: url\n          size: size\n        out:\n          - results\n  # convert.sh - takes input args `--url`\n  - class: CommandLineTool\n    id: convert\n    requirements:\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: eoepca/convert:latest\n    baseCommand: convert.sh\n    inputs:\n      fn:\n        type: string\n        inputBinding:\n          position: 1\n      url:\n        type: string\n        inputBinding:\n          position: 2\n          prefix: --url\n      size:\n        type: string\n        inputBinding:\n          position: 3\n    outputs:\n      results:\n        type: Directory\n        outputBinding:\n          glob: .\n\"\"\"\n\nconvert_url_proc.update(cwl_yaml=cwl_yaml)\nprint(convert_url_proc.id, convert_url_proc.description)\n\nconvert-url Convert URL YAML\n\n\nLet’s execute our deployed process. We need to provide inputs as a dictionary in this format, see Process.inputs_schema property for inputs this particular workflow is expecting.\n\nconvert_url_job = convert_url_proc.execute(\n    {\n        \"fn\": \"resize\",\n        \"url\": \"https://eoepca.org/media_portal/images/logo6_med.original.png\",\n        \"size\": \"50%\",\n    }\n)\n\nprint(convert_url_job.id, convert_url_job.status, convert_url_job.message)\n\n672f0742-b6f5-11ef-a8b0-6a040e2afd6f running ZOO-Kernel accepted to run your service!\n\n\nThe job should now be running. Call Job.refresh() method to get the most up-to-date status and interrogate Job.status and Job.message properties. Note that these properties only hold the latest response from the API, and don’t keep any historical records.\n\nconvert_url_job.refresh()\nprint(convert_url_job.id, convert_url_job.status, convert_url_job.message)\n\n\nWe can continually poll the job using a simple loop and print status and message udpates like so:\n\nfrom pyeodh.ades import AdesJobStatus\nimport time\n\n\nold_status = \"\"\nold_message = \"\"\nwhile convert_url_job.status == AdesJobStatus.RUNNING.value:\n    time.sleep(2)\n    convert_url_job.refresh()\n    if convert_url_job.status != old_status:\n        print(\"\\n\")\n        print(f\"Status: {convert_url_job.status}\")\n    if convert_url_job.message != old_message:\n        print(f\"Message: {convert_url_job.message}\")\n\n    old_status = convert_url_job.status\n    old_message = convert_url_job.message\n\n\nAfter the job has finished successfully, we can view the results, where the data files are referenced by assets\n\nresults = convert_url_job.get_result_items()\nfor res in results:\n    print(res.id, res.assets)",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Running Workflows"
    ]
  },
  {
    "objectID": "api-client/planet-stac-proxy.html",
    "href": "api-client/planet-stac-proxy.html",
    "title": "Remove in production release",
    "section": "",
    "text": "This notebook demostrates usage of the EODH resource catalog API using pyeodh\n\nEnsure\n\na clear description of purpose,\nintended audience and/or use case\nlinkages between notebooks and other training resources (if required)\n\n\n\nRecord\n\nTechnical dependencies,\nPlatform and Service Dependencies,\nPython Language versions,\nlibraries, additional scripts and files.\n\n\n\nRemember\n“When Jupyter notebooks are used in an educational context, they should not only be conceptualized to teach a specific topic but should also set a good example by following and implementing best practices for scientific computing”\n\nNeed in-order execution of notebook cells\nGood-quality code\nNo code duplication\nImports at the beginning of a notebook\nConsistent code style and formatting\nMeaningful names for variables\nLicence for code and training resources\n\n\n\n\nPlanet STAC Example\nDemo of retrieving data from Planet via their STAC proxy api using the PySTAC Client. Requires an api-key saved in as .planet.json which has the content {\"key\": \"API_KEY\"}.\n\nimport base64\nfrom pystac_client import Client\nimport requests\nfrom shapely import Point\nimport planet\nfrom IPython.display import Image\n\napi_key = planet.Auth.from_file().value\n\nuserpass = f\"{api_key}:\"\n\nheaders = {\n    \"Authorization\": f\"Basic {base64.b64encode(userpass.encode()).decode()}\"\n}\n\n\nclient = Client.open(\n    url=\"https://api.planet.com/x/data/\",\n    headers=headers\n)\n\nitem_search = client.search(\n    collections=['PSScene'],\n    datetime='2024-06-01/2024-08-01',\n    intersects=Point(-111, 45.68),\n    filter={\n        \"op\": \"&gt;=\",\n        \"args\": [{\"property\": \"clear_percent\"}, 50]\n    }\n)\n\nitem = next(item_search.items())\nthumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\nImage(data=thumbnail.content)\n\n\n\n\n\n\n\n\n\nimport pyeodh\nimport base64\nfrom io import BytesIO\nfrom pystac_client import Client\nimport requests\nfrom shapely import Point\nimport planet\nfrom IPython.display import Image\n\napi_key = planet.Auth.from_file().value\n\nuserpass = f\"{api_key}:\"\n\nheaders = {\n    \"Authorization\": f\"Basic {base64.b64encode(userpass.encode()).decode()}\"\n}\n\nclient = pyeodh.Client(username=api_key).get_catalog_service()\nitem_search = client.search(\n    collections=['PSScene'],\n    datetime='2024-06-01/2024-08-01',\n    intersects=Point(-111, 45.68),\n    filter={\n        \"op\": \"&gt;=\",\n        \"args\": [{\"property\": \"clear_percent\"}, 50]\n    }\n)\n\nitem = item_search[0]\nthumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\nImage(data=thumbnail.content)\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[10], line 29\n     18 client = pyeodh.Client(username=api_key).get_catalog_service()\n     19 item_search = client.search(\n     20     collections=['PSScene'],\n     21     datetime='2024-06-01/2024-08-01',\n   (...)\n     26     }\n     27 )\n---&gt; 29 item = item_search[0]\n     30 thumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\n     31 Image(data=thumbnail.content)\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:82, in PaginatedList.__getitem__(self, index)\n     80 def __getitem__(self, index: int) -&gt; T:\n     81     assert isinstance(index, int)\n---&gt; 82     self._fetch_to_index(index)\n     83     return self._elements[index]\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:87, in PaginatedList._fetch_to_index(self, index)\n     85 def _fetch_to_index(self, index: int) -&gt; None:\n     86     while len(self._elements) &lt;= index and self._has_next():\n---&gt; 87         new_elements = self._fetch_next()\n     88         self._elements += new_elements\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:96, in PaginatedList._fetch_next(self)\n     94 if not self._next_url:\n     95     raise RuntimeError(\"Next url not specified!\")\n---&gt; 96 headers, resp_data = self._client._request_json(\n     97     self._method,\n     98     self._next_url,\n     99     headers=self._headers,\n    100     params=self._params,\n    101     data=self._data,\n    102 )\n    103 next_link = next(\n    104     filter(lambda ln: ln.get(\"rel\") == \"next\", resp_data.get(\"links\", {})), {}\n    105 )\n    106 self._next_url = next_link.get(\"href\")\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/client.py:102, in Client._request_json(self, method, url, headers, params, data, encode)\n     93 def _request_json(\n     94     self,\n     95     method: RequestMethod,\n   (...)\n    100     encode: Callable[[Any], tuple[str, Any]] = _encode_json,\n    101 ) -&gt; tuple[Headers, Any]:\n--&gt; 102     status, resp_headers, resp_data = self._request_json_raw(\n    103         method, url, headers, params, data, encode\n    104     )\n    106     if not len(resp_data):\n    107         return resp_headers, None\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/client.py:89, in Client._request_json_raw(self, method, url, headers, params, data, encode)\n     82 logger.debug(\n     83     f\"Received response {response.status_code}\\nheaders: {response.headers}\"\n     84     f\"\\ncontent: {response.text}\"\n     85 )\n     86 # TODO consider moving this to _requst_json() and raise own exceptions\n     87 # so that we can user _raw in e.g. delete methods where we expect a 409 and\n     88 # want to recover\n---&gt; 89 response.raise_for_status()\n     91 return response.status_code, response.headers, response.text\n\nFile /opt/jaspy/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n   1019     http_error_msg = (\n   1020         f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n   1021     )\n   1023 if http_error_msg:\n-&gt; 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 400 Client Error: Bad Request for url: https://test.eodatahub.org.uk/api/catalogue/stac/search"
  },
  {
    "objectID": "platform/2_Components.html",
    "href": "platform/2_Components.html",
    "title": "Web Presence",
    "section": "",
    "text": "Description & purpose: This webpage is designed to provide an introduction to the core components of the EODH.\nAuthor(s): Alastair Graham, EODH\nDate created: 2024-12-05\nDate last modified: 2024-12-05\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWeb Presence\nThe Web Presence (WP) is likely to be the route most new users take to interact with the EODH\n\n\nResource Catalogue\nThe Resource Catalogue (RC) is a searchable store of data links and a way to access saved workflows and processed datasets.\n\n\nAppHub and Notebook Service\nThe AppHub is a JupyterHub instance made available to users to ensure simple access to information held within the RC. It also allows the creation of scientific analytical workflows and associated data processing.\n\n\nWorkflow Runner\nThe workflow runner (WR) is a component that the majority of users will not have direct access to but which will be important to any process looking to scale up data processing to generate a data service.\n\n\nMap Viewer\nThe Map Viewer (MV) provides a simple way of visualising OGC web services.\nTO DO is this going to be available in the initial release?",
    "crumbs": [
      "Hub Platform",
      "EODH Platform Components"
    ]
  },
  {
    "objectID": "presentations/DEFRA/1_202409_Defra_DataDiscovery.html",
    "href": "presentations/DEFRA/1_202409_Defra_DataDiscovery.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment\n\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh geopandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s data discovery\nThere are a number of API endpoints that are exposed by the EODH. Oxidian have developed a Python API Client, pyeodh, that makes the Hub’s API endpoints available to Python users. pyeodh is available on PyPi (https://pypi.org/project/pyeodh/) and can be installed using pip. Documentation for the API Client is available at: https://pyeodh.readthedocs.io/en/latest/api.html\nWe will use pyeodh throughout this presentation.\n\n# Imports\nimport pyeodh\n\nimport os\n\nimport shapely \nimport geopandas as gpd\nimport folium\n\nimport urllib.request\nfrom io import BytesIO \nfrom PIL import Image\n\nHaving imported the necessary libraries the next task is to set up the locations of the areas of interest. Having created the AOI points the user needs to connect to the Resource Catalogue so that they can start to find some data.\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\n\n# Optional cell\n# If you want to see these points on a map run this cell\n\n# Create a map (m) centered between the two points\ncenter_lat = (rut_pnt.y + thet_pnt.y) / 2\ncenter_lon = (rut_pnt.x + thet_pnt.x) / 2\n\nm = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n\n# Add markers for each point\nfolium.Marker([rut_pnt.y, rut_pnt.x], popup=\"Rutland Site\", icon=folium.Icon(color=\"blue\")).add_to(m)\nfolium.Marker([thet_pnt.y, thet_pnt.x], popup=\"Thetford Site\", icon=folium.Icon(color=\"green\")).add_to(m)\n\n# Step 4: Display the map\nm\n\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"{collect.id}: {collect.description}\")\n\ncmip6: CMIP6\ncordex: CORDEX\nukcp: UKCP\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\nairbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\nairbus_data_example: Airbus data\nsentinel2_ard: sentinel 2 ARD\nsentinel1: Sentinel 1\nnaip: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n\n\n\n# The next thing to do is find some open data\n# For this presentation we want to find Sentinel-2 analysis ready (ARD) imagery near Rutland\n\n# First we just want to understand some of the parameters linked to the data collection\n# We will just print the first 5 records and the dataset temporal extent   \nsentinel2_ard = client.get_catalog(\"supported-datasets/ceda-stac-fastapi\").get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\nlim = 5\ni = 0\n\nfor item in sentinel2_ard.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('DATASET TEMPORAL EXTENT: ', [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb\nDATASET TEMPORAL EXTENT:  ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# To find out information about all the imagery in the collection then use this cell\n# It undertakes a search for specific date ranges (November 2023) and limits the pagination return to 10\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n    ],\n    limit=10,\n)\n\n# The item id and start time of image capture can be printed\n# If end time is also required, add the following code to the print statement: item.properties[\"end_datetime\"]  \nfor item in item_search:\n    print(item.id, item.properties[\"start_datetime\"])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65 2023-11-21T11:43:49+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn527lonw0007_T30UXD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0037_T30UVC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0022_T30UWC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn518lonw0008_T30UXC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn510lonw0036_T30UVB_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.18.S2B_20231118_latn554lonw0053_T30UUG_ORB080_20231118122250_utm30n_osgb 2023-11-18T11:33:19+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn519lone0023_T31UDT_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn509lone0009_T31UCS_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\n\n\n\n# To find specific imagery for the Rutland site we need to add the intersects parameter. We set this to be our AOI point.\n# We can also filter the search by cloud cover, in this case limiting our search to images with less than 50% cloud in them\n\nitems = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n        'Cloud Coverage Assessment&lt;=50.0'\n    ],\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of items found: ', items.total_count)\n\nNumber of items found:  2\n\n\n\n# For the purposes of this presentation we will look at the second record ([1]) in more detail\n# First we need to understand what information we can access\n\na = items[1].to_dict()\nprint(a.keys())\n\ndict_keys(['type', 'stac_version', 'id', 'properties', 'geometry', 'links', 'assets', 'bbox', 'stac_extensions', 'collection'])\n\n\n\n# The data we want to access is stored under the 'assets' key. But what information is held in that? \nfor key in (a['assets']):\n    print(key)\n\ncloud\ncloud_probability\ncog\nmetadata\nsaturated_pixels\nthumbnail\ntopographic_shadow\nvalid_pixels\n\n\n\n# Now we can get the link to each of the different assets\nfor key, value in items[1].assets.items():\n    print(key, value.href)\n\ncloud https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds.tif\ncloud_probability https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds_prob.tif\ncog https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif\nmetadata https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml\nsaturated_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_sat.tif\nthumbnail https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_thumbnail.jpg\ntopographic_shadow https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_toposhad.tif\nvalid_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_valid.tif\n\n\n\n# We can use this information to view the image thumbnail\n\nasset_dict = items[1].assets\n\n# Get the url as a string\nthumbnail_assets = [v for k, v in asset_dict.items() if 'thumbnail' in k]\nthumbnail_url = thumbnail_assets[0].href\n\n# Here we open the remote URL, read the data and dislay the thumbnail \nwith urllib.request.urlopen(thumbnail_url) as url:\n    img = Image.open(BytesIO(url.read()))\n\ndisplay(img)\n\n\n\n\n\n\n\n\nThis shows that we can relatively easily interrogate the Resource Catalogue and filter the results so that we can find the data we require in the EODH. With a bit of tweaking of the code the user could also generate a list of assets and accompanying URLs to the datasets (for this and other datasets).\nNow our user wants to see what commercial data exists for the Thetford site.\n\n# Find some commercial data\n\nfor collect in client.get_collections():\n    if 'defra' in collect.id: \n        print(f\"{collect.id}: {collect.description}\")\n\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\n\n\n\n# Let's search for information on the Planet holdings  \nplanet = client.get_catalog(\"supported-datasets/defra\").get_collection('defra-planet')\nplanet.get_items()\n\nlim = 5\ni = 0\n\nfor item in planet.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('PLANET DATASET TEMPORAL EXTENT: ', [str(d) for d in planet.extent.temporal.intervals[0]])\n\n2024-08-23_strip_7527622_composite\n2024-08-23_strip_7527462_composite\nPLANET DATASET TEMPORAL EXTENT:  ['2024-08-23 11:09:19.358417+00:00', '2024-08-23 11:24:40.991786+00:00']\n\n\n\n# To find specific imagery for the Thetford site we need to add the intersects parameter. We set this to be our AOI point.\nitems1 = client.search(\n    collections=['defra-planet'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\nitems2 = client.search(\n    collections=['defra-airbus'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of Planet items found: ', items1.total_count)\nprint('Number of Airbus items found: ', items2.total_count)\n\nNumber of Planet items found:  1\nNumber of Airbus items found:  2\n\n\n\nfor key, value in items1[0].assets.items():\n    print('Planet: ', key, value)\n\nPlanet:  data &lt;Asset href=2024-08-23_strip_7527462_composite_file_format.tif&gt;\nPlanet:  udm2 &lt;Asset href=2024-08-23_strip_7527462_composite_udm2_file_format.tif&gt;\n\n\nThe final step would be to use the ordering service integrated into the EODH resource catalogue to purchase the required commercial imagery. This would be stored in a users workspace and could then be used in specific workflows or for data analytics (depending on licence restrictions).\nFor the purposes of this presentation we looked at the different commercial datasets offline in QGIS.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Discovery"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "title": "Demonstration for DEFRA",
    "section": "a) Viewing images with specific band configurations",
    "text": "a) Viewing images with specific band configurations\nOnce you have the URL to the Cloud Optimised Geotiff from the STAC records held in the resource catalogue it is possible to view different band combinations. The following code uses the URL to the Sentinel 2 ARD image discovered in the Data Discovery notebook and displays an interactive false colour composite.\n\n# Path to raster (URL or local path)\ndata_url = 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif'\n\n# Check that the dataset is a valid COG. If invalid, returns False\nvalidate_cog(data_url)\n\nTrue\n\n\n\n\n# First, create TileClient using example file\ndclient = TileClient(data_url)\n\n# Create 2 tile layers from same raster viewing different bands\nl = get_leaflet_tile_layer(dclient, indexes=[6, 2, 1])\n\n# Make the ipyleaflet map\nm = Map(center=dclient.center(), zoom=dclient.default_zoom)\nm.add(l)\nm.add_control(ScaleControl(position='bottomleft'))\nm.add_control(FullScreenControl())\nm\n\n\n\n\nimage.png\n\n\n\n# we can use the same tool to view rendered data stored locally e.g. in a user's workspace.\nmap = TileClient('data/S2A_clip_rend.tif')\nmap\n\n\n\n\nimage.png",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#b-towards-a-data-cube",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#b-towards-a-data-cube",
    "title": "Demonstration for DEFRA",
    "section": "b) Towards a data cube",
    "text": "b) Towards a data cube\nThis is very much work in progress, and follows the outline of the tutorial supplied here. The tutorial at that link is designed to create a data cube in the USA using data held in an Element 84 STAC catalogue. Here we alter the processing to make use of the CEDA STAC catalogue. Ultimately both of these datasets will be held within the EODH resource catalogue and this code will be rewritten to utilise pyeodh and the EODH data holdings.\n\n# Set up\n# A helper method for changing bounding box representation to leaflet notation\n# (lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))\n\ndef convert_bounds(bbox, invert_y=False):\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\nThe next thing the user needs to do is find some data in a STAC catalogue that intersects with the site of interest for a given time period. Therefore, we need to know what datasets are available, over what period and for what locations.\n\n# Find some data using STAC\n# note: here we are using `pystac`. This will be replaced by `pyeodh` in future. \n\nurl = \"https://api.stac.ceda.ac.uk/\"\n\nclient = Client.open(url)\nfor coll in client.get_collections():\n    print(f\"{coll.id}: {coll.description}\")\n\nsentinel2_ard = client.get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\n# check the spatial and temporal extent of the collection\nprint('')\nprint(\"spatial extent:\", sentinel2_ard.extent.spatial.bboxes)\nprint(\"Temporal range:\", [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\ncmip6: CMIP6\ncordex: CORDEX\nland_cover: land_cover\nsentinel1: Sentinel 1\nsentinel2_ard: sentinel 2 ARD\nsst-cdrv3-collection: collection of EOCIS SST CDR V3\nukcp: UKCP\n\nspatial extent: [[-9.00034454651177, 49.48562028352171, 3.1494256015866995, 61.33444247301668]]\nTemporal range: ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# Find imagery that intersects with the site in Rutland \n# Uncomment max_items to limit the seatch to a set number of items\n\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-04-01',\n        'end_datetime&lt;=2023-10-01',\n        'eo:cloud_cover&lt;=75.0'\n      ],\n    # max_items=10,\n)\n\nitems = list(item_search.items())\nlen(items)\n\n30\n\n\nNow that we know we have available data we can start to build out a data cube.\n\n# First we create a dask client\n\nclientd = dask.distributed.Client()\nconfigure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client) # sets up gdal for cloud use\ndisplay(clientd)\n\n\nprint(f\"Found: {len(items):d} datasets\")\n\n# Convert STAC items into a GeoJSON FeatureCollection\nstac_json = item_search.item_collection_as_dict()\n\nFound: 30 datasets\n\n\n\ngdf = gpd.GeoDataFrame.from_features(stac_json, \"epsg:4326\")\ngdf.columns\n\nIndex(['geometry', 'file_count', 'start_datetime', 'end_datetime',\n       'NSSDC Identifier', 'created', 'Instrument Family Name',\n       'Platform Number', 'Datatake Type', 'esa_file_name',\n       'Ground Tracking Direction', 'datetime', 'instance_id', 'size',\n       'Product Type', 'Instrument Family Name Abbreviation',\n       'Start Orbit Number', 'eo:cloud_cover', 'Start Relative Orbit Number',\n       'updated', 'Instrument Mode', 'EPSG'],\n      dtype='object')\n\n\n\n# Plot an outline of the data items as a sense check\nf = folium.Figure(width=600, height=300)\nm = folium.Map(location=[52, 2], zoom_start=5).add_to(f)\n\ngdf.explore(\n    \"esa_file_name\",\n    categorical=True,\n    tooltip=[\n        \"esa_file_name\",\n        \"datetime\",\n        \"eo:cloud_cover\",\n    ],\n    popup=False,\n    legend=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"STAC\",\n    m=m,\n)\n\n\n\n\n\n# Construct the dask dataset\n\n# Since we will plot it on a map we need to use `EPSG:3857` projection\ncrs = \"epsg:3857\"\nzoom = 2**5  # overview level 5\n\nxx = stac_load(\n    items,\n    crs=crs,\n    resolution=10 * zoom,\n    chunks={\"x\": 2048, \"y\": 2048},  # &lt;-- use Dask\n    groupby=\"solar_day\",\n)\n\nprint(f\"Bands: {','.join(list(xx.data_vars))}\")\n#display(xx)\n\nBands: cloud,cloud_probability,thumbnail,topographic_shadow,cog,valid_pixels,saturated_pixels\n\n\nWe are currently looking into workarounds for a known conflict of using the eo STAC extension. Once a suitable way forward is identiified a full description of how to create and manipulate the CEDA Sentinel 2 ARD in a data cube (generated from STAC records) will be posted in the training materials repository.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "science/03_NetcdfData.html",
    "href": "science/03_NetcdfData.html",
    "title": "EODH Training Materials",
    "section": "",
    "text": "https://github.com/cedadev/stac-notebooks/blob/main/stac-api-example.ipynb\n\n# Import the Python API Client\nimport pyeodh\n\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"- {collect.id}: {collect.description}\")\n\n- cmip6: CMIP6\n- cordex: CORDEX\n- ukcp: UKCP\n- airbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\n- defra-airbus: A collection of Airbus data for the DEFRA use case.\n- defra-planet: A collection of Planet data for the DEFRA use case.\n- eocis-sst-cdrv3: EOCIS Sea-Surface Temperatures V3\n- sentinel2_ard: sentinel 2 ARD\n- sentinel1: Sentinel 1\n- naip: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n\n\n\ncmip6 = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\").get_collection('cmip6')\n\ncmip6.extent.to_dict()\n\nitem_search = client.search(\n    collections=['cmip6'],\n    query=[\n        'start_datetime&lt;=2023-01-01',\n        'end_datetime&gt;=2023-02-28',\n        'experiment_id=ssp585',\n    ],\n    limit=10,\n)\n\n\nitem_search\n\n&lt;pyeodh.pagination.PaginatedList at 0x7dd003ff1400&gt;"
  },
  {
    "objectID": "science/2_Commercial.html",
    "href": "science/2_Commercial.html",
    "title": "EODH Training Materials",
    "section": "",
    "text": "# test\nimport math",
    "crumbs": [
      "Science & code examples",
      "Commercial data"
    ]
  },
  {
    "objectID": "workflows/2_EOAP.html",
    "href": "workflows/2_EOAP.html",
    "title": "What are Earth Observation Application Packages?",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWhat are Earth Observation Application Packages?\nEOAPs are self contained amalgamations of tools and code that allows an algorithm, or set of algorithms, to be shared between and run on different data processing platforms. The core concepts behind this are that: * algorithms and processing methods are not tied to a specific platform * the underlying infrastructure is not important as long as a suitable workflow runner exists on that infrastructure * the workflows preferentially create data services\n\n\nWhy containers?\nContainers solve the need for dependencies between software required to run specific commands. Containers are described by an image file (a template), where the container itself is an instantiation of the image.\n\n\nOther information\nAdditional information on the EOAP specification and construction is available here: https://eoap.github.io/mastering-app-package/. This is a comprehensive tutorial that introduces the user to different tools (some of which are mentioned elsewhere in these training materials). Much of the tutorial uses hand-crafted code which is generally not recommended as a method of generation (see notes on the EOAP Generator for further details) but in this case it does enable detailed assesment of the different components. A full set of resources are available from the official EOAP Github repository: https://github.com/eoap. Further to this, a related ‘hands on’ tutorial is available here.\ngdal example: https://github.com/EO-DataHub/eodhp-ades-workspace-access/blob/main/gdal-example/gdal-workflow-test.cwl\nMore examples: https://github.com/EO-DataHub/eodhp-ades-demonstration/blob/main/eodhp-ades-demonstration.ipynb\n\n\nLinks\n\nMy first CWL workflow: explainer\nEO Application Package example: Waterbodies\nOGC best practices: EO app pkg\nCWL for EO tutorials: adding conditions\nTerradue training: hands-on - not sure all the scripts work as they are supposed to…\nEO App Pkgs: overview, examples",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "EO App Packages"
    ]
  },
  {
    "objectID": "workflows/4_UserExample1.html",
    "href": "workflows/4_UserExample1.html",
    "title": "User Example 1: coherence",
    "section": "",
    "text": "Description & purpose: TODO\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nBackground\nA requirement to process coherence from Sentinel 1 data was articulated.\n\n\nThe process\nsome of the packages and confgurations that are required e.g. yaml file and account on copernicus\n\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nimport json\nfrom itertools import combinations\n\nimport eo_tools as eo\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport rioxarray as riox\nimport shapely\nfrom eo_tools.S1.process import process_insar\nfrom eodag import EODataAccessGateway\nfrom shapely.geometry import shape\n\nimport pyeodh\n\nINFO:eodag.config:Loading user configuration from: /home/al/.config/eodag/eodag.yml\nINFO:eodag.core:usgs: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:aws_eos: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:meteoblue: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:hydroweb_next: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:wekeo: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:creodias_s3: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:Locations configuration loaded from /home/al/.config/eodag/locations.yml\n\n\n\n# location\nbbox = {\n    \"lonmin\": 0.08905898091569497,\n    \"latmin\": 52.15527412683906,\n    \"lonmax\": 0.9565339502005088,\n    \"latmax\": 52.69722175598818,\n}\nshp = shapely.box(bbox[\"lonmin\"], bbox[\"latmin\"], bbox[\"lonmax\"], bbox[\"latmax\"])\n\n\n\nCopernicus method\n\ndag = EODataAccessGateway()\n# make sure cop_dataspace will be used\ndag.set_preferred_provider(\"cop_dataspace\")\nlogging.basicConfig(level=logging.INFO)\n\n\n# Run search\n\nsearch_criteria = {\n    \"productType\": \"S1_SAR_SLC\",\n    \"start\": \"2023-09-03\",\n    \"end\": \"2023-09-17\",\n    \"geom\": bbox,\n}\n\nresults, _ = dag.search(**search_criteria)\n\nINFO:eodag.core:Searching product type 'S1_SAR_SLC' on provider: cop_dataspace\nINFO:eodag.search.qssearch:Sending search request: http://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2023-09-03&completionDate=2023-09-17&geometry=POLYGON ((0.0891 52.1553, 0.0891 52.6972, 0.9565 52.6972, 0.9565 52.1553, 0.0891 52.1553))&productType=SLC&maxRecords=20&page=1&exactCount=1\nINFO:eodag.core:Found 7 result(s) on provider 'cop_dataspace'\n\n\n\n# Find overlaps\ndata = []\nfor item in results:\n    id = item.properties[\"id\"]\n    geom = shape(item.geometry)\n    data.append({\"id\": id, \"geometry\": geom})\n\ngdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")  # Assuming WGS84\n\n# 98% overlap\nthreshold = 0.98\n\noverlaps = []\nfor (idx1, row1), (idx2, row2) in combinations(gdf.iterrows(), 2):\n    intersection = row1[\"geometry\"].intersection(row2[\"geometry\"])\n    if not intersection.is_empty:\n        # Calculate overlap ratio as the area of intersection divided by the area of the smaller polygon\n        overlap_ratio = intersection.area / min(\n            row1[\"geometry\"].area, row2[\"geometry\"].area\n        )\n        if overlap_ratio &gt;= threshold:\n            overlaps.append((row1[\"id\"], row2[\"id\"], overlap_ratio))\n\noverlap_ids = [entry[:-1] for entry in overlaps]\noverlap_ids\n\n[('S1A_IW_SLC__1SDV_20230904T174209_20230904T174236_050181_060A2D_E88F',\n  'S1A_IW_SLC__1SDV_20230916T174209_20230916T174236_050356_061016_8033'),\n ('S1A_IW_SLC__1SDV_20230904T174144_20230904T174211_050181_060A2D_46E8',\n  'S1A_IW_SLC__1SDV_20230916T174145_20230916T174212_050356_061016_9D13')]\n\n\n\n# Set download dirs\ndata_dir = \"/home/al/Downloads/eotools\"\n\nids = [\n    \"S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C\",\n    \"S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046\"\n]\n    #\"S1A_IW_SLC__1SDV_20230904T174209_20230904T174236_050181_060A2D_E88F\",\n    #\"S1A_IW_SLC__1SDV_20230916T174209_20230916T174236_050356_061016_8033\",\n#]\n\nprimary_dir = f\"{data_dir}/{ids[0]}.zip\"\nsecondary_dir = f\"{data_dir}/{ids[1]}.zip\"\noutputs_prefix = f\"{data_dir}/res/test-full-processor\"\n\n\n# AOI around Thetford\nfile_aoi = f\"{data_dir}/thetfordaoi.geojson\"\nshp2 = gpd.read_file(file_aoi).geometry[0]\n\n\neo.util.explore_products(results, shp2)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nGenerating the workflow\n\n\nAdditional information\nAirbus ONEATLAS DEVELOPER PORTAL https://www.geoapi-airbusds.com/index.html\nHigh Resolution SAR data: https://intelligence.airbus.com/imagery/our-optical-and-radar-satellite-imagery/radar-constellation/ Interferometry case studies: https://intelligence.airbus.com/search/?q=interferometry",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: coherence"
    ]
  }
]