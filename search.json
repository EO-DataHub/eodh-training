[
  {
    "objectID": "workflows/5_UserExample2.html",
    "href": "workflows/5_UserExample2.html",
    "title": "User Example: Temporal Mosaics",
    "section": "",
    "text": "Description & purpose: This Notebook introduces the science behind a user example looking to create temporal best pixel temporal mosaics using the Sentinel 2 ARD dataset. It also introduces the EOAP that has been created to run a scaled workflow on EODH.\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: temporal mosaics"
    ]
  },
  {
    "objectID": "workflows/5_UserExample2.html#scientific-method",
    "href": "workflows/5_UserExample2.html#scientific-method",
    "title": "User Example: Temporal Mosaics",
    "section": "Scientific method",
    "text": "Scientific method\nFirst we need to import the required packages.\n\nimport pyeodh\nimport xarray as xr\nimport rioxarray\nimport os\nfrom pathlib import Path\nimport requests\nimport threading\n\n\n# set the area of interest\n\nthetford_aoi = {\n    \"coordinates\": [\n        [\n            [0.08905898091569497, 52.69722175598818],\n            [0.08905898091569497, 52.15527412683906],\n            [0.9565339502005088, 52.15527412683906],\n            [0.9565339502005088, 52.69722175598818],\n            [0.08905898091569497, 52.69722175598818],\n        ]\n    ],\n    \"type\": \"Polygon\",\n}\n\nNext we need to connect to the resource catalogue and undertake a search of the sentinel2_ard catalogue for the AOI location within a specific date range\n\n# connect to \nrc = pyeodh.Client().get_catalog_service()\n\nitems = rc.search(\n    collections=[\"sentinel2_ard\"],\n    catalog_paths=[\"supported-datasets/ceda-stac-catalogue\"],\n    intersects=thetford_aoi,\n    query=[\n        \"start_datetime&gt;=2023-04-01\",\n        \"end_datetime&lt;=2023-06-30\",\n    ],\n    limit=1,\n)\n\nNext, for each item open the cog, cloud and valid .tif assets and download them if needed.\n\nfor item in items.get_limited()[0:1]:\n    cloud_href = item.assets[\"cloud\"].href\n    valid_href = item.assets[\"valid_pixels\"].href\n    cog_href = item.assets[\"cog\"].href\n    item\n    print(item.id, cloud_href, valid_href, cog_href, sep=\"\\n\")\n    valid = rioxarray.open_rasterio(\n        valid_href,\n        chunks=True,\n    )\n    cloud = rioxarray.open_rasterio(\n        cloud_href,\n        chunks=True,\n    )\n\n    # Check if cog file exists locally, if not download it\n    cog_filename = f\"data/{Path(cog_href).name}\"\n    if not os.path.isfile(cog_filename):\n        with requests.get(cog_href, stream=True) as r:\n            r.raise_for_status()\n            with open(cog_filename, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n    cog = rioxarray.open_rasterio(cog_filename, chunks=True)\n    print(\"==\" * 10)\n    print(\"success\")\n    break\n\nThe next steps are to create a definitive mask using the amalgamation of the valid pixels layer and the cloud mask layer.\n\nresult = valid + cloud\n\n# Set values greater than 1 to 0, others remain as 1\nresult = xr.where(result &gt; 1, 0, 1)\n\nIf we now multiply the cloud optimised geotif file by the result maskfile, expanded to match the shape of cog then each of the bands will have the clouds removed. This is a brute force approach for the purposes of generating a demonstrable workflow and in an operational context a user would likely want to undertand the impact of cloud in individual bands and use specific methods to remove that.\nFinally the output is saved to a file\n\n# Multiply cog.tif by the result\n# We need to expand the result to match the shape of cog\nfin = cog * result.squeeze(\"band\").expand_dims(band=cog.band)\n\n# save to a file\nfin.rio.to_raster(raster_path=f\"data/rm_cloud.tif\", tiled=True, lock=threading.Lock())\n\nAs mentioned, one method of generating the composite is to use pktools. It’s an older code, but it checks out. An alternative using Python might be to use xarray and the functions within that package. pktools works well for this use case as the command can be inserted directly\nhttps://pktools.nongnu.org/html/pkcomposite.html",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: temporal mosaics"
    ]
  },
  {
    "objectID": "workflows/5_UserExample2.html#outputs",
    "href": "workflows/5_UserExample2.html#outputs",
    "title": "User Example: Temporal Mosaics",
    "section": "Outputs",
    "text": "Outputs\nThe following images provide an indication of how the pkcomposite tool works. The first image shows three input images (single band in this instance, coloured red, green and blue for clarity) that overlap a specific area of interest. The AOI is marked by the red boundary.\n\nThe second image plots the result of the pkcomposite command over the top. It shows how a median layer is created within the bounding box of the AOI.\n\nIn an EOAP cointext this could be scaled up across multiple multi-band images for a larger AOI.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: temporal mosaics"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html",
    "href": "workflows/3_eoapgen.html",
    "title": "The EOAP Generator",
    "section": "",
    "text": "Description & purpose: This Notebook introduces the EOAP generation tool created to help users make compliant EO application packages ready to be run using the EODH workflow runner.\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2025-01-07\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html#requirements",
    "href": "workflows/3_eoapgen.html#requirements",
    "title": "The EOAP Generator",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements that are needed for the tool to create a working EOAP. These are: * Python scripts. These must use argparse or click and the parameters will be mapped to the CWL CommandLineTool inputs * A pip requirements file for each script being wrapped into the EOAP * A compliant eoap-gen configuration file",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html#steps",
    "href": "workflows/3_eoapgen.html#steps",
    "title": "The EOAP Generator",
    "section": "Steps",
    "text": "Steps\nA full tutorial is provided with the repository (see https://github.com/EO-DataHub/eoap-gen/blob/main/ades_guide.md). Here, we will outline the main steps required in using the eoap-gen tool.\nThe first thing a user is required to do is understand the workflow that they want to wrap. At it’s most simple the steps of a workflow are threefold: * find your input data, * process your input data, and * create a STAC output of the processed data.\nFor the eoap-gen tool these steps will always be required and when using the workflow runner (WR) (aka ADES) on the EODH the output will always need to be a directory output containing a STAC catalog. When using the EODH it is recommended that the Python API client pyeodh is used to access the API endpoints on the Hub.\nThe following directory structure is recommended when using the eoap-gen tool:\n\n.github\n└── workflows\n    └── build.yml\nget_urls\n├── get_urls.py\n└── get_urls_reqs.txt\nmake_stac\n├── make_stac.py\n└── make_stac_reqs.txt\nconfig.yml\n\nDespite simplifying the process, it is still complex to create these packages. A configuration file is needed and this is then used to create the EOAP. More information about this can be found in the repositry for the tool, but the example of a configuration file for a single step workflow (below) demonstrates the need to understand the full data procesisng chain.\n\nid: resize-collection\ndoc: Resize collection cogs\nlabel: Resize collection cogs\ninputs:\n  - id: catalog\n    label: catalog\n    doc: full catalog path\n    type: string\n    default: supported-datasets/ceda-stac-fastapi\n  - id: collection\n    label: collection id\n    doc: collection id\n    type: string\n    default: sentinel2_ard\noutputs:\n  - id: stac_output\n    type: Directory\n    source: step3/stac_catalog\nsteps:\n  - id: get_urls\n    script: playground/get_urls.py\n    requirements: playground/get_urls_reqs.txt\n    inputs:\n      - id: catalog\n        source: resize-collection/catalog\n      - id: collection\n        source: resize-collection/collection\n    outputs:\n      - id: urls\n        type: string[]\n        outputBinding:\n          loadContents: true\n          glob: urls.txt\n          outputEval: $(self[0].contents.split('\\n'))\n      - id: ids\n        type: string[]\n        outputBinding:\n          loadContents: true\n          glob: ids.txt\n          outputEval: $(self[0].contents.split('\\n'))\n\nOnce the required files are in place the user needs to execute the eoap-gen tool. The specific command will change with the file names, but the following code snippet shows the form it would take\n\neoap-gen generate \\\n  --config=eoap-gen-config.yml \\\n  --output=eoap-gen-out \\\n  --docker-url-base=ghcr.io/user/repo \\\n  --docker-tag=main",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/3_eoapgen.html#other-tools",
    "href": "workflows/3_eoapgen.html#other-tools",
    "title": "The EOAP Generator",
    "section": "Other tools",
    "text": "Other tools\nOther useful tools that you may want to try include:\n\ncwltool\nThe cwltool is “the reference implementation of the Common Workflow Language open standards. It is intended to be feature complete and provide comprehensive validation of CWL files as well as provide other tools related to working with CWL”. It is a commandline tool designed to run locally and is an excellent piece of software to help check that CWL is compliant. It is designed for use on Linux and will also run on a Mac or Windows (through WSL - windows Subsystem for Linux). It can implement Docker, Podman, Singularity and others for the containerisatoion of commandline components.\n\n\nscriptcwl\nScriptcwl is a Python package for creating CWL workflows and the latest doscumentation gives an indepth explanation of its use. Be aware that this tool has not been developed on or updated for many years.\n\n\ncwl-utils\nStill actively developed, cwl-utils provides Python utilities and autogenerated classes for loading and parsing CWL documents. Although not specific to EOAPs this set of tools may be helpful when developing your workflows. Documentation is relatively sparse.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Using eoap-gen"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html",
    "href": "workflows/1_Workflows.html",
    "title": "Workflow Introduction",
    "section": "",
    "text": "Description & purpose: This Notebook explains what a workflow is (in the context of the EODH platform) and provides information on the technology surrounding the workflows.\nAuthor(s): Alastair Graham\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#why-cwl",
    "href": "workflows/1_Workflows.html#why-cwl",
    "title": "Workflow Introduction",
    "section": "Why CWL?",
    "text": "Why CWL?\nThe Common Workflow Language (CWL) is an open standard designed for defining and executing data analysis workflows. It provides a formal way to describe the individual steps in a workflow, the inputs and outputs of each step, and how those steps are connected. CWL is platform-independent and focuses on portability, reproducibility, and scalability, enabling workflows to be executed in various environments, from personal computers to large cloud infrastructures.\nBy fostering collaboration and standardisation, CWL plays a crucial role in advancing research, and is particularly used in fields such as bioinformatics and climate science.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#scripting-workflows",
    "href": "workflows/1_Workflows.html#scripting-workflows",
    "title": "Workflow Introduction",
    "section": "Scripting workflows",
    "text": "Scripting workflows\nWhile shell scripts or other code scripts (e.g. Python) can meet the need of data processing workflows, using a formal workflow language (such as CWL) brings additional benefits such as abstraction and improved scalability and portability.\nComputational workflows explicitly create a divide between a user’s dataflow and the computational details which underpin the chain of tools.\nThe dataflow is described by the workflow and the tool implementation is specified by descriptors that remove the workflow complexity.\nWorkflow managers such as cwltool (see below) help with the automation, monitoring and tracking of a dataflow. By producing computational workflows in a standardised format, and publishing them (alongside any data) with open access, the workflows become more FAIR (Findable, Accessible, Interoperable, and Reusable). The Common Workflow Language (CWL) standard has been developed to standardise workflow needs across different thematic areas.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#execution-sequence",
    "href": "workflows/1_Workflows.html#execution-sequence",
    "title": "Workflow Introduction",
    "section": "Execution sequence",
    "text": "Execution sequence\nThe generic execution sequence of a CWL process (including Workflows and CommandLineTools) is as follows.\n\nLoad an input object.\nLoad, process and validate a CWL document.\nIf there are multiple process objects (due to $graph) then choose the process with the id of “#main” or “main”.\nValidate the input object against the inputs schema for the process.\nPerform any further setup required by the specific process type.\nExecute the process.\nCapture results of process execution into the output object.\nValidate the output object against the outputs schema for the process.\nReport the output object to the process caller.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#context",
    "href": "workflows/1_Workflows.html#context",
    "title": "Workflow Introduction",
    "section": "Context",
    "text": "Context\nNote: This workflow has been designed and tested using cwltool on a local machine.\nThe first thing to do when designing a workflow is understand the context of what is desired, and how that may need to be referred to in the workflow. For this example workflow, we will take a list of Sentinel-2 ARD images, clip them to an area of interest, and stack them. The flow will look like the following:\ngraph LR;\n  get_data -- S2_ARD --&gt; clip -- clipped --&gt; stack -- stacked--&gt; Output ;\n\nThe next thing to do is access the data to be used in the Workflow. In this case we will download two bands of a Sentinel 2 image held on AWS. We will use the curl tool to do this, saving the accessed image as B0$.tif (where $ is the band number):\n\ncurl -o B04.tif https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/53/H/PA/2021/7/S2B_53HPA_20210723_0_L2A/B04.tif\n\ncurl -o B03.tif https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/53/H/PA/2021/7/S2B_53HPA_20210723_0_L2A/B03.tif \n\nThe commands that we will use in the workflow are all available through gdal.\n\nClip the image\nWe will use gdal_-_translate to clip the larger image to a smaller more manageable dataset. The gdal command that we can test and that we will need to replicate in CWL is:\n\ngdal_translate -projwin ULX ULY LRX LRY  -projwin_srs EPSG:4326 BO4.tif B04_clipped.tif\n\nwhere the coordinates UL refer to upper left and LR to lower right X and Y.\n\n\nStack the clips\nSimilarly, we will use gdal_merge.py to construct the stacked images from the clipped image. This can be tested using the following command:\n\ngdal_merge.py -separate B04_clipped.tif B03_clipped.tif -o stacked.tif",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#building-the-workflow",
    "href": "workflows/1_Workflows.html#building-the-workflow",
    "title": "Workflow Introduction",
    "section": "Building the Workflow",
    "text": "Building the Workflow\n\nRequired files\nThere are three main files that are required to construct a CWL Workflow. These are: * DockerFile or existing online container * CWL file * YAML file\nIt may be that other files e.g. a .sh script or a Python script are also needed, depending on how bespoke and/or complex the desired workflow is.\n\n\ncwltool\nTo run CWL workflows you will need a CWL runner. The most commonly used (locally) is cwltool which is maintained by the CWL community. cwltool is the reference executor for Common Workflow Language standards and supports everything in the current CWL specification. cwltool can be installed using pip or variants of conda. More information can be found here and here, or via ReadTheDocs.\n\n\nContainers\nFor the purposes of this example, we will be pulling the GDAL container from the OSgeo repository (see here).\nNOTE: There are a number of different images that can be accessed. To use the .py tools available through GDAL then ‘GDAL Python’ is required.\nIf we wanted to we could also build our own bespoke image using a DockerFile and then run that. This is often used when data processing scripts need to be copied into the container.\nWe will also be using Podman as our container software. ‘podman’ is a drop in replacement for Docker but does require the --podman arguement in the cwltool command. If using Windows, or if you are more familiar with Docker, then using Docker is the default containerisation method.\n\n\nCWL files\nFor this example we require a CWL CommandLine file for both the clipping and stacking components of the workflow. We will also need a CWL Workflow file to bring these together and run the entire process. The next block of code outlines the overall Workflow file.\nNote: This example is based on the example found here. Some errors were found in the original CWL files and the version presented here has been tested and is known to work on a local Linux (Debian based) system.\n\nclass: Workflow\nlabel: Sentinel-2 clipping and stacking\ndoc:  This workflow creates a stacked composite. File name: composite.cwl\nid: main\n\nrequirements:\n- class: ScatterFeatureRequirement\n\ninputs:\n\n  geotiff:\n    doc: list of geotifs\n    type: File[]\n\n  bbox: \n    doc: area of interest as a bounding box\n    type: string\n\n  epsg:\n    doc: EPSG code \n    type: string\n    default: \"EPSG:4326\"\n\noutputs:\n  rgb:\n    outputSource:\n    - node_concatenate/composite\n    type: File\n\nsteps:\n\n  node_translate:\n\n    run: gdal-translate.cwl\n\n    in:\n\n      geotiff: geotiff  \n      bbox: bbox\n      epsg: epsg\n\n    out:\n    - clipped_tif\n\n    scatter: geotiff\n    scatterMethod: dotproduct\n\n  node_concatenate:\n\n    run: concatenate2.cwl\n\n    in: \n      tifs:\n        source: node_translate/clipped_tif\n\n    out:\n    - composite\n\n\ncwlVersion: v1.0\n\nFrom this example, we can see that we require two CommandLine CWL files: gdal-translate.cwl and concatenate2.cwl. Let’s deal with these in order.\n\nclass: CommandLineTool\n\ncwlVersion: v1.0\ndoc:  This runs GDAL Translate to clip an image to bbox corner coordinates.\n\nrequirements: \n  InlineJavascriptRequirement: {}\n  DockerRequirement: \n    dockerPull: ghcr.io/osgeo/gdal:ubuntu-small-latest\n\nbaseCommand: gdal_translate\n\narguments:\n- -projwin \n- valueFrom: ${ return inputs.bbox.split(\",\")[0]; }\n- valueFrom: ${ return inputs.bbox.split(\",\")[3]; }\n- valueFrom: ${ return inputs.bbox.split(\",\")[2]; }\n- valueFrom: ${ return inputs.bbox.split(\",\")[1]; }\n- valueFrom: ${ return inputs.geotiff.basename.replace(\".tif\", \"\") + \"_clipped.tif\"; }\n  position: 8\n\ninputs:\n  geotiff: \n    type: File\n    inputBinding:\n      position: 7\n  bbox: \n    type: string\n  epsg:\n    type: string\n    default: \"EPSG:4326\" \n    inputBinding:\n      position: 6\n      prefix: -projwin_srs\n      separate: true\n\noutputs:\n  clipped_tif:\n    outputBinding:\n      glob: '*_clipped.tif'\n    type: File\n\n\nclass: CommandLineTool\n\ncwlVersion: v1.0\ndoc: This runs GDAL Merge to stack images together.\n\nrequirements:\n  InlineJavascriptRequirement: {}\n  DockerRequirement: \n    dockerPull: ghcr.io/osgeo/gdal:ubuntu-small-latest\n\nbaseCommand: gdal_merge.py\n\narguments: \n- -separate \n- valueFrom: ${ return inputs.tifs; }\n- -o\n- composite.tif\n# gdal_merge.py -separate 1.tif 2.tif 3.tif -o rgb.tif\n\ninputs:\n\n  tifs:\n    type: File[]\n\noutputs:\n\n  composite:\n    outputBinding:\n      glob: '*.tif'\n    type: File\n\nNOTE: YAML generally doesn’t play well with tabs as whitespace so it’s best practice to use spaces for indentations",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#running-the-workflow",
    "href": "workflows/1_Workflows.html#running-the-workflow",
    "title": "Workflow Introduction",
    "section": "Running the Workflow",
    "text": "Running the Workflow\nNow that we have our commandline CWL component files, and the Workflow CWL file that brings the tools together, we need to specify the input parameters. This is done using a parameters.yml file, where the name of the file can be anything that you want. The contents should follow the layout that we will be using:\n\nbbox: \"136.659,-35.96,136.923,-35.791\"\ngeotiff: \n- { \"class\": \"File\", \"path\": \"../B04.tif\" }\n- { \"class\": \"File\", \"path\": \"../B03.tif\" }\nepsg: \"EPSG:4326\"\n\nYou will need to change the path parameter to match the location of your input files.\nNow we run it with the command:\ncwltool --podman composite.cwl composite-params.yml\nNote: remember that if you are using Docker then you do not need the --podman arguement.",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#outputs",
    "href": "workflows/1_Workflows.html#outputs",
    "title": "Workflow Introduction",
    "section": "Outputs",
    "text": "Outputs\nThis workflow takes a couple of minutes to run, during which time the executed commands and their runtime messages are displayed on the command line. Once the workflow completes, the output file will be found in the directory from where the workflow was run. Intermediate files that are not specified in the out block in the workflow are automatically deleted.\nThe output .tif file can now be opened in QGIS or a similar software application to check that the output is as expected (in this case a 2-layer image of a clipped area of the extent of the input files).",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "workflows/1_Workflows.html#tips",
    "href": "workflows/1_Workflows.html#tips",
    "title": "Workflow Introduction",
    "section": "Tips",
    "text": "Tips\nYou can pass --leave-tmpdirs to the cwltool command. This is often helpful to figure out if the outputs from a step are what you think they should be.\nAnother good (non-spatial) tutorial can be found here: https://andrewjesaitis.com/posts/2017-02-06-cwl-tutorial/",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "Workflow Introduction"
    ]
  },
  {
    "objectID": "science/2_Commercial.html",
    "href": "science/2_Commercial.html",
    "title": "Commercial Data",
    "section": "",
    "text": "Description & purpose: This notebook is designed to introduce the user to the commercial data accessible via the EODH platform.\nAuthor(s): Alastair Graham\nDate created: 2024-12-12\nDate last modified: 2024-12-17\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nWIP Icon Attribution\n\nTo be added\n\nAirbus SAR\nAirbus Optical\nPlanet"
  },
  {
    "objectID": "presentations/videos.html",
    "href": "presentations/videos.html",
    "title": "Recorded presentations",
    "section": "",
    "text": "Description & purpose: This notebook provides access to video recordings of presnetations about the EODH platform.\nAuthor(s): Alastair Graham\nDate created: 2024-12-17\nDate last modified: 2024-12-17\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nDefra presentation\nThe following video guides the user through the notebooks presented to Defra in September 2024 and available via this website.\n\n\n\n\nHigh resolution\nThe following video explains how the user interacts with the Hub and shows data through using OGC services.\n\n\n\n\nSparkgeo Climate App\nThe following video introduces the climate application being developed using the EODH.\n\n\n\n\nEO Pro\nThese videos introduce the EO Pro application that interfaces with the EODH.",
    "crumbs": [
      "Presentations",
      "Recorded presentations"
    ]
  },
  {
    "objectID": "presentations/Workshop/2_202502_workshop_DataProcessing.html",
    "href": "presentations/Workshop/2_202502_workshop_DataProcessing.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include: * A Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more * A Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements * A Web Presence - an intuitive user interface to allow account management, data discovery and mapping * An App Hub - a science portal providing access to a Jupyter lab environment\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh pandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s massive compute\nThe EODH compute architecture is built around a new OGC standard called EO Application Packages (EOAP). These are complex constructions of code and data, and at their core is the concept of a Common Workflow Language (CWL) workflow. To run CWL workflows you need a CWL runner, and the EODH Workflow Runner provides that. The EOAPs require a workflow description in CWL, a Docker container, bespoke scripts and links to the data. In the case of EODH, the data inputs and outputs are to be provided as STAC catalogues. Oxidian, as part of our work developing integrations for the Hub, have created a generator tool eoap-gen that abstracts away much of the complexity (check out the training materials repository and website for more details).\nOxidian have also developed a QGIS plugin to allow desktop users to discover, parameterise and execute workflows on the Hub.\n\n# Imports\nimport pyeodh\n\nimport os\nfrom requests import HTTPError\n\n\n# First the user needs to connect to the Workflow Runner\n# Currently this is done by obtaining a user account and API key from the core development team. Here, those details have been saved in secrets.txt\n# secrets.txt should contain the two lines below where USERNAME and API_KEY are specific to the user:\n# USER=\"USERNAME\"\n# PSWD=\"API_KEY\" \n\nwith open('secrets.txt', 'r') as file:\n    lines = file.readlines()\n    username = lines[0].strip().split('=')[1].strip('\"')\n    token = lines[1].strip().split('=')[1].strip('\"')\n\nclientwfr = pyeodh.Client(username=username, token=token, s3_token=token)\nwfr = clientwfr.get_ades()\n\nOur user wants to know what workflows they have access to in their workspace on the Hub.\n\n# List the workflows available in the user workspace\nfor p in wfr.get_processes():\n    print(p.id)\n\ndisplay\necho\nconvert-img\n\n\nAs development continues the number of demonstration workflows available to users will increase. With uptake of the generator tool users will also be able to create their own bespoke workflows. In time, the Hub will contain organisational accounts and the ability to share workflow files.\nOur user wants to deploy a workflow that they have found online. They can do so for compliant CWL files by submitting the URl.\n\n# Deploy a workflow using a URL to a .cwl file hosted online\nconvert_url_proc = wfr.deploy_process(\n    cwl_url=\"https://raw.githubusercontent.com/EOEPCA/deployment-guide/main/deploy/samples/requests/processing/convert-url-app.cwl\"\n)\nprint(convert_url_proc.id, convert_url_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nconvert-url Convert URL : Deployed\n\ndisplay\necho\nconvert-img\nconvert-url\n\n\n\n# If a user wants to tidy their workspace or no longer wants access to a workflow they can remove it\ntry:\n    wfr.get_process(\"convert-url\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\nProcess removed\n\n\nThe user is interested in the ARD files, but they are too large for the task that they want to undertake. The workflow file linked to here takes a series of data from the Sentinel 2 ARD collection and resizes the imagery using gdal_translate. The CWL file needs to be submitted to the Workflow Runner and parameterised, before it can then be run.\n\n# Deploy the workflow\n# Remove an existing nstance of the workflow\ntry:\n    wfr.get_process(\"convert-img\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\n# Deploy from a URL to a .cwl file hosted online\n#img_proc = wfr.deploy_process(cwl_yaml=cwl_yaml)\nimg_proc = wfr.deploy_process(cwl_url='https://raw.githubusercontent.com/ajgwords/eodh-tests/main/resize-col.cwl')\n\nprint(img_proc.id, img_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nProcess not found\nresize-col Resize collection cogs : Deployed\n\ndisplay\necho\nresize-col\n\n\nAs part of the presentation we looked at the plugin offline. Screenshots are provided here for those who do not have the plugin installed.\nOnce the CWL file has been submitted then it is possible for users with suitable permissions to also view the list of available workflows through the QGIS plugin.\n\n\n\nimage.png\n\n\nThe user can choose and parameterise a workflow using the QGIS plugin as shown in this screenshot. A series of defaults are provided with all workflow files so a user can run the workflow straight away using those. To do so in code, the user provides an empty dictionary.\n\n\n\nimage.png\n\n\n\n# Run the workflow using the defaults\npyeodh.set_log_level(10) # set the logging to be verbose\n\nresize_job = img_proc.execute(\n    {\n    }\n)\n\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'POST', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution', 'headers': {'Prefer': 'respond-async'}, 'params': None, 'data': {'inputs': {'workspace': 'ajgwords'}}, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: POST https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution\n       headers: {'Prefer': 'respond-async', 'Content-Type': 'application/json'}\n       params: None\n       body: {\"inputs\": {\"workspace\": \"ajgwords\"}}\nDEBUG: Received response 201\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:46 GMT', 'x-powered-by': 'ZOO-Project-DRU', 'x-also-powered-by': 'jwt.securityIn', 'x-also-also-powered-by': 'dru.securityIn', 'preference-applied': 'respond-async', 'x-also-also-also-powered-by': 'dru.securityOut', 'location': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'RtavU9ppINxjxyCvlYOnhxdNqGnDtX-HNJmyLlCbtYPf3Dcd5MgLsg=='}\n       content: {\n         \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\",\n         \"type\": \"process\",\n         \"processID\": \"resize-col\",\n         \"created\": \"2024-09-18T14:05:46.805Z\",\n         \"started\": \"2024-09-18T14:05:46.805Z\",\n         \"updated\": \"2024-09-18T14:05:46.805Z\",\n         \"status\": \"running\",\n         \"message\": \"ZOO-Kernel accepted to run your service!\",\n         \"links\": [\n           {\n             \"title\": \"Status location\",\n             \"rel\": \"monitor\",\n             \"type\": \"application/json\",\n             \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"\n           }\n         ]\n       }\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running ZOO-Kernel accepted to run your service!\n\n\n\nresize_job.refresh()\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'GET', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'headers': None, 'params': None, 'data': None, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: GET https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\n       headers: {}\n       params: None\n       body: None\nDEBUG: Received response 200\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '507', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:54 GMT', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'IakHOzKo8AnbmuIW_lFytmYjofpvygKjKotlpOjxhGiQ-a5G6RH22A=='}\n       content: {\"progress\": 15, \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\", \"type\": \"process\", \"processID\": \"resize-col\", \"created\": \"2024-09-18T14:05:46.805Z\", \"started\": \"2024-09-18T14:05:46.805Z\", \"updated\": \"2024-09-18T14:05:48.804Z\", \"status\": \"running\", \"message\": \"processing environment created, preparing execution\", \"links\": [{\"title\": \"Status location\", \"rel\": \"monitor\", \"type\": \"application/json\", \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"}]}\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running processing environment created, preparing execution\n\n\nThe outputs above show the status of the running job: \"status\": \"running\". The job status can also be monitored using the QGIS plugin.\n\n\n\nimage.png\n\n\n\n\n\nimage-2.png\n\n\nThe outputs are accessible and loadable from the QGIS plugin.\nOriginal image: \nResampled image: \nThe workflow outputs are available in a users’s workspace and are attached to the job id presented through the QGIS plugin. A user can open any output from within the plugin to be displayed in the QGIS map window."
  },
  {
    "objectID": "presentations/DEFRA2/1_202412_Defra_Technology.html",
    "href": "presentations/DEFRA2/1_202412_Defra_Technology.html",
    "title": "Demonstration for Space Horizon Scanning Network: DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook provides access to the slides presented to Defra in December 2024 and designed to showcase the initial functionality of the Earth Observation Data Hub.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-12-13\nDate last modified: 2024-12-17\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment\n\n\n\nSlides\n\n\n\nSlide 1\n\n\n\n\n\nSlide 2\n\n\n\n\n\nSlide 3\n\n\n\n\n\nSlide 4\n\n\n\n\n\nSlide 5\n\n\n\n\n\nSlide 6\n\n\n\n\n\nSlide 7\n\n\n\n\n\nSlide 8\n\n\n\n\n\nSlide 9\n\n\n\n\n\nSlide 10\n\n\n\n\n\nSlide 11\n\n\n\n\n\nSlide 12\n\n\n\n\n\nSlide 13\n\n\n\n\n\nSlide 14\n\n\n\n\n\nSlide 15\n\n\n\n\n\nSlide 16",
    "crumbs": [
      "Presentations",
      "Space Horizon Scanning Network"
    ]
  },
  {
    "objectID": "presentations/DEFRA/2_202409_Defra_DataProcessing.html",
    "href": "presentations/DEFRA/2_202409_Defra_DataProcessing.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include: * A Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more * A Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements * A Web Presence - an intuitive user interface to allow account management, data discovery and mapping * An App Hub - a science portal providing access to a Jupyter lab environment\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh pandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s massive compute\nThe EODH compute architecture is built around a new OGC standard called EO Application Packages (EOAP). These are complex constructions of code and data, and at their core is the concept of a Common Workflow Language (CWL) workflow. To run CWL workflows you need a CWL runner, and the EODH Workflow Runner provides that. The EOAPs require a workflow description in CWL, a Docker container, bespoke scripts and links to the data. In the case of EODH, the data inputs and outputs are to be provided as STAC catalogues. Oxidian, as part of our work developing integrations for the Hub, have created a generator tool eoap-gen that abstracts away much of the complexity (check out the training materials repository and website for more details).\nOxidian have also developed a QGIS plugin to allow desktop users to discover, parameterise and execute workflows on the Hub.\n\n# Imports\nimport pyeodh\n\nimport os\nfrom requests import HTTPError\n\n\n# First the user needs to connect to the Workflow Runner\n# Currently this is done by obtaining a user account and API key from the core development team. Here, those details have been saved in secrets.txt\n# secrets.txt should contain the two lines below where USERNAME and API_KEY are specific to the user:\n# USER=\"USERNAME\"\n# PSWD=\"API_KEY\" \n\nwith open('secrets.txt', 'r') as file:\n    lines = file.readlines()\n    username = lines[0].strip().split('=')[1].strip('\"')\n    token = lines[1].strip().split('=')[1].strip('\"')\n\nclientwfr = pyeodh.Client(username=username, token=token, s3_token=token)\nwfr = clientwfr.get_ades()\n\nOur user wants to know what workflows they have access to in their workspace on the Hub.\n\n# List the workflows available in the user workspace\nfor p in wfr.get_processes():\n    print(p.id)\n\ndisplay\necho\nconvert-img\n\n\nAs development continues the number of demonstration workflows available to users will increase. With uptake of the generator tool users will also be able to create their own bespoke workflows. In time, the Hub will contain organisational accounts and the ability to share workflow files.\nOur user wants to deploy a workflow that they have found online. They can do so for compliant CWL files by submitting the URl.\n\n# Deploy a workflow using a URL to a .cwl file hosted online\nconvert_url_proc = wfr.deploy_process(\n    cwl_url=\"https://raw.githubusercontent.com/EOEPCA/deployment-guide/main/deploy/samples/requests/processing/convert-url-app.cwl\"\n)\nprint(convert_url_proc.id, convert_url_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nconvert-url Convert URL : Deployed\n\ndisplay\necho\nconvert-img\nconvert-url\n\n\n\n# If a user wants to tidy their workspace or no longer wants access to a workflow they can remove it\ntry:\n    wfr.get_process(\"convert-url\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\nProcess removed\n\n\nThe user is interested in the ARD files, but they are too large for the task that they want to undertake. The workflow file linked to here takes a series of data from the Sentinel 2 ARD collection and resizes the imagery using gdal_translate. The CWL file needs to be submitted to the Workflow Runner and parameterised, before it can then be run.\n\n# Deploy the workflow\n# Remove an existing nstance of the workflow\ntry:\n    wfr.get_process(\"convert-img\").delete()\n    print(\"Process removed\")\nexcept HTTPError:\n    print(\"Process not found\")\n\n# Deploy from a URL to a .cwl file hosted online\n#img_proc = wfr.deploy_process(cwl_yaml=cwl_yaml)\nimg_proc = wfr.deploy_process(cwl_url='https://raw.githubusercontent.com/ajgwords/eodh-tests/main/resize-col.cwl')\n\nprint(img_proc.id, img_proc.description, \": Deployed\")\nprint('')\n# List the available workflows\nfor p in wfr.get_processes():\n    print(p.id)\n\nProcess not found\nresize-col Resize collection cogs : Deployed\n\ndisplay\necho\nresize-col\n\n\nAs part of the presentation we looked at the plugin offline. Screenshots are provided here for those who do not have the plugin installed.\nOnce the CWL file has been submitted then it is possible for users with suitable permissions to also view the list of available workflows through the QGIS plugin.\n\n\n\nimage.png\n\n\nThe user can choose and parameterise a workflow using the QGIS plugin as shown in this screenshot. A series of defaults are provided with all workflow files so a user can run the workflow straight away using those. To do so in code, the user provides an empty dictionary.\n\n\n\nimage.png\n\n\n\n# Run the workflow using the defaults\npyeodh.set_log_level(10) # set the logging to be verbose\n\nresize_job = img_proc.execute(\n    {\n    }\n)\n\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'POST', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution', 'headers': {'Prefer': 'respond-async'}, 'params': None, 'data': {'inputs': {'workspace': 'ajgwords'}}, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: POST https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/processes/resize-col/execution\n       headers: {'Prefer': 'respond-async', 'Content-Type': 'application/json'}\n       params: None\n       body: {\"inputs\": {\"workspace\": \"ajgwords\"}}\nDEBUG: Received response 201\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:46 GMT', 'x-powered-by': 'ZOO-Project-DRU', 'x-also-powered-by': 'jwt.securityIn', 'x-also-also-powered-by': 'dru.securityIn', 'preference-applied': 'respond-async', 'x-also-also-also-powered-by': 'dru.securityOut', 'location': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'RtavU9ppINxjxyCvlYOnhxdNqGnDtX-HNJmyLlCbtYPf3Dcd5MgLsg=='}\n       content: {\n         \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\",\n         \"type\": \"process\",\n         \"processID\": \"resize-col\",\n         \"created\": \"2024-09-18T14:05:46.805Z\",\n         \"started\": \"2024-09-18T14:05:46.805Z\",\n         \"updated\": \"2024-09-18T14:05:46.805Z\",\n         \"status\": \"running\",\n         \"message\": \"ZOO-Kernel accepted to run your service!\",\n         \"links\": [\n           {\n             \"title\": \"Status location\",\n             \"rel\": \"monitor\",\n             \"type\": \"application/json\",\n             \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"\n           }\n         ]\n       }\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running ZOO-Kernel accepted to run your service!\n\n\n\nresize_job.refresh()\nprint(resize_job.id, resize_job.status, resize_job.message)\n\nDEBUG: _request_json_raw received {'self': &lt;pyeodh.client.Client object at 0x7d35e47e45c0&gt;, 'method': 'GET', 'url': 'https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3', 'headers': None, 'params': None, 'data': None, 'encode': &lt;function _encode_json at 0x7d3635f30540&gt;}\nDEBUG: Making request: GET https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\n       headers: {}\n       params: None\n       body: None\nDEBUG: Received response 200\n       headers: {'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '507', 'Connection': 'keep-alive', 'Server': 'nginx/1.27.0', 'Date': 'Wed, 18 Sep 2024 14:05:54 GMT', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Methods': 'GET, PUT, POST, DELETE, PATCH, OPTIONS', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Prefer', 'Access-Control-Max-Age': '1728000', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 a6a1a17bbe377bf7c4423397c71959da.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'LHR50-P1', 'X-Amz-Cf-Id': 'IakHOzKo8AnbmuIW_lFytmYjofpvygKjKotlpOjxhGiQ-a5G6RH22A=='}\n       content: {\"progress\": 15, \"jobID\": \"19efd624-75c7-11ef-9849-4e9264aaf6f3\", \"type\": \"process\", \"processID\": \"resize-col\", \"created\": \"2024-09-18T14:05:46.805Z\", \"started\": \"2024-09-18T14:05:46.805Z\", \"updated\": \"2024-09-18T14:05:48.804Z\", \"status\": \"running\", \"message\": \"processing environment created, preparing execution\", \"links\": [{\"title\": \"Status location\", \"rel\": \"monitor\", \"type\": \"application/json\", \"href\": \"https://test.eodatahub.org.uk/ades/ajgwords/ogc-api/jobs/19efd624-75c7-11ef-9849-4e9264aaf6f3\"}]}\n\n\n19efd624-75c7-11ef-9849-4e9264aaf6f3 running processing environment created, preparing execution\n\n\nThe outputs above show the status of the running job: \"status\": \"running\". The job status can also be monitored using the QGIS plugin.\n\n\n\nimage.png\n\n\n\n\n\nimage-2.png\n\n\nThe outputs are accessible and loadable from the QGIS plugin.\nOriginal image: \nResampled image: \nThe workflow outputs are available in a users’s workspace and are attached to the job id presented through the QGIS plugin. A user can open any output from within the plugin to be displayed in the QGIS map window.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Processing"
    ]
  },
  {
    "objectID": "plugin/3_OtherTools.html",
    "href": "plugin/3_OtherTools.html",
    "title": "Other QGIS Plugins",
    "section": "",
    "text": "Description & purpose: This notebook highlights other ways to interact with the EODH platform through QGIS.\nAuthor(s): Alastair Graham\nDate created: 2024-12-04\nDate last modified: 2024-12-17\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nSTAC API Browser\nA major benefit derived from constructing the EODH platform using open components is that the technologies and tools can integrate with existing third-party software. One useful pre-existing QGIS plugin is the STAC API Browser. The STAC API Browser is the defacto way of finding and interacting with STAC catalogues on the internet.\nUse this URL in the STAC API Browser plugin (https://test.eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue/) to find assets accessible through the EODH Resource Catalogue (RC).\nFirst you need to set up a connection as shown in the screenshot below.\n\nThen a search can be initiated. Depending on the quality of your connection and the number of returned items this could be slow.\n\nOne useful aspect of this plugin is to plot the returned item boundaries, as shown below. It is also possible to access specific item assets i.e. the data, but this can be very slow given the size of the assets themselves. It is recommended that a tool such as pyeodh or the EODH web interface is used to directly access the datasets referenced by the catalogues.",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "Other Tools"
    ]
  },
  {
    "objectID": "plugin/1_introduction.html",
    "href": "plugin/1_introduction.html",
    "title": "QGIS Plugin",
    "section": "",
    "text": "Description & purpose: The QGIS Plugin has been designed to allow a user with an EODH account to discover and run workflows that are within their account. For users that do not create workflows themselves this will be an especially useful no-code way to run public and organisational workflows.\nAuthor(s): Alastair Graham\nDate created: 2024-10-23\nDate last modified: 2024-12-18\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWhy do we need this plugin?\nA QGIS plugin is an extension or add-on for QGIS, a popular open-source Geographic Information System (GIS) software package. Although fully fnctioned, plugins enhance QGIS by adding specialised tools, workflows, or functionalities that extend its core capabilities. These plugins are written in Python and are highly customisable and diverse in functionality. They allow users to perform complex geospatial tasks more efficiently, integrate with external tools or datasets, and automate repetitive tasks, all within the QGIS environment. This adaptability makes QGIS a powerful and flexible tool for GIS professionals across various industries.\nThe EODH development team recognised that a tranche of potential users benefiting from the platform would be GIS specialists. As such, a QGIS plugin that automates connection to a user’s workspace on the Hub has been created. The plugin allows a user to connect to the EODH and access EO Application Packages (EOAP - for further information see pages relating to workflows). Once a required EOAP has been found, the user can parameterise and execute the workflow. Once complete the GIS user can then interact with the returned data assets and can view the processing logs.\n\n\nSupported versions\nThe plugin is known to run with QGIS 3.34 (long term support) and QGIS 3.40 on Windows, MacOS and Ubuntu variants of Linux.\n\n\nInstallation\nThere are two ways in which the plugin can be installed.\nInstall from zip\n\nGo to the plugin GitHub repository\nFind the latest release and download for your operating system\nOpen QGIS and click ….\nUse the “Install from Zip” function to install the plugin\n\nInstall from repository\n\nOpen QGIS and click ….\nSearch for EODH under the All or Uninstalled tabs\nSelect and click “Install”",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "QGIS Plugin Introduction"
    ]
  },
  {
    "objectID": "platform/1_Intro.html",
    "href": "platform/1_Intro.html",
    "title": "The EO Data Hub",
    "section": "",
    "text": "Description & purpose: This webpage is designed to provide an introduction to the idea behind the Hub and borrows text from the EODH test server ‘About’ page. It also outlines the potential user journey that different practitioners might take.\nAuthor(s): Alastair Graham\nDate created: 2024-10-23\nDate last modified: 2024-12-17\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nEODH Introduction\nThe overall goal of the Earth Observation Data Hub (EODH) project is to develop and operate a new centralised software infrastructure – the Hub – to provide a ‘single point’ of access for UK Earth Observation (EO) data obtained from distributed sources, to include public and commercial centres.\nBy providing this single point of access, the objective is to provide a standard common set of services and APIs upon which new EO services and tools can be developed and accessed by the UK EO data community. The project is currently in a Pathfinder phase (2023-2025) that brings new thinking and experimental developments to bear, resulting in practical services for a variety of users in a short period of time.\nBy the end of the pathfinder phase (March 2025), it is expected that there will be a community of researchers, industry and government users working together to provide and intereact with EO data in new and innovative ways.\n\n\nExample User Journeys\nThe journey through the technology made accessbile by the Hub will be different depending on the needs of the user. The following diagram presents a simplified and high level route for four different user types: * a GIS user who spends most of their working time in QGIS * a web application developer, interested in hooking into the processing power of the EODH and the associated datasets * a data scientist, who will use code in a Jupyter notebook to process different spatial datasets * a dev-ops specialist, tasked with creating and maintaining data services hosted on the EODH\n\nThe concepts of the AppHub and its associated notebook service, workflows, data collections and the Resource Catalogue, and account access are all covered in other sections of these training materials.",
    "crumbs": [
      "Hub Platform",
      "EODH Platform Introduction"
    ]
  },
  {
    "objectID": "api-client/4_Services.html",
    "href": "api-client/4_Services.html",
    "title": "Web Services",
    "section": "",
    "text": "Description & purpose: This notebook introduces the webservices available through the EODH platform and API endpoints.\nAuthor(s): Alastair Graham\nDate created: 2024-12-05\nDate last modified: 2024-12-17\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nWIP Icon Attribution",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Using Mapping Services"
    ]
  },
  {
    "objectID": "api-client/4_Services.html#records",
    "href": "api-client/4_Services.html#records",
    "title": "Web Services",
    "section": "Records",
    "text": "Records\nOGC API - Records is designed for discovering and accessing metadata about geospatial resources. It enables users to search, retrieve, and manage metadata records, such as datasets, services, and other resources. The standard supports flexible query capabilities, filtering, and sorting to help users discover relevant geospatial content efficiently. By adhering to web and open standards, OGC API - Records ensures interoperability across platforms, making it easier for organizations to share and access geospatial metadata in a consistent, machine-readable format. This API service is used on the backend of EODH.\n\nTo be added\n\nList of web services and what they do\nExample code on how to access them and visualise the related data\nHow to use the services in QGIS",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Using Mapping Services"
    ]
  },
  {
    "objectID": "api-client/2_ResourceCatalog.html",
    "href": "api-client/2_ResourceCatalog.html",
    "title": "Resource Catalog",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of December 2024.\nAuthor(s): Alastair Graham, Dusan Figala, EODH\nDate created: 2024-09-05\nDate last modified: 2024-12-11\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nWIP Icon Attribution\nThe first thing to do is ensure that the most recent version of pyeodh is installed on your system. It is good practice to run the following cell if you have not installed pyeodh or have not used it in a while.\n\n# Run this cell if pyeodh is not installed, or needs updating\n!pip install --upgrade pyeodh\n\n\nExploring the Resource Catalogue\nNow we are ready to investigate the Resource Catalogue. First off, we need to import the pyeodh package.\n\n# Import the Python API Client\nimport pyeodh\n\nNext we need to create an instance of the Client, which is our entrypoint to EODH APIs. From there we can start to search the collections held within the platform.\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"- {collect.id}: {collect.description}\")\n\n- ukcp: UKCP\n- sentinel2_ard: sentinel 2 ARD\n- sentinel1: Sentinel 1\n- sentinel-2-l2a: Global Sentinel-2 data from the Multispectral Instrument (MSI) onboard Sentinel-2\n- sentinel-2-l1c: Global Sentinel-2 data from the Multispectral Instrument (MSI) onboard Sentinel-2\n- sentinel-2-c1-l2a: Sentinel-2 Collection 1 L2A, data from the Multispectral Instrument (MSI) onboard Sentinel-2\n- sentinel-1-grd: Sentinel-1 is a pair of Synthetic Aperture Radar (SAR) imaging satellites launched in 2014 and 2016 by the European Space Agency (ESA). Their 6 day revisit cycle and ability to observe through clouds makes this dataset perfect for sea and land monitoring, emergency response due to environmental disasters, and economic applications. This dataset represents the global Sentinel-1 GRD archive, from beginning to the present, converted to cloud-optimized GeoTIFF format.\n- naip: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n- landsat-c2-l2: Atmospherically corrected global Landsat Collection 2 Level-2 data from the Thematic Mapper (TM) onboard Landsat 4 and 5, the Enhanced Thematic Mapper Plus (ETM+) onboard Landsat 7, and the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) onboard Landsat 8 and 9.\n- land_cover: Land Cover\n\n\nThe attributes of a catalogue are mapped to a series of properties. For instance, in the following cell we print the id, title and description for the supported-datasets catalogue.\n\nceda_cat = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\")\nprint(\"id: \", ceda_cat.id)\nprint(\"title: \", ceda_cat.title)\nprint(\"description: \", ceda_cat.description)\n\nid:  ceda-stac-catalogue\ntitle:  stac-fastapi-elasticsearch\ndescription:  stac-fastapi-elasticsearch\n\n\nThe Hub API endpoints are wrapped in methods inside pyeodh and are structured into classes, following the same logic as the underlying APIs. This means that, for xample, to fetch a collection item we first need to get the collection from the resource catalogue. The following cell provedes a code example to do this.\n\n# GET /stac-fastapi/collections/{collection_id}/items/{item_id}\ncmip6 = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\").get_collection('cmip6')\ncmip6\n\n&lt;pyeodh.resource_catalog.Collection at 0x781cce199910&gt;\n\n\nSome API responses are paginated (e.g. collection items), and you can simply iterate over them.\n\n# GET /stac-fastapi/collections/cmip6/items\nitems = cmip6.get_items()\n\n# Warning: this will take a long time for large catalogues such as cmip6\nfor item in items:\n    print(item.id)\n\n\n\nCreating, removing and updating collections\n\nTo be added * Fix the following cells * Improve explanations of the code blocks\nAttempting to create a collection with id that already exists will result in 409 error code. To see the example in action delete the test collection first by running the cell below.\nDelete a colletion\n\nclient.get_collection(\"test1\").delete()\n\nCreate new collection example\n\ntest1 = client.create_collection(id=\"test1\", title=\"Test\", description=\"Test collection\")\nprint(test1.description)\n\nUpdate a collection\n\ntest1.update(description=\"Different description\")\nprint(test1.description)\n\n\n\nInteracting with items\nCreate an item\n\ntestitem1 = test1.create_item(id=\"test1.testitem1\")\nprint(f\"Created {testitem1.id} in collection {testitem1.collection}\")\n\nUpdate an item\n\ntestitem1.update(properties={\"foo\": \"bar\"})\nprint(testitem1.properties)\n\nDelete an item\n\ntestitem1.delete()\n\nFind out more about the Resource Catalog\n\nprint(f\"Livecheck: PING-{rc.ping()}\")\nprint(\"\\nAPI conforms to:\", *rc.get_conformance(), sep=\"\\n\")",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Using the Resource Catalogue"
    ]
  },
  {
    "objectID": "website/about.html",
    "href": "website/about.html",
    "title": "About",
    "section": "",
    "text": "The Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "website/about.html#using-this-website",
    "href": "website/about.html#using-this-website",
    "title": "About",
    "section": "Using this website",
    "text": "Using this website\nThe code in a given Jupyter notebook can either be copied and run on the Python command-line or in a script. The entire notebook can be downloaded and run wholly or stepwise through each cell. The code has been tested at the time of creation and will work with the required import packages.\nIf there are any issues running the code, please create an issue in the GitHub repository explaining the problem, post in the discussion forum or if you are able to fix the code then please create a pull request and add to these help pages.\nTo clone the repository you will git installed and to then use the following command in a suitable directory: git clone https://github.com/EO-DataHub/eodh-training.git\nThe notebooks can be run anywhere that a Jupyter server is available. To use on the AppHub, download the required notebooks locally and then upload them using the upload button at the top-left of the AppHub window.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "website/about.html#acronym-list",
    "href": "website/about.html#acronym-list",
    "title": "About",
    "section": "Acronym list",
    "text": "Acronym list\n\n\n\n\n\n\n\nAcronym\nMeaning\n\n\n\n\nADES\nApplication Deployment and Execution Service (the EODH workflow runner)\n\n\nAPI\nApplication Programming Interface\n\n\nARD\nAnalysis Ready Data\n\n\nAWS\nAmazon Web Services\n\n\nCEDA\nCentre for Environmental Data Analysis\n\n\nCWL\nCommon Workflow Language\n\n\nEO\nEarth observation\n\n\nEOAP\nEarth observation Application Package\n\n\nEODH\nEarth observation Data Hub\n\n\nHTTP\nHypertext Transfer Protocol\n\n\nRC\nResource Catalogue\n\n\nSAR\nSynthetic Aperture Radar\n\n\nSLC\nSingle Look Complex\n\n\nSTAC\nSpatio-Temporal Asset Catalog\n\n\nWP\nWeb Presence\n\n\nWR\nWorkflow Runner\n\n\nYAML\nYet Another Markup Language (or YAML ain’t markup language)",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EODH Training Materials",
    "section": "",
    "text": "Welcome\nThis site is in heavy development.\nWelcome to the eodh-training repository! This repository aims to provide a live set of documents to demonstrate how to use the EODH (Earth Observation Data Hub) and associated tools such as the pyeodh Python API client, eoap-gen workflow generator and QGIS plugin. Other training materials may be added to this repository in future.\nWhether you’re a user looking to explore the project or a developer wanting to contribute, you’ll find all the information you need here.\n\n\nContent\nYou’ll be able to find the content you require by navigating through this website using the sidebar to the left.\n\n\nAccess to EODH\nThe EODH project is keen to hear from interested early adopters and potential users. If you would like to apply for a user account, or just find out more about the project, please use enquiries@eodatahub.org.uk to contact the core development team.\n\n\nAdditional Support\nWe have set up an accessible Discussion Group as a location that users can interact with other users, as well as the Hub owners and developers. Please use this resource to build a vibrant community around the EODH platform. It is also accessible via the GitHub icon at the top of every page.\nIf you require specific development information on the API client then please check out the ReadTheDocs page for the client.\n\nThis website is made using Quarto.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "api-client/1_ClientIntro.html",
    "href": "api-client/1_ClientIntro.html",
    "title": "What is pyeodh?",
    "section": "",
    "text": "Description & purpose: A Python API client has been produced by Oxidian to allow simplified, yet powerful, access to the EODH API endpoints. This Notebook introduces the API client.\nAuthor(s): Alastair Graham\nDate created: 2024-11-08\nDate last modified: 2024-12-19\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nIntroduction\nAs part of the EODH project, pyeodh has been created. This is a lightweight Python client for easy access to EODH APIs.\nAn API client is a software tool or library designed to simplify interactions between a user’s application and an external API (Application Programming Interface). In the case of pyeodh, it is a Python-based tool tailored to facilitate communication with the specific API endpoints exposed by the EODH platform. Using pyeodh, developers and scientists can programmatically access the API’s features—such as sending requests, retrieving data, or executing commands—without needing to handle the underlying details such as crafting HTTP requests or managing authentication manually.\nBy abstracting these complexities, ‘pyeodh’ makes it easier to integrate the API into Python applications, enabling developers to focus on building features rather than managing low-level networking tasks.\n\n\nWhy is pyeodh needed?\nA key group of expected users are data scientists, and the key tools for this group tend to be written in Python. The pyeodh API client will simplify the interaction with the EODH platform allowing for more rapid, frictionless scientific development.\n\n\nBefore you start\nAccess to the EODH platform is largely free and open. However, in order to complete tasks on the workflow runner (WR) you will require an API token. To generate the token you will require a user account whch can be requested by contacting enquiries@eodatahub.org.uk. Once you have the account, login and navigate to ‘Workspaces’. Under the ‘Applications’ tab click on the ‘DataHub’ as shown in the screenshot below. From there you will be able to generate a new API token and manage other tokens that you have access to.",
    "crumbs": [
      "Simplifying access - pyeodh",
      "What is pyeodh?"
    ]
  },
  {
    "objectID": "api-client/3_WorkflowRunner.html",
    "href": "api-client/3_WorkflowRunner.html",
    "title": "Running Workflows",
    "section": "",
    "text": "Description & purpose: A set of code snippets and guidance to help users submit and execute workflows using the pyeodh API client.\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-09-10\nDate last modified: 2024-12-10\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nIntroduction\nThis notebook demonstrates how to use the EODH API through pyeodh to interact with the Workflow Runner (WR).\nThe EODH platform provides access to a component called the Application Deployment & Execution Service (ADES), otherwise known as the WR. To this, a user can deploy CWL workflows and execute parametrised processing jobs. pyeodh provides an interface to simplify interaction with ADES from python scripts.\n\n\nHow-to\nNote: This API requires authentication credentials to be provided by the user (in this case read from environment variables). This is a subject to change as the hub is implementing proper IAM solution. For information about obtaining API credentials, please see the page introducing the API client.\nNote: Once you have the necessary API token, create a .env file and point dotenv_path (see below) at the file. The file should contain at lease two lines in the following format:\nADES_USER=username ADES_TOKEN=api_token\nwhere username and api_token are the values you use on the system.\nFirst we need to instantiate the pyeodh client and create an ADES entrypoint.\n\n# package imports\nfrom requests import HTTPError\nimport os\nfrom pprint import pp\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nimport pyeodh\n\n# import API token information\ndotenv_path = Path('reqts/.env')\nload_dotenv(dotenv_path=dotenv_path)\n\nusername = os.getenv(\"ADES_USER\")\ntoken = os.getenv(\"ADES_TOKEN\")\n\n\n# Create the link to ADES\nclient = pyeodh.Client(username=username, token=token)\nades = client.get_ades()\n\nWorkflows (represented as executable processes) are predefined applications which can be parameterised and run by users. To get a list of currently available processes in our user workspace we need to implement the ades.get_processes() method:\n\n\nfor p in ades.get_processes():\n    print(p.id)\n\necho\nconvert-url\n\n\nOnce we can see how many workflows are in the workspace, it is possible to fetch information about a specific workflow (assuming the user knows its name). The process object also contains metadata giving us more information about the process and how to execute it.\nFor example, it is possible to interrogate the schema of inputs. From this it is possible to parameterise the process, or set the output schema.\n\nconvert_url_proc = ades.get_process(\"convert-url\")\n\nprint('Input Schema')\npp(convert_url_proc.inputs_schema)\n\nprint('-'*40)\n\nprint('Output Schema')\npp(convert_url_proc.outputs_schema)\n\nInput Schema\n{'fn': {'title': 'the operation to perform',\n        'description': 'the operation to perform',\n        'schema': {'type': 'string'}},\n 'size': {'title': 'the percentage for a resize operation',\n          'description': 'the percentage for a resize operation',\n          'schema': {'type': 'string'}},\n 'url': {'title': 'the image to convert',\n         'description': 'the image to convert',\n         'schema': {'type': 'string'}}}\n----------------------------------------\nOutput Schema\n{'converted_image': {'title': 'converted_image',\n                     'description': 'None',\n                     'extended-schema': {'oneOf': [{'allOf': [{'$ref': 'http://zoo-project.org/dl/link.json'},\n                                                              {'type': 'object',\n                                                               'properties': {'type': {'enum': ['application/json']}}}]},\n                                                   {'type': 'object',\n                                                    'required': ['value'],\n                                                    'properties': {'value': {'oneOf': [{'type': 'object'}]}}}]},\n                     'schema': {'oneOf': [{'type': 'object'}]}}}\n\n\nFurther information can be found out about the process object, including its id, process_id (name) and its status.\n\nfor j in ades.get_jobs():\n    print(j.id, j.process_id, j.status)\n\n672f0742-b6f5-11ef-a8b0-6a040e2afd6f convert-url successful\n\n\nOnly one process with the same ID can exist. To demonstrate deploying a process further down in this notebook, we first need to undeploy convert-url. Note that attempting to delete a non-existent process will result in an error.\n\ntry:\n    ades.get_process(\"convert-url\").delete()\nexcept HTTPError:\n    print(\"Process not found, no need to undeploy.\")\n\nLet’s deploy the convert-url process again.\nThere are two ways we can provide the CWL file - either referencing the file by URL or by passing the CWL file content directly. Note that the ades.deploy_process() command will fail if we try to create a process with an ID that already exists. If we want to update an existing process, we should use the process.update() method instead.\nBoth methods can handle URL or CWL YAML inputs. In the example below a process referenced by URL is deployed and then updated by passing the new CWL YAML content directly.\nNote: When updating a worklow you need to provide the entire workflow, the API does not support partial updates (i.e. to change the description we need to provide the entire workflow again).\n\nconvert_url_proc = ades.deploy_process(\n    cwl_url=\"https://raw.githubusercontent.com/EOEPCA/deployment-guide/main/deploy/samples/requests/processing/convert-url-app.cwl\"\n)\nprint(convert_url_proc.id, convert_url_proc.description)\n\nconvert-url Convert URL\n\n\n\ncwl_yaml = \"\"\"cwlVersion: v1.0\n$namespaces:\n  s: https://schema.org/\ns:softwareVersion: 0.1.2\nschemas:\n  - http://schema.org/version/9.0/schemaorg-current-http.rdf\n$graph:\n  # Workflow entrypoint\n  - class: Workflow\n    id: convert-url\n    label: convert url app\n    doc: Convert URL YAML\n    requirements:\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 1024\n    inputs:\n      fn:\n        label: the operation to perform\n        doc: the operation to perform\n        type: string\n      url:\n        label: the image to convert\n        doc: the image to convert\n        type: string\n      size:\n        label: the percentage for a resize operation\n        doc: the percentage for a resize operation\n        type: string\n    outputs:\n      - id: converted_image\n        type: Directory\n        outputSource:\n          - convert/results\n    steps:\n      convert:\n        run: \"#convert\"\n        in:\n          fn: fn\n          url: url\n          size: size\n        out:\n          - results\n  # convert.sh - takes input args `--url`\n  - class: CommandLineTool\n    id: convert\n    requirements:\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n    hints:\n      DockerRequirement:\n        dockerPull: eoepca/convert:latest\n    baseCommand: convert.sh\n    inputs:\n      fn:\n        type: string\n        inputBinding:\n          position: 1\n      url:\n        type: string\n        inputBinding:\n          position: 2\n          prefix: --url\n      size:\n        type: string\n        inputBinding:\n          position: 3\n    outputs:\n      results:\n        type: Directory\n        outputBinding:\n          glob: .\n\"\"\"\n\nconvert_url_proc.update(cwl_yaml=cwl_yaml)\nprint(convert_url_proc.id, convert_url_proc.description)\n\nconvert-url Convert URL YAML\n\n\nThe process needs to be parameterised before it is run, but how does a user know what inputs this particular workflow is expecting? That is where the process.inputs_schema response is useful (see above).\nLet’s execute the deployed process. The inputs are best supplied as a dictionary.\n\nconvert_url_job = convert_url_proc.execute(\n    {\n        \"fn\": \"resize\",\n        \"url\": \"https://eoepca.org/media_portal/images/logo6_med.original.png\",\n        \"size\": \"50%\",\n    }\n)\n\nprint(convert_url_job.id, convert_url_job.status, convert_url_job.message)\n\n007b69d0-b70b-11ef-aba4-6a040e2afd6f running ZOO-Kernel accepted to run your service!\n\n\nThe job should now be running.\nTo get the most up-to-date status of the job the user can call the job.refresh() method and then interrogate the job.status and job.message properties.\nNote: these properties only hold the latest response from the API, and don’t keep any historical records.\n\nconvert_url_job.refresh()\nprint(convert_url_job.id, convert_url_job.status, convert_url_job.message)\n\n007b69d0-b70b-11ef-aba4-6a040e2afd6f running upload required files\n\n\n\n# We can continually poll the job using a simple loop and print status and message updates like so:\n\nfrom pyeodh.ades import AdesJobStatus\nimport time\n\n\nold_status = \"\"\nold_message = \"\"\nwhile convert_url_job.status == AdesJobStatus.RUNNING.value:\n    time.sleep(2)\n    convert_url_job.refresh()\n    if convert_url_job.status != old_status:\n        print(\"\\n\")\n        print(f\"Status: {convert_url_job.status}\")\n    if convert_url_job.message != old_message:\n        print(f\"Message: {convert_url_job.message}\")\n\n    old_status = convert_url_job.status\n    old_message = convert_url_job.message\n\n\n\nStatus: running\nMessage: execution submitted\nMessage: delivering outputs, logs and usage report\n\n\nStatus: successful\nMessage: ZOO-Kernel successfully run your service!\n\n\nAfter the job has finished successfully, we can view the results as a link to where the data files are stored.\nNote: the outputs of a workflow is a directory conataining a STAC catalogue, where individual assets are represented in the metadata.\n\nresults = convert_url_job.get_result_items()\nfor res in results:\n    print(res.id, res.assets)\n\nlogo6_med.original-resize-1733844363.263300377 {'logo6_med.original-resize': &lt;Asset href=https://ajgwords.workspaces.test.eodhp.eco-ke-staging.com/files/workspaces-eodhp-test/processing-results/cat_007b69d0-b70b-11ef-aba4-6a040e2afd6f/col_007b69d0-b70b-11ef-aba4-6a040e2afd6f/logo6_med.original-resize.png&gt;}",
    "crumbs": [
      "Simplifying access - pyeodh",
      "Running Workflows"
    ]
  },
  {
    "objectID": "api-client/planet-stac-proxy.html",
    "href": "api-client/planet-stac-proxy.html",
    "title": "Remove in production release",
    "section": "",
    "text": "This notebook demostrates usage of the EODH resource catalog API using pyeodh\n\nEnsure\n\na clear description of purpose,\nintended audience and/or use case\nlinkages between notebooks and other training resources (if required)\n\n\n\nRecord\n\nTechnical dependencies,\nPlatform and Service Dependencies,\nPython Language versions,\nlibraries, additional scripts and files.\n\n\n\nRemember\n“When Jupyter notebooks are used in an educational context, they should not only be conceptualized to teach a specific topic but should also set a good example by following and implementing best practices for scientific computing”\n\nNeed in-order execution of notebook cells\nGood-quality code\nNo code duplication\nImports at the beginning of a notebook\nConsistent code style and formatting\nMeaningful names for variables\nLicence for code and training resources\n\n\n\n\nPlanet STAC Example\nDemo of retrieving data from Planet via their STAC proxy api using the PySTAC Client. Requires an api-key saved in as .planet.json which has the content {\"key\": \"API_KEY\"}.\n\nimport base64\nfrom pystac_client import Client\nimport requests\nfrom shapely import Point\nimport planet\nfrom IPython.display import Image\n\napi_key = planet.Auth.from_file().value\n\nuserpass = f\"{api_key}:\"\n\nheaders = {\n    \"Authorization\": f\"Basic {base64.b64encode(userpass.encode()).decode()}\"\n}\n\n\nclient = Client.open(\n    url=\"https://api.planet.com/x/data/\",\n    headers=headers\n)\n\nitem_search = client.search(\n    collections=['PSScene'],\n    datetime='2024-06-01/2024-08-01',\n    intersects=Point(-111, 45.68),\n    filter={\n        \"op\": \"&gt;=\",\n        \"args\": [{\"property\": \"clear_percent\"}, 50]\n    }\n)\n\nitem = next(item_search.items())\nthumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\nImage(data=thumbnail.content)\n\n\n\n\n\n\n\n\n\nimport pyeodh\nimport base64\nfrom io import BytesIO\nfrom pystac_client import Client\nimport requests\nfrom shapely import Point\nimport planet\nfrom IPython.display import Image\n\napi_key = planet.Auth.from_file().value\n\nuserpass = f\"{api_key}:\"\n\nheaders = {\n    \"Authorization\": f\"Basic {base64.b64encode(userpass.encode()).decode()}\"\n}\n\nclient = pyeodh.Client(username=api_key).get_catalog_service()\nitem_search = client.search(\n    collections=['PSScene'],\n    datetime='2024-06-01/2024-08-01',\n    intersects=Point(-111, 45.68),\n    filter={\n        \"op\": \"&gt;=\",\n        \"args\": [{\"property\": \"clear_percent\"}, 50]\n    }\n)\n\nitem = item_search[0]\nthumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\nImage(data=thumbnail.content)\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[10], line 29\n     18 client = pyeodh.Client(username=api_key).get_catalog_service()\n     19 item_search = client.search(\n     20     collections=['PSScene'],\n     21     datetime='2024-06-01/2024-08-01',\n   (...)\n     26     }\n     27 )\n---&gt; 29 item = item_search[0]\n     30 thumbnail = requests.get(item.assets[\"thumbnail\"].href, headers=headers)\n     31 Image(data=thumbnail.content)\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:82, in PaginatedList.__getitem__(self, index)\n     80 def __getitem__(self, index: int) -&gt; T:\n     81     assert isinstance(index, int)\n---&gt; 82     self._fetch_to_index(index)\n     83     return self._elements[index]\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:87, in PaginatedList._fetch_to_index(self, index)\n     85 def _fetch_to_index(self, index: int) -&gt; None:\n     86     while len(self._elements) &lt;= index and self._has_next():\n---&gt; 87         new_elements = self._fetch_next()\n     88         self._elements += new_elements\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/pagination.py:96, in PaginatedList._fetch_next(self)\n     94 if not self._next_url:\n     95     raise RuntimeError(\"Next url not specified!\")\n---&gt; 96 headers, resp_data = self._client._request_json(\n     97     self._method,\n     98     self._next_url,\n     99     headers=self._headers,\n    100     params=self._params,\n    101     data=self._data,\n    102 )\n    103 next_link = next(\n    104     filter(lambda ln: ln.get(\"rel\") == \"next\", resp_data.get(\"links\", {})), {}\n    105 )\n    106 self._next_url = next_link.get(\"href\")\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/client.py:102, in Client._request_json(self, method, url, headers, params, data, encode)\n     93 def _request_json(\n     94     self,\n     95     method: RequestMethod,\n   (...)\n    100     encode: Callable[[Any], tuple[str, Any]] = _encode_json,\n    101 ) -&gt; tuple[Headers, Any]:\n--&gt; 102     status, resp_headers, resp_data = self._request_json_raw(\n    103         method, url, headers, params, data, encode\n    104     )\n    106     if not len(resp_data):\n    107         return resp_headers, None\n\nFile ~/.local/lib/python3.11/site-packages/pyeodh/client.py:89, in Client._request_json_raw(self, method, url, headers, params, data, encode)\n     82 logger.debug(\n     83     f\"Received response {response.status_code}\\nheaders: {response.headers}\"\n     84     f\"\\ncontent: {response.text}\"\n     85 )\n     86 # TODO consider moving this to _requst_json() and raise own exceptions\n     87 # so that we can user _raw in e.g. delete methods where we expect a 409 and\n     88 # want to recover\n---&gt; 89 response.raise_for_status()\n     91 return response.status_code, response.headers, response.text\n\nFile /opt/jaspy/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n   1019     http_error_msg = (\n   1020         f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n   1021     )\n   1023 if http_error_msg:\n-&gt; 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 400 Client Error: Bad Request for url: https://test.eodatahub.org.uk/api/catalogue/stac/search"
  },
  {
    "objectID": "platform/2_Components.html",
    "href": "platform/2_Components.html",
    "title": "EODH Components",
    "section": "",
    "text": "Description & purpose: This webpage is designed to provide an introduction to the core components of the EODH.\nAuthor(s): Alastair Graham\nDate created: 2024-12-05\nDate last modified: 2024-12-17\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWeb Presence\nThe Web Presence (WP) is likely to be the route most new users take to interact with the EODH platform. This is a UI hosted on the Hub website that allows users to interface with EODH and manage their user account. From here API keys can be generated and managed, and the AppHub and Resource Catalogue can be accessed.\n\n\nResource Catalogue\nThe Resource Catalogue (RC) is a searchable store of data links and a way to access saved workflows and processed datasets. It is a collection of STAC catalogues (click for more information regarding STAC) that can be searched visually via the Web Presence or programatically via clients such as pyeodh. There are a series of fully open public resources, commercial resources that point towards an ordering service and private workspace resources. A complex resource access system manages this on the back-end.\n\n\nAppHub and Notebook Service\nThe AppHub is a JupyterHub instance made available to users to ensure simple access to information held within the Resource Catalogue. It also allows the creation of scientific analytical workflows and associated data processing. Once logged in to the AppHub a user is able to upload notebooks and data, or create new notebooks, from where they can interact with other EODH components using tools such as pyeodh.\n\n\nWorkflow Runner\nThe workflow runner (WR) is a component that the majority of users will not have direct access to but which will be important to any process looking to scale up data processing to generate a data service. Advanced technical users (ideally with some dev-ops experience) will be able to create workflows that run on the WR. However, all users will be able to submit publically available workflows to their workspace and run them, either via the AppHub, pyeodh or QGIS.",
    "crumbs": [
      "Hub Platform",
      "EODH Platform Components"
    ]
  },
  {
    "objectID": "plugin/2_Runningworkflows.html",
    "href": "plugin/2_Runningworkflows.html",
    "title": "Workflows from the Plugin",
    "section": "",
    "text": "Description & purpose: This notebook steps a user through the main way to use the QGIS plugin.\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-12-18\nDate last modified: 2024-12-18\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nConnect to the EODH\nThe first thing a user will need to do is supply their username an API token under the Settings tab. In this screengrab we can see that user gary is logging in to their EODH workspace. Plugin and pyeodh updates can also be installed from this part of the plugin.\n\n\n\nChoose and Parameterise\nIn gary’s workspace are two available workflows: * echo * convert-url\nIt is up to the creator of the workflow to provide a useful description for the CWL workflow. Also, if users will need to run them, the workflow should be held in an online location with accessible help and about information so that the user understands what the workflow does and how much resource it might use.\n\nThe convert-url workflow is a small demonstrator that takes the URL to an image and resizes it. This is the workflow used in this demonstrtaion. The image to be resized is the thumbnail for a sentinel2_ard asset available through the EODH. The thumbnail is accessible here and is shown below.\n\nOnce the user clicks on the Execute button they are taken to the parameterisation configuration. This rebuilds for different workflows, depending on the required inputs. Here we can see that a function (resize), a resizing value (50%) and the URL to the image are required. Once completed the user will press Execute to complete the processing. \n\n\nSuccess!\nThe plugin will report back the progress of the workflow on the platform. It will also provide access to any outputs, which will laso be available through the workspace interface on the user’s web prescence.\n\nInformation about a workflow run is also stored under the Jobs tab. From here, the status of all jobs that have been run in a user’s workspace will be available to be interrogated.",
    "crumbs": [
      "GIS interface - QGIS plugin",
      "Running A Workflow"
    ]
  },
  {
    "objectID": "presentations/DEFRA/1_202409_Defra_DataDiscovery.html",
    "href": "presentations/DEFRA/1_202409_Defra_DataDiscovery.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment\n\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh geopandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s data discovery\nThere are a number of API endpoints that are exposed by the EODH. Oxidian have developed a Python API Client, pyeodh, that makes the Hub’s API endpoints available to Python users. pyeodh is available on PyPi (https://pypi.org/project/pyeodh/) and can be installed using pip. Documentation for the API Client is available at: https://pyeodh.readthedocs.io/en/latest/api.html\nWe will use pyeodh throughout this presentation.\n\n# Imports\nimport pyeodh\n\nimport os\n\nimport shapely \nimport geopandas as gpd\nimport folium\n\nimport urllib.request\nfrom io import BytesIO \nfrom PIL import Image\n\nHaving imported the necessary libraries the next task is to set up the locations of the areas of interest. Having created the AOI points the user needs to connect to the Resource Catalogue so that they can start to find some data.\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\n\n# Optional cell\n# If you want to see these points on a map run this cell\n\n# Create a map (m) centered between the two points\ncenter_lat = (rut_pnt.y + thet_pnt.y) / 2\ncenter_lon = (rut_pnt.x + thet_pnt.x) / 2\n\nm = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n\n# Add markers for each point\nfolium.Marker([rut_pnt.y, rut_pnt.x], popup=\"Rutland Site\", icon=folium.Icon(color=\"blue\")).add_to(m)\nfolium.Marker([thet_pnt.y, thet_pnt.x], popup=\"Thetford Site\", icon=folium.Icon(color=\"green\")).add_to(m)\n\n# Step 4: Display the map\nm\n\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"{collect.id}: {collect.description}\")\n\ncmip6: CMIP6\ncordex: CORDEX\nukcp: UKCP\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\nairbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\nairbus_data_example: Airbus data\nsentinel2_ard: sentinel 2 ARD\nsentinel1: Sentinel 1\nnaip: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n\n\n\n# The next thing to do is find some open data\n# For this presentation we want to find Sentinel-2 analysis ready (ARD) imagery near Rutland\n\n# First we just want to understand some of the parameters linked to the data collection\n# We will just print the first 5 records and the dataset temporal extent   \nsentinel2_ard = client.get_catalog(\"supported-datasets/ceda-stac-fastapi\").get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\nlim = 5\ni = 0\n\nfor item in sentinel2_ard.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('DATASET TEMPORAL EXTENT: ', [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb\nDATASET TEMPORAL EXTENT:  ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# To find out information about all the imagery in the collection then use this cell\n# It undertakes a search for specific date ranges (November 2023) and limits the pagination return to 10\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n    ],\n    limit=10,\n)\n\n# The item id and start time of image capture can be printed\n# If end time is also required, add the following code to the print statement: item.properties[\"end_datetime\"]  \nfor item in item_search:\n    print(item.id, item.properties[\"start_datetime\"])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65 2023-11-21T11:43:49+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn527lonw0007_T30UXD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0037_T30UVC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0022_T30UWC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn518lonw0008_T30UXC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn510lonw0036_T30UVB_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.18.S2B_20231118_latn554lonw0053_T30UUG_ORB080_20231118122250_utm30n_osgb 2023-11-18T11:33:19+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn519lone0023_T31UDT_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn509lone0009_T31UCS_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\n\n\n\n# To find specific imagery for the Rutland site we need to add the intersects parameter. We set this to be our AOI point.\n# We can also filter the search by cloud cover, in this case limiting our search to images with less than 50% cloud in them\n\nitems = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n        'Cloud Coverage Assessment&lt;=50.0'\n    ],\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of items found: ', items.total_count)\n\nNumber of items found:  2\n\n\n\n# For the purposes of this presentation we will look at the second record ([1]) in more detail\n# First we need to understand what information we can access\n\na = items[1].to_dict()\nprint(a.keys())\n\ndict_keys(['type', 'stac_version', 'id', 'properties', 'geometry', 'links', 'assets', 'bbox', 'stac_extensions', 'collection'])\n\n\n\n# The data we want to access is stored under the 'assets' key. But what information is held in that? \nfor key in (a['assets']):\n    print(key)\n\ncloud\ncloud_probability\ncog\nmetadata\nsaturated_pixels\nthumbnail\ntopographic_shadow\nvalid_pixels\n\n\n\n# Now we can get the link to each of the different assets\nfor key, value in items[1].assets.items():\n    print(key, value.href)\n\ncloud https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds.tif\ncloud_probability https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds_prob.tif\ncog https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif\nmetadata https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml\nsaturated_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_sat.tif\nthumbnail https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_thumbnail.jpg\ntopographic_shadow https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_toposhad.tif\nvalid_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_valid.tif\n\n\n\n# We can use this information to view the image thumbnail\n\nasset_dict = items[1].assets\n\n# Get the url as a string\nthumbnail_assets = [v for k, v in asset_dict.items() if 'thumbnail' in k]\nthumbnail_url = thumbnail_assets[0].href\n\n# Here we open the remote URL, read the data and dislay the thumbnail \nwith urllib.request.urlopen(thumbnail_url) as url:\n    img = Image.open(BytesIO(url.read()))\n\ndisplay(img)\n\n\n\n\n\n\n\n\nThis shows that we can relatively easily interrogate the Resource Catalogue and filter the results so that we can find the data we require in the EODH. With a bit of tweaking of the code the user could also generate a list of assets and accompanying URLs to the datasets (for this and other datasets).\nNow our user wants to see what commercial data exists for the Thetford site.\n\n# Find some commercial data\n\nfor collect in client.get_collections():\n    if 'defra' in collect.id: \n        print(f\"{collect.id}: {collect.description}\")\n\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\n\n\n\n# Let's search for information on the Planet holdings  \nplanet = client.get_catalog(\"supported-datasets/defra\").get_collection('defra-planet')\nplanet.get_items()\n\nlim = 5\ni = 0\n\nfor item in planet.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('PLANET DATASET TEMPORAL EXTENT: ', [str(d) for d in planet.extent.temporal.intervals[0]])\n\n2024-08-23_strip_7527622_composite\n2024-08-23_strip_7527462_composite\nPLANET DATASET TEMPORAL EXTENT:  ['2024-08-23 11:09:19.358417+00:00', '2024-08-23 11:24:40.991786+00:00']\n\n\n\n# To find specific imagery for the Thetford site we need to add the intersects parameter. We set this to be our AOI point.\nitems1 = client.search(\n    collections=['defra-planet'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\nitems2 = client.search(\n    collections=['defra-airbus'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of Planet items found: ', items1.total_count)\nprint('Number of Airbus items found: ', items2.total_count)\n\nNumber of Planet items found:  1\nNumber of Airbus items found:  2\n\n\n\nfor key, value in items1[0].assets.items():\n    print('Planet: ', key, value)\n\nPlanet:  data &lt;Asset href=2024-08-23_strip_7527462_composite_file_format.tif&gt;\nPlanet:  udm2 &lt;Asset href=2024-08-23_strip_7527462_composite_udm2_file_format.tif&gt;\n\n\nThe final step would be to use the ordering service integrated into the EODH resource catalogue to purchase the required commercial imagery. This would be stored in a users workspace and could then be used in specific workflows or for data analytics (depending on licence restrictions).\nFor the purposes of this presentation we looked at the different commercial datasets offline in QGIS.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Discovery"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "title": "Demonstration for DEFRA",
    "section": "a) Viewing images with specific band configurations",
    "text": "a) Viewing images with specific band configurations\nOnce you have the URL to the Cloud Optimised Geotiff from the STAC records held in the resource catalogue it is possible to view different band combinations. The following code uses the URL to the Sentinel 2 ARD image discovered in the Data Discovery notebook and displays an interactive false colour composite.\n\n# Path to raster (URL or local path)\ndata_url = 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif'\n\n# Check that the dataset is a valid COG. If invalid, returns False\nvalidate_cog(data_url)\n\nTrue\n\n\n\n\n# First, create TileClient using example file\ndclient = TileClient(data_url)\n\n# Create 2 tile layers from same raster viewing different bands\nl = get_leaflet_tile_layer(dclient, indexes=[6, 2, 1])\n\n# Make the ipyleaflet map\nm = Map(center=dclient.center(), zoom=dclient.default_zoom)\nm.add(l)\nm.add_control(ScaleControl(position='bottomleft'))\nm.add_control(FullScreenControl())\nm\n\n\n\n\nimage.png\n\n\n\n# we can use the same tool to view rendered data stored locally e.g. in a user's workspace.\nmap = TileClient('data/S2A_clip_rend.tif')\nmap\n\n\n\n\nimage.png",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#b-towards-a-data-cube",
    "href": "presentations/DEFRA/3_202409_Defra_DataAnalysis.html#b-towards-a-data-cube",
    "title": "Demonstration for DEFRA",
    "section": "b) Towards a data cube",
    "text": "b) Towards a data cube\nThis is very much work in progress, and follows the outline of the tutorial supplied here. The tutorial at that link is designed to create a data cube in the USA using data held in an Element 84 STAC catalogue. Here we alter the processing to make use of the CEDA STAC catalogue. Ultimately both of these datasets will be held within the EODH resource catalogue and this code will be rewritten to utilise pyeodh and the EODH data holdings.\n\n# Set up\n# A helper method for changing bounding box representation to leaflet notation\n# (lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))\n\ndef convert_bounds(bbox, invert_y=False):\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\nThe next thing the user needs to do is find some data in a STAC catalogue that intersects with the site of interest for a given time period. Therefore, we need to know what datasets are available, over what period and for what locations.\n\n# Find some data using STAC\n# note: here we are using `pystac`. This will be replaced by `pyeodh` in future. \n\nurl = \"https://api.stac.ceda.ac.uk/\"\n\nclient = Client.open(url)\nfor coll in client.get_collections():\n    print(f\"{coll.id}: {coll.description}\")\n\nsentinel2_ard = client.get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\n# check the spatial and temporal extent of the collection\nprint('')\nprint(\"spatial extent:\", sentinel2_ard.extent.spatial.bboxes)\nprint(\"Temporal range:\", [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\ncmip6: CMIP6\ncordex: CORDEX\nland_cover: land_cover\nsentinel1: Sentinel 1\nsentinel2_ard: sentinel 2 ARD\nsst-cdrv3-collection: collection of EOCIS SST CDR V3\nukcp: UKCP\n\nspatial extent: [[-9.00034454651177, 49.48562028352171, 3.1494256015866995, 61.33444247301668]]\nTemporal range: ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# Find imagery that intersects with the site in Rutland \n# Uncomment max_items to limit the seatch to a set number of items\n\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-04-01',\n        'end_datetime&lt;=2023-10-01',\n        'eo:cloud_cover&lt;=75.0'\n      ],\n    # max_items=10,\n)\n\nitems = list(item_search.items())\nlen(items)\n\n30\n\n\nNow that we know we have available data we can start to build out a data cube.\n\n# First we create a dask client\n\nclientd = dask.distributed.Client()\nconfigure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client) # sets up gdal for cloud use\ndisplay(clientd)\n\n\nprint(f\"Found: {len(items):d} datasets\")\n\n# Convert STAC items into a GeoJSON FeatureCollection\nstac_json = item_search.item_collection_as_dict()\n\nFound: 30 datasets\n\n\n\ngdf = gpd.GeoDataFrame.from_features(stac_json, \"epsg:4326\")\ngdf.columns\n\nIndex(['geometry', 'file_count', 'start_datetime', 'end_datetime',\n       'NSSDC Identifier', 'created', 'Instrument Family Name',\n       'Platform Number', 'Datatake Type', 'esa_file_name',\n       'Ground Tracking Direction', 'datetime', 'instance_id', 'size',\n       'Product Type', 'Instrument Family Name Abbreviation',\n       'Start Orbit Number', 'eo:cloud_cover', 'Start Relative Orbit Number',\n       'updated', 'Instrument Mode', 'EPSG'],\n      dtype='object')\n\n\n\n# Plot an outline of the data items as a sense check\nf = folium.Figure(width=600, height=300)\nm = folium.Map(location=[52, 2], zoom_start=5).add_to(f)\n\ngdf.explore(\n    \"esa_file_name\",\n    categorical=True,\n    tooltip=[\n        \"esa_file_name\",\n        \"datetime\",\n        \"eo:cloud_cover\",\n    ],\n    popup=False,\n    legend=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"STAC\",\n    m=m,\n)\n\n\n\n\n\n# Construct the dask dataset\n\n# Since we will plot it on a map we need to use `EPSG:3857` projection\ncrs = \"epsg:3857\"\nzoom = 2**5  # overview level 5\n\nxx = stac_load(\n    items,\n    crs=crs,\n    resolution=10 * zoom,\n    chunks={\"x\": 2048, \"y\": 2048},  # &lt;-- use Dask\n    groupby=\"solar_day\",\n)\n\nprint(f\"Bands: {','.join(list(xx.data_vars))}\")\n#display(xx)\n\nBands: cloud,cloud_probability,thumbnail,topographic_shadow,cog,valid_pixels,saturated_pixels\n\n\nWe are currently looking into workarounds for a known conflict of using the eo STAC extension. Once a suitable way forward is identiified a full description of how to create and manipulate the CEDA Sentinel 2 ARD in a data cube (generated from STAC records) will be posted in the training materials repository.",
    "crumbs": [
      "Presentations",
      "DEFRA Data Analysis"
    ]
  },
  {
    "objectID": "presentations/Workshop/1_202502_workshop_DataDiscovery.html",
    "href": "presentations/Workshop/1_202502_workshop_DataDiscovery.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/\n\nWhat is the EODH?\nThe Earth Observation Data Hub is:\n“A UK Pathfinder project delivering access to Earth Observation (EO) data for effective decisionmaking across government, business and academia. The Earth Observation DataHub (EODH) brings together an expert project delivery team and industrial partners in an ambitious project… Users of the Hub will be able to explore areas of interest in the UK and across the globe… It will also enable selected users to support their own analyses, services and tools using the Hub’s workflow and compute environments.”\nMore details can be found online at https://eodatahub.org.uk/\nComponents of the Hub include:\n\nA Resource Catalogue - a STAC compliant catalogue of open and commercial satellite imagery, climate data, model results, workflows and more\nA Workflow Runner - a dedicated piece of cloud infrastructure to horizontally scale workflow requirements\nA Web Presence - an intuitive user interface to allow account management, data discovery and mapping\nAn App Hub - a science portal providing access to a Jupyter lab environment\n\n\nPresentation set up\nThe following cell only needs to be run on the EODH AppHub. If you have a local Python environment running, please install the required packages as you would normally.\n\n# If needed you can install a package in the current AppHub Jupyter environment using pip\n# For instance, we will need at least the following libraries\nimport sys\n!{sys.executable} -m pip install --upgrade pyeodh geopandas matplotlib numpy pillow folium\n\n\n\n\nEODH: it’s data discovery\nThere are a number of API endpoints that are exposed by the EODH. Oxidian have developed a Python API Client, pyeodh, that makes the Hub’s API endpoints available to Python users. pyeodh is available on PyPi (https://pypi.org/project/pyeodh/) and can be installed using pip. Documentation for the API Client is available at: https://pyeodh.readthedocs.io/en/latest/api.html\nWe will use pyeodh throughout this presentation.\n\n# Imports\nimport pyeodh\n\nimport os\n\nimport shapely \nimport geopandas as gpd\nimport folium\n\nimport urllib.request\nfrom io import BytesIO \nfrom PIL import Image\n\nHaving imported the necessary libraries the next task is to set up the locations of the areas of interest. Having created the AOI points the user needs to connect to the Resource Catalogue so that they can start to find some data.\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\n\n# Optional cell\n# If you want to see these points on a map run this cell\n\n# Create a map (m) centered between the two points\ncenter_lat = (rut_pnt.y + thet_pnt.y) / 2\ncenter_lon = (rut_pnt.x + thet_pnt.x) / 2\n\nm = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n\n# Add markers for each point\nfolium.Marker([rut_pnt.y, rut_pnt.x], popup=\"Rutland Site\", icon=folium.Icon(color=\"blue\")).add_to(m)\nfolium.Marker([thet_pnt.y, thet_pnt.x], popup=\"Thetford Site\", icon=folium.Icon(color=\"green\")).add_to(m)\n\n# Step 4: Display the map\nm\n\n\n# Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor collect in client.get_collections():\n    print(f\"{collect.id}: {collect.description}\")\n\ncmip6: CMIP6\ncordex: CORDEX\nukcp: UKCP\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\nairbus_sar_data: The German TerraSAR-X / TanDEM-X satellite formation and the Spanish PAZ satellite (managed by Hisdesat Servicios Estratégicos S.A.) are being operated in the same orbit tube and feature identical ground swaths and imaging modes - allowing Airbus and Hisdesat to establish a unique commercial Radar Constellation. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor in order to acquire datasets ranging from very high-resolution imagery to wide area coverage.\nairbus_data_example: Airbus data\nsentinel2_ard: sentinel 2 ARD\nsentinel1: Sentinel 1\nnaip: The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\n\n\n\n# The next thing to do is find some open data\n# For this presentation we want to find Sentinel-2 analysis ready (ARD) imagery near Rutland\n\n# First we just want to understand some of the parameters linked to the data collection\n# We will just print the first 5 records and the dataset temporal extent   \nsentinel2_ard = client.get_catalog(\"supported-datasets/ceda-stac-fastapi\").get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\nlim = 5\ni = 0\n\nfor item in sentinel2_ard.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('DATASET TEMPORAL EXTENT: ', [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb\nDATASET TEMPORAL EXTENT:  ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# To find out information about all the imagery in the collection then use this cell\n# It undertakes a search for specific date ranges (November 2023) and limits the pagination return to 10\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n    ],\n    limit=10,\n)\n\n# The item id and start time of image capture can be printed\n# If end time is also required, add the following code to the print statement: item.properties[\"end_datetime\"]  \nfor item in item_search:\n    print(item.id, item.properties[\"start_datetime\"])\n\nneodc.sentinel_ard.data.sentinel_2.2023.11.21.S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65 2023-11-21T11:43:49+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn563lonw0037_T30VVH_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn546lonw0037_T30UVF_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn536lonw0007_T30UXE_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn528lonw0022_T30UWD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn527lonw0007_T30UXD_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0037_T30UVC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn519lonw0022_T30UWC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn518lonw0008_T30UXC_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.20.S2A_20231120_latn510lonw0036_T30UVB_ORB037_20231120132420_utm30n_osgb 2023-11-20T11:23:51+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.18.S2B_20231118_latn554lonw0053_T30UUG_ORB080_20231118122250_utm30n_osgb 2023-11-18T11:33:19+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn519lone0023_T31UDT_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\nneodc.sentinel_ard.data.sentinel_2.2023.11.17.S2A_20231117_latn509lone0009_T31UCS_ORB137_20231117131218_utm31n_osgb 2023-11-17T11:13:31+00:00\n\n\n\n# To find specific imagery for the Rutland site we need to add the intersects parameter. We set this to be our AOI point.\n# We can also filter the search by cloud cover, in this case limiting our search to images with less than 50% cloud in them\n\nitems = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"supported-datasets/ceda-stac-fastapi\"],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-11-01',\n        'end_datetime&lt;=2023-11-30', \n        'Cloud Coverage Assessment&lt;=50.0'\n    ],\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of items found: ', items.total_count)\n\nNumber of items found:  2\n\n\n\n# For the purposes of this presentation we will look at the second record ([1]) in more detail\n# First we need to understand what information we can access\n\na = items[1].to_dict()\nprint(a.keys())\n\ndict_keys(['type', 'stac_version', 'id', 'properties', 'geometry', 'links', 'assets', 'bbox', 'stac_extensions', 'collection'])\n\n\n\n# The data we want to access is stored under the 'assets' key. But what information is held in that? \nfor key in (a['assets']):\n    print(key)\n\ncloud\ncloud_probability\ncog\nmetadata\nsaturated_pixels\nthumbnail\ntopographic_shadow\nvalid_pixels\n\n\n\n# Now we can get the link to each of the different assets\nfor key, value in items[1].assets.items():\n    print(key, value.href)\n\ncloud https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds.tif\ncloud_probability https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_clouds_prob.tif\ncog https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif\nmetadata https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml\nsaturated_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_sat.tif\nthumbnail https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_thumbnail.jpg\ntopographic_shadow https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_toposhad.tif\nvalid_pixels https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_valid.tif\n\n\n\n# We can use this information to view the image thumbnail\n\nasset_dict = items[1].assets\n\n# Get the url as a string\nthumbnail_assets = [v for k, v in asset_dict.items() if 'thumbnail' in k]\nthumbnail_url = thumbnail_assets[0].href\n\n# Here we open the remote URL, read the data and dislay the thumbnail \nwith urllib.request.urlopen(thumbnail_url) as url:\n    img = Image.open(BytesIO(url.read()))\n\ndisplay(img)\n\n\n\n\n\n\n\n\nThis shows that we can relatively easily interrogate the Resource Catalogue and filter the results so that we can find the data we require in the EODH. With a bit of tweaking of the code the user could also generate a list of assets and accompanying URLs to the datasets (for this and other datasets).\nNow our user wants to see what commercial data exists for the Thetford site.\n\n# Find some commercial data\n\nfor collect in client.get_collections():\n    if 'defra' in collect.id: \n        print(f\"{collect.id}: {collect.description}\")\n\ndefra-airbus: A collection of Airbus data for the DEFRA use case.\ndefra-planet: A collection of Planet data for the DEFRA use case.\n\n\n\n# Let's search for information on the Planet holdings  \nplanet = client.get_catalog(\"supported-datasets/defra\").get_collection('defra-planet')\nplanet.get_items()\n\nlim = 5\ni = 0\n\nfor item in planet.get_items():\n    if i &lt; lim:\n        print(item.id)\n        i += 1\n\nprint('PLANET DATASET TEMPORAL EXTENT: ', [str(d) for d in planet.extent.temporal.intervals[0]])\n\n2024-08-23_strip_7527622_composite\n2024-08-23_strip_7527462_composite\nPLANET DATASET TEMPORAL EXTENT:  ['2024-08-23 11:09:19.358417+00:00', '2024-08-23 11:24:40.991786+00:00']\n\n\n\n# To find specific imagery for the Thetford site we need to add the intersects parameter. We set this to be our AOI point.\nitems1 = client.search(\n    collections=['defra-planet'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\nitems2 = client.search(\n    collections=['defra-airbus'],\n    catalog_paths=[\"supported-datasets/defra\"],\n    intersects=thet_pnt,\n    limit=10,\n)\n\n# We can then count the number of items returned by the search \nprint('Number of Planet items found: ', items1.total_count)\nprint('Number of Airbus items found: ', items2.total_count)\n\nNumber of Planet items found:  1\nNumber of Airbus items found:  2\n\n\n\nfor key, value in items1[0].assets.items():\n    print('Planet: ', key, value)\n\nPlanet:  data &lt;Asset href=2024-08-23_strip_7527462_composite_file_format.tif&gt;\nPlanet:  udm2 &lt;Asset href=2024-08-23_strip_7527462_composite_udm2_file_format.tif&gt;\n\n\nThe final step would be to use the ordering service integrated into the EODH resource catalogue to purchase the required commercial imagery. This would be stored in a users workspace and could then be used in specific workflows or for data analytics (depending on licence restrictions).\nFor the purposes of this presentation we looked at the different commercial datasets offline in QGIS."
  },
  {
    "objectID": "presentations/Workshop/3_202502_workshop_DataAnalysis.html",
    "href": "presentations/Workshop/3_202502_workshop_DataAnalysis.html",
    "title": "Demonstration for DEFRA",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase the initial functionality of the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of September 2024. The Notebook “user” would like to understand more about the satellite data available for their test areas. This user is also interested in obtaining a series of smaller images and ultimately creating a data cube. The Notebook series (of 3) is designed in such a way that it can be run on the EODH AppHub (Notebook Service) or from a local environment.\nAuthor(s): Alastair Graham, Dusan Figala, Phil Kershaw\nDate created: 2024-09-05\nDate last modified: 2024-09-18\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nLinks: * Oxidian: https://www.oxidian.com/ * CEDA: https://www.ceda.ac.uk/ * EO Data Hub: https://eodatahub.org.uk/"
  },
  {
    "objectID": "presentations/Workshop/3_202502_workshop_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "href": "presentations/Workshop/3_202502_workshop_DataAnalysis.html#a-viewing-images-with-specific-band-configurations",
    "title": "Demonstration for DEFRA",
    "section": "a) Viewing images with specific band configurations",
    "text": "a) Viewing images with specific band configurations\nOnce you have the URL to the Cloud Optimised Geotiff from the STAC records held in the resource catalogue it is possible to view different band combinations. The following code uses the URL to the Sentinel 2 ARD image discovered in the Data Discovery notebook and displays an interactive false colour composite.\n\n# Path to raster (URL or local path)\ndata_url = 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/17/S2A_20231117_latn527lonw0007_T30UXD_ORB137_20231117131218_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif'\n\n# Check that the dataset is a valid COG. If invalid, returns False\nvalidate_cog(data_url)\n\nTrue\n\n\n\n\n# First, create TileClient using example file\ndclient = TileClient(data_url)\n\n# Create 2 tile layers from same raster viewing different bands\nl = get_leaflet_tile_layer(dclient, indexes=[6, 2, 1])\n\n# Make the ipyleaflet map\nm = Map(center=dclient.center(), zoom=dclient.default_zoom)\nm.add(l)\nm.add_control(ScaleControl(position='bottomleft'))\nm.add_control(FullScreenControl())\nm\n\n\n\n\nimage.png\n\n\n\n# we can use the same tool to view rendered data stored locally e.g. in a user's workspace.\nmap = TileClient('data/S2A_clip_rend.tif')\nmap\n\n\n\n\nimage.png"
  },
  {
    "objectID": "presentations/Workshop/3_202502_workshop_DataAnalysis.html#b-towards-a-data-cube",
    "href": "presentations/Workshop/3_202502_workshop_DataAnalysis.html#b-towards-a-data-cube",
    "title": "Demonstration for DEFRA",
    "section": "b) Towards a data cube",
    "text": "b) Towards a data cube\nThis is very much work in progress, and follows the outline of the tutorial supplied here. The tutorial at that link is designed to create a data cube in the USA using data held in an Element 84 STAC catalogue. Here we alter the processing to make use of the CEDA STAC catalogue. Ultimately both of these datasets will be held within the EODH resource catalogue and this code will be rewritten to utilise pyeodh and the EODH data holdings.\n\n# Set up\n# A helper method for changing bounding box representation to leaflet notation\n# (lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))\n\ndef convert_bounds(bbox, invert_y=False):\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n# Areas of Interest\nrut_pnt = shapely.Point(-0.683261054299237, 52.672193937442586) # a site near Rutland\nthet_pnt = shapely.Point(0.6715892933273722, 52.414471075812315) # a site near Thetford\n\nThe next thing the user needs to do is find some data in a STAC catalogue that intersects with the site of interest for a given time period. Therefore, we need to know what datasets are available, over what period and for what locations.\n\n# Find some data using STAC\n# note: here we are using `pystac`. This will be replaced by `pyeodh` in future. \n\nurl = \"https://api.stac.ceda.ac.uk/\"\n\nclient = Client.open(url)\nfor coll in client.get_collections():\n    print(f\"{coll.id}: {coll.description}\")\n\nsentinel2_ard = client.get_collection('sentinel2_ard')\nsentinel2_ard.get_items()\n\n# check the spatial and temporal extent of the collection\nprint('')\nprint(\"spatial extent:\", sentinel2_ard.extent.spatial.bboxes)\nprint(\"Temporal range:\", [str(d) for d in sentinel2_ard.extent.temporal.intervals[0]])\n\ncmip6: CMIP6\ncordex: CORDEX\nland_cover: land_cover\nsentinel1: Sentinel 1\nsentinel2_ard: sentinel 2 ARD\nsst-cdrv3-collection: collection of EOCIS SST CDR V3\nukcp: UKCP\n\nspatial extent: [[-9.00034454651177, 49.48562028352171, 3.1494256015866995, 61.33444247301668]]\nTemporal range: ['2023-01-01 11:14:51+00:00', '2023-11-01 11:43:49+00:00']\n\n\n\n# Find imagery that intersects with the site in Rutland \n# Uncomment max_items to limit the seatch to a set number of items\n\nitem_search = client.search(\n    collections=['sentinel2_ard'],\n    intersects=rut_pnt,\n    query=[\n        'start_datetime&gt;=2023-04-01',\n        'end_datetime&lt;=2023-10-01',\n        'eo:cloud_cover&lt;=75.0'\n      ],\n    # max_items=10,\n)\n\nitems = list(item_search.items())\nlen(items)\n\n30\n\n\nNow that we know we have available data we can start to build out a data cube.\n\n# First we create a dask client\n\nclientd = dask.distributed.Client()\nconfigure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client) # sets up gdal for cloud use\ndisplay(clientd)\n\n\nprint(f\"Found: {len(items):d} datasets\")\n\n# Convert STAC items into a GeoJSON FeatureCollection\nstac_json = item_search.item_collection_as_dict()\n\nFound: 30 datasets\n\n\n\ngdf = gpd.GeoDataFrame.from_features(stac_json, \"epsg:4326\")\ngdf.columns\n\nIndex(['geometry', 'file_count', 'start_datetime', 'end_datetime',\n       'NSSDC Identifier', 'created', 'Instrument Family Name',\n       'Platform Number', 'Datatake Type', 'esa_file_name',\n       'Ground Tracking Direction', 'datetime', 'instance_id', 'size',\n       'Product Type', 'Instrument Family Name Abbreviation',\n       'Start Orbit Number', 'eo:cloud_cover', 'Start Relative Orbit Number',\n       'updated', 'Instrument Mode', 'EPSG'],\n      dtype='object')\n\n\n\n# Plot an outline of the data items as a sense check\nf = folium.Figure(width=600, height=300)\nm = folium.Map(location=[52, 2], zoom_start=5).add_to(f)\n\ngdf.explore(\n    \"esa_file_name\",\n    categorical=True,\n    tooltip=[\n        \"esa_file_name\",\n        \"datetime\",\n        \"eo:cloud_cover\",\n    ],\n    popup=False,\n    legend=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"STAC\",\n    m=m,\n)\n\n\n\n\n\n# Construct the dask dataset\n\n# Since we will plot it on a map we need to use `EPSG:3857` projection\ncrs = \"epsg:3857\"\nzoom = 2**5  # overview level 5\n\nxx = stac_load(\n    items,\n    crs=crs,\n    resolution=10 * zoom,\n    chunks={\"x\": 2048, \"y\": 2048},  # &lt;-- use Dask\n    groupby=\"solar_day\",\n)\n\nprint(f\"Bands: {','.join(list(xx.data_vars))}\")\n#display(xx)\n\nBands: cloud,cloud_probability,thumbnail,topographic_shadow,cog,valid_pixels,saturated_pixels\n\n\nWe are currently looking into workarounds for a known conflict of using the eo STAC extension. Once a suitable way forward is identiified a full description of how to create and manipulate the CEDA Sentinel 2 ARD in a data cube (generated from STAC records) will be posted in the training materials repository."
  },
  {
    "objectID": "science/1_Snippets.html",
    "href": "science/1_Snippets.html",
    "title": "Code Snippets",
    "section": "",
    "text": "Description & purpose: This Notebook is designed to showcase different tools and functions that are of scientific interest/use and implement the Earth Observation Data Hub. It provides a snapshot of the Hub, the pyeodh API client and the various datasets as of November 2024.\nAuthor(s): Alastair Graham\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This notebook is licensed under Creative Commons Attribution-ShareAlike 4.0 International. The code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nCreate data cube using sentinel2_ard STAC records\nUsers may have a need to generate a data cube directly from the CEDA Sentinel 2 Analysis Ready Data (ARD) (sentinel2_ard). As the STAC catalogue for that dataset has been created in a specific way, users will need to be aware of this and make alterations from the default. The following code was supplied by Pete Gadomski of Development Seed and enables the generation of a data cube from the ARD catalogue (as structured in November 2024). The STAC catalogue may be reprocessed in future to allow the default settings for odc-stac to be implemented.\n\n# Note that this uses pystac and pystac_client\n\nimport os\n\nimport dask.distributed\nimport odc.stac\nimport rasterio\nfrom pystac.extensions.raster import RasterBand\nfrom pystac_client import Client\n\n\nclient = dask.distributed.Client()\n\n\nclient = Client.open(\"https://api.stac.ceda.ac.uk/\")\nitem_collection = client.search(\n    collections=[\"sentinel2_ard\"],\n    intersects={\"type\": \"Point\", \"coordinates\": [-1.3144, 51.5755]},\n    sortby=\"-properties.datetime\",\n    max_items=10,\n).item_collection()\n\n# Commented out for clarity when submitted to training materials website.\n# Uncomment the next line to view the Feature Collection\n\n#item_collection\n\nAs of November 2024, the STAC items in sentinel2_ard are missing information because the projection and raster extensions are not installed. This means that odc-stac will refuse to compute the “geobox” and hence the subsequent components needed to generate and view the data cube. The code in the following cell adds those extensions.\n\nfor item in item_collection.items:\n    asset = item.assets[\"cog\"]\n    cog = rasterio.open(asset.href)\n    epsg = cog.crs.to_epsg()\n    dtypes = cog.dtypes\n    shape = cog.shape\n    transform = list(cog.transform)\n\n    item.ext.add(\"proj\")\n    item.ext.add(\"raster\")\n\n    item.ext.proj.epsg = epsg\n\n    cog = item.assets[\"cog\"]\n    cog.ext.raster.bands = [RasterBand.create(data_type=dtype) for dtype in dtypes]\n    cog.ext.proj.shape = shape\n    cog.ext.proj.transform = transform\n\n\nbbox = [-1.2, 51.6, -1.1, 51.7]\ndataset = odc.stac.load(item_collection, bands=(\"B03\", \"B04\", \"B08\"), chunks={}, bbox=bbox)\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 48MB\nDimensions:      (y: 1121, x: 706, time: 10)\nCoordinates:\n  * y            (y) float64 9kB 2.005e+05 2.005e+05 ... 1.893e+05 1.893e+05\n  * x            (x) float64 6kB 4.554e+05 4.554e+05 ... 4.624e+05 4.624e+05\n    spatial_ref  int32 4B 27700\n  * time         (time) datetime64[ns] 80B 2023-10-03T11:08:09 ... 2023-11-20...\nData variables:\n    B03          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n    B04          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n    B08          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;xarray.DatasetDimensions:y: 1121x: 706time: 10Coordinates: (4)y(y)float642.005e+05 2.005e+05 ... 1.893e+05units :metreresolution :-10.0crs :EPSG:27700array([200525., 200515., 200505., ..., 189345., 189335., 189325.])x(x)float644.554e+05 4.554e+05 ... 4.624e+05units :metreresolution :10.0crs :EPSG:27700array([455385., 455395., 455405., ..., 462415., 462425., 462435.])spatial_ref()int3227700spatial_ref :PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9.01,61.01,2.01]],ID[\"EPSG\",27700]]crs_wkt :PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9.01,61.01,2.01]],ID[\"EPSG\",27700]]semi_major_axis :6377563.396semi_minor_axis :6356256.909237285inverse_flattening :299.3249646reference_ellipsoid_name :Airy 1830longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :OSGB36horizontal_datum_name :Ordnance Survey of Great Britain 1936projected_crs_name :OSGB36 / British National Gridgrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :49.0longitude_of_central_meridian :-2.0false_easting :400000.0false_northing :-100000.0scale_factor_at_central_meridian :0.9996012717GeoTransform :455380 10 0 200530 0 -10array(27700, dtype=int32)time(time)datetime64[ns]2023-10-03T11:08:09 ... 2023-11-...array(['2023-10-03T11:08:09.000000000', '2023-10-06T11:21:19.000000000',\n       '2023-10-08T11:09:41.000000000', '2023-10-13T11:09:19.000000000',\n       '2023-10-26T11:21:19.000000000', '2023-10-28T11:11:51.000000000',\n       '2023-11-10T11:23:11.000000000', '2023-11-15T11:22:39.000000000',\n       '2023-11-17T11:13:31.000000000', '2023-11-20T11:23:51.000000000'],\n      dtype='datetime64[ns]')Data variables: (3)B03(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n15.10 MiB\n1.51 MiB\n\n\nShape\n(10, 1121, 706)\n(1, 1121, 706)\n\n\nDask graph\n10 chunks in 3 graph layers\n\n\nData type\nuint16 numpy.ndarray\n\n\n\n\n                                           706 1121 10\n\n\n\n\nB04(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n15.10 MiB\n1.51 MiB\n\n\nShape\n(10, 1121, 706)\n(1, 1121, 706)\n\n\nDask graph\n10 chunks in 3 graph layers\n\n\nData type\nuint16 numpy.ndarray\n\n\n\n\n                                           706 1121 10\n\n\n\n\nB08(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n15.10 MiB\n1.51 MiB\n\n\nShape\n(10, 1121, 706)\n(1, 1121, 706)\n\n\nDask graph\n10 chunks in 3 graph layers\n\n\nData type\nuint16 numpy.ndarray\n\n\n\n\n                                           706 1121 10\n\n\n\n\nIndexes: (3)yPandasIndexPandasIndex(Index([200525.0, 200515.0, 200505.0, 200495.0, 200485.0, 200475.0, 200465.0,\n       200455.0, 200445.0, 200435.0,\n       ...\n       189415.0, 189405.0, 189395.0, 189385.0, 189375.0, 189365.0, 189355.0,\n       189345.0, 189335.0, 189325.0],\n      dtype='float64', name='y', length=1121))xPandasIndexPandasIndex(Index([455385.0, 455395.0, 455405.0, 455415.0, 455425.0, 455435.0, 455445.0,\n       455455.0, 455465.0, 455475.0,\n       ...\n       462345.0, 462355.0, 462365.0, 462375.0, 462385.0, 462395.0, 462405.0,\n       462415.0, 462425.0, 462435.0],\n      dtype='float64', name='x', length=706))timePandasIndexPandasIndex(DatetimeIndex(['2023-10-03 11:08:09', '2023-10-06 11:21:19',\n               '2023-10-08 11:09:41', '2023-10-13 11:09:19',\n               '2023-10-26 11:21:19', '2023-10-28 11:11:51',\n               '2023-11-10 11:23:11', '2023-11-15 11:22:39',\n               '2023-11-17 11:13:31', '2023-11-20 11:23:51'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\ndataset.odc.geobox\ndataset = dataset.compute()\n\n\n_ = dataset.isel(time=0).to_array(\"band\").plot.imshow(vmin=0, vmax=256)",
    "crumbs": [
      "Science & code examples",
      "Code Snippets"
    ]
  },
  {
    "objectID": "science/3_Use_Cases.html",
    "href": "science/3_Use_Cases.html",
    "title": "Use Cases for Climate Datasets",
    "section": "",
    "text": "Description & purpose: This notebook is designed to introduce the user to model data held within the CMIP6 STAC and to suggest simple ways to visualise the information held within the NetCDF files that are accessed through the STAC asset URLs and Kerchunk reference files.\nAuthor(s): Alastair Graham, Dusan Figala, Daniel Westwood\nDate created: 2024-12-12\nDate last modified: 2025-02-10\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nWIP Icon Attribution"
  },
  {
    "objectID": "science/3_Use_Cases.html#plot-a-single-time-step",
    "href": "science/3_Use_Cases.html#plot-a-single-time-step",
    "title": "Use Cases for Climate Datasets",
    "section": "1. Plot a single time-step",
    "text": "1. Plot a single time-step\nFrom the above information (and the file name) we can see that the dataset contained in the NetCDF file is precipitation flux (pr). There are a large number of timeperiods that have been included in the file. We can plot a single slice of the data using GeoViews as laid out in the following code.\n\n%matplotlib inline\n# select the first time step\noneframe = data.pr.isel(time=0)\n\n# generate the image to show\nfigdata = gv.Dataset(oneframe, [\"lon\", \"lat\"], crs=crs.PlateCarree())\nimages = figdata.to(gv.Image)\n\n# Plot\nimages.opts(cmap=\"viridis\", colorbar=True, width=600, height=500) * gf.coastline\n\n\n\n\n\n  \n\n\n\n\n## 2. Plot a regional average\nWe can perform multiple selections and operations on the data lazily, where those operations are not computed until the data is plotted at the end. Here we take an example of selecting some cloud height data and plotting the average over a period of 5 years.\n\n# Another way to do the item search and return an object\nitem_search = client.search(\n    # collections=[\"cmip6\"],\n    ids=[\n        \"CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222\"\n    ],\n    limit=10,\n)\nitem=item_search[0]\n\nWe can very quickly retrieve the cloud-format product to use for accessing the data.\n\nproduct = item.get_cloud_products()\nproduct\n\n&lt;DataPointCloudProduct: CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222-reference_file (Format: kerchunk)&gt;\n - bbox: [-180.0, -89.14152, 178.875, 89.14152]\n - asset_id: CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222-reference_file\n - cloud_format: kerchunk\nAttributes:\n - datetime: 2057-12-31T12:00:00+00:00\n\n\n\nds = product.open_dataset()\nds\n\nWARNING [ceda_datapoint.core.cloud]: Property \"mapper_kwargs\" for None is undefined.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 211MB\nDimensions:    (time: 1032, lat: 160, lon: 320, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 1kB -89.14 -88.03 -86.91 ... 86.91 88.03 89.14\n  * lon        (lon) float64 3kB 0.0 1.125 2.25 3.375 ... 356.6 357.8 358.9\n  * time       (time) datetime64[ns] 8kB 2015-01-16T12:00:00 ... 2100-12-16T1...\nDimensions without coordinates: bnds\nData variables:\n    clt        (time, lat, lon) float32 211MB dask.array&lt;chunksize=(1, 160, 320), meta=np.ndarray&gt;\n    lat_bnds   (lat, bnds) float64 3kB dask.array&lt;chunksize=(160, 2), meta=np.ndarray&gt;\n    lon_bnds   (lon, bnds) float64 5kB dask.array&lt;chunksize=(320, 2), meta=np.ndarray&gt;\n    time_bnds  (time, bnds) datetime64[ns] 17kB dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes: (12/44)\n    Conventions:            CF-1.7 CMIP-6.2\n    activity_id:            ScenarioMIP\n    branch_method:          standard\n    branch_time_in_child:   60265.0\n    branch_time_in_parent:  60265.0\n    cmor_version:           3.4.0\n    ...                     ...\n    table_id:               Amon\n    table_info:             Creation Date:(14 December 2018) MD5:b2d32d1a0d9b...\n    title:                  MRI-ESM2-0 output prepared for CMIP6\n    tracking_id:            hdl:21.14100/3fef816c-0095-4950-a45a-ba080ec2b7eb\n    variable_id:            clt\n    variant_label:          r1i1p1f1xarray.DatasetDimensions:time: 1032lat: 160lon: 320bnds: 2Coordinates: (3)lat(lat)float64-89.14 -88.03 ... 88.03 89.14axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_northarray([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398, -77.94262, -76.82124,\n       -75.69984, -74.57843, -73.45701, -72.33558, -71.21414, -70.09269,\n       -68.97124, -67.84978, -66.72833, -65.60686, -64.4854 , -63.36393,\n       -62.24246, -61.12099, -59.99952, -58.87804, -57.75657, -56.63509,\n       -55.51361, -54.39214, -53.27066, -52.14917, -51.02769, -49.90621,\n       -48.78473, -47.66325, -46.54176, -45.42028, -44.29879, -43.17731,\n       -42.05582, -40.93434, -39.81285, -38.69137, -37.56988, -36.44839,\n       -35.32691, -34.20542, -33.08393, -31.96244, -30.84096, -29.71947,\n       -28.59798, -27.47649, -26.355  , -25.23351, -24.11203, -22.99054,\n       -21.86905, -20.74756, -19.62607, -18.50458, -17.38309, -16.2616 ,\n       -15.14011, -14.01862, -12.89713, -11.77564, -10.65415,  -9.53266,\n        -8.41117,  -7.28968,  -6.16819,  -5.0467 ,  -3.92521,  -2.80372,\n        -1.68223,  -0.56074,   0.56074,   1.68223,   2.80372,   3.92521,\n         5.0467 ,   6.16819,   7.28968,   8.41117,   9.53266,  10.65415,\n        11.77564,  12.89713,  14.01862,  15.14011,  16.2616 ,  17.38309,\n        18.50458,  19.62607,  20.74756,  21.86905,  22.99054,  24.11203,\n        25.23351,  26.355  ,  27.47649,  28.59798,  29.71947,  30.84096,\n        31.96244,  33.08393,  34.20542,  35.32691,  36.44839,  37.56988,\n        38.69137,  39.81285,  40.93434,  42.05582,  43.17731,  44.29879,\n        45.42028,  46.54176,  47.66325,  48.78473,  49.90621,  51.02769,\n        52.14917,  53.27066,  54.39214,  55.51361,  56.63509,  57.75657,\n        58.87804,  59.99952,  61.12099,  62.24246,  63.36393,  64.4854 ,\n        65.60686,  66.72833,  67.84978,  68.97124,  70.09269,  71.21414,\n        72.33558,  73.45701,  74.57843,  75.69984,  76.82124,  77.94262,\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152])lon(lon)float640.0 1.125 2.25 ... 357.8 358.9axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_eastarray([  0.   ,   1.125,   2.25 , ..., 356.625, 357.75 , 358.875])time(time)datetime64[ns]2015-01-16T12:00:00 ... 2100-12-...axis :Tbounds :time_bndslong_name :timestandard_name :timearray(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', ..., '2100-10-16T12:00:00.000000000',\n       '2100-11-16T00:00:00.000000000', '2100-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (4)clt(time, lat, lon)float32dask.array&lt;chunksize=(1, 160, 320), meta=np.ndarray&gt;cell_measures :area: areacellacell_methods :area: time: meancomment :Total cloud area fraction for the whole atmospheric column, as seen from the surface or the top of the atmosphere. Includes both large-scale and convective cloud.history :2019-02-20T14:25:47Z altered by CMOR: replaced missing value flag (-9.99e+33) with standard missing value (1e+20).long_name :Total Cloud Cover Percentageoriginal_name :TCLOUDstandard_name :cloud_area_fractionunits :%\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n201.56 MiB\n200.00 kiB\n\n\nShape\n(1032, 160, 320)\n(1, 160, 320)\n\n\nDask graph\n1032 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             320 160 1032\n\n\n\n\nlat_bnds(lat, bnds)float64dask.array&lt;chunksize=(160, 2), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.50 kiB\n2.50 kiB\n\n\nShape\n(160, 2)\n(160, 2)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         2 160\n\n\n\n\nlon_bnds(lon, bnds)float64dask.array&lt;chunksize=(320, 2), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n5.00 kiB\n5.00 kiB\n\n\nShape\n(320, 2)\n(320, 2)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         2 320\n\n\n\n\ntime_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n16.12 kiB\n16 B\n\n\nShape\n(1032, 2)\n(1, 2)\n\n\nDask graph\n1032 chunks in 2 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n                           2 1032\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398,\n       ...\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152],\n      dtype='float64', name='lat', length=160))lonPandasIndexPandasIndex(Index([    0.0,   1.125,    2.25,   3.375,     4.5,   5.625,    6.75,   7.875,\n           9.0,  10.125,\n       ...\n        348.75, 349.875,   351.0, 352.125,  353.25, 354.375,   355.5, 356.625,\n        357.75, 358.875],\n      dtype='float64', name='lon', length=320))timePandasIndexPandasIndex(DatetimeIndex(['2015-01-16 12:00:00', '2015-02-15 00:00:00',\n               '2015-03-16 12:00:00', '2015-04-16 00:00:00',\n               '2015-05-16 12:00:00', '2015-06-16 00:00:00',\n               '2015-07-16 12:00:00', '2015-08-16 12:00:00',\n               '2015-09-16 00:00:00', '2015-10-16 12:00:00',\n               ...\n               '2100-03-16 12:00:00', '2100-04-16 00:00:00',\n               '2100-05-16 12:00:00', '2100-06-16 00:00:00',\n               '2100-07-16 12:00:00', '2100-08-16 12:00:00',\n               '2100-09-16 00:00:00', '2100-10-16 12:00:00',\n               '2100-11-16 00:00:00', '2100-12-16 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=1032, freq=None))Attributes: (44)Conventions :CF-1.7 CMIP-6.2activity_id :ScenarioMIPbranch_method :standardbranch_time_in_child :60265.0branch_time_in_parent :60265.0cmor_version :3.4.0creation_date :2019-02-20T14:25:47Zdata_specs_version :01.00.29experiment :low-end scenario reaching 1.9 W m-2, based on SSP1experiment_id :ssp119external_variables :areacellaforcing_index :1frequency :monfurther_info_url :https://furtherinfo.es-doc.org/CMIP6.MRI.MRI-ESM2-0.ssp119.none.r1i1p1f1grid :native atmosphere TL159 gaussian grid (160x320 latxlon)grid_label :gnhistory :2019-02-20T14:25:47Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.;\nOutput from run-Dr064_ssp119_145 (sfc_avr_mon.ctl)initialization_index :1institution :Meteorological Research Institute, Tsukuba, Ibaraki 305-0052, Japaninstitution_id :MRIlicense :CMIP6 model data produced by MRI is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.mip_era :CMIP6nominal_resolution :100 kmparent_activity_id :CMIPparent_experiment_id :historicalparent_mip_era :CMIP6parent_source_id :MRI-ESM2-0parent_time_units :days since 1850-01-01parent_variant_label :r1i1p1f1physics_index :1product :model-outputrealization_index :1realm :atmossource :MRI-ESM2.0 (2017): \naerosol: MASINGAR mk2r4 (TL95; 192 x 96 longitude/latitude; 80 levels; top level 0.01 hPa)\natmos: MRI-AGCM3.5 (TL159; 320 x 160 longitude/latitude; 80 levels; top level 0.01 hPa)\natmosChem: MRI-CCM2.1 (T42; 128 x 64 longitude/latitude; 80 levels; top level 0.01 hPa)\nland: HAL 1.0\nlandIce: none\nocean: MRI.COM4.4 (tripolar primarily 0.5 deg latitude/1 deg longitude with meridional refinement down to 0.3 deg within 10 degrees north and south of the equator; 360 x 364 longitude/latitude; 61 levels; top grid cell 0-2 m)\nocnBgchem: MRI.COM4.4\nseaIce: MRI.COM4.4source_id :MRI-ESM2-0source_type :AOGCM AER CHEMsub_experiment :nonesub_experiment_id :nonetable_id :Amontable_info :Creation Date:(14 December 2018) MD5:b2d32d1a0d9b196411429c8895329d8ftitle :MRI-ESM2-0 output prepared for CMIP6tracking_id :hdl:21.14100/3fef816c-0095-4950-a45a-ba080ec2b7ebvariable_id :cltvariant_label :r1i1p1f1\n\n\nApplying our data slices and analyses can be performed in a few short steps that takes very little time thanks to the lazily loaded dataset.\n\nds.clt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'clt' (time: 1032, lat: 160, lon: 320)&gt; Size: 211MB\ndask.array&lt;open_dataset-clt, shape=(1032, 160, 320), dtype=float32, chunksize=(1, 160, 320), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float64 1kB -89.14 -88.03 -86.91 -85.79 ... 86.91 88.03 89.14\n  * lon      (lon) float64 3kB 0.0 1.125 2.25 3.375 ... 355.5 356.6 357.8 358.9\n  * time     (time) datetime64[ns] 8kB 2015-01-16T12:00:00 ... 2100-12-16T12:...\nAttributes:\n    cell_measures:  area: areacella\n    cell_methods:   area: time: mean\n    comment:        Total cloud area fraction for the whole atmospheric colum...\n    history:        2019-02-20T14:25:47Z altered by CMOR: replaced missing va...\n    long_name:      Total Cloud Cover Percentage\n    original_name:  TCLOUD\n    standard_name:  cloud_area_fraction\n    units:          %xarray.DataArray'clt'time: 1032lat: 160lon: 320dask.array&lt;chunksize=(1, 160, 320), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n201.56 MiB\n200.00 kiB\n\n\nShape\n(1032, 160, 320)\n(1, 160, 320)\n\n\nDask graph\n1032 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             320 160 1032\n\n\n\n\nCoordinates: (3)lat(lat)float64-89.14 -88.03 ... 88.03 89.14axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_northarray([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398, -77.94262, -76.82124,\n       -75.69984, -74.57843, -73.45701, -72.33558, -71.21414, -70.09269,\n       -68.97124, -67.84978, -66.72833, -65.60686, -64.4854 , -63.36393,\n       -62.24246, -61.12099, -59.99952, -58.87804, -57.75657, -56.63509,\n       -55.51361, -54.39214, -53.27066, -52.14917, -51.02769, -49.90621,\n       -48.78473, -47.66325, -46.54176, -45.42028, -44.29879, -43.17731,\n       -42.05582, -40.93434, -39.81285, -38.69137, -37.56988, -36.44839,\n       -35.32691, -34.20542, -33.08393, -31.96244, -30.84096, -29.71947,\n       -28.59798, -27.47649, -26.355  , -25.23351, -24.11203, -22.99054,\n       -21.86905, -20.74756, -19.62607, -18.50458, -17.38309, -16.2616 ,\n       -15.14011, -14.01862, -12.89713, -11.77564, -10.65415,  -9.53266,\n        -8.41117,  -7.28968,  -6.16819,  -5.0467 ,  -3.92521,  -2.80372,\n        -1.68223,  -0.56074,   0.56074,   1.68223,   2.80372,   3.92521,\n         5.0467 ,   6.16819,   7.28968,   8.41117,   9.53266,  10.65415,\n        11.77564,  12.89713,  14.01862,  15.14011,  16.2616 ,  17.38309,\n        18.50458,  19.62607,  20.74756,  21.86905,  22.99054,  24.11203,\n        25.23351,  26.355  ,  27.47649,  28.59798,  29.71947,  30.84096,\n        31.96244,  33.08393,  34.20542,  35.32691,  36.44839,  37.56988,\n        38.69137,  39.81285,  40.93434,  42.05582,  43.17731,  44.29879,\n        45.42028,  46.54176,  47.66325,  48.78473,  49.90621,  51.02769,\n        52.14917,  53.27066,  54.39214,  55.51361,  56.63509,  57.75657,\n        58.87804,  59.99952,  61.12099,  62.24246,  63.36393,  64.4854 ,\n        65.60686,  66.72833,  67.84978,  68.97124,  70.09269,  71.21414,\n        72.33558,  73.45701,  74.57843,  75.69984,  76.82124,  77.94262,\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152])lon(lon)float640.0 1.125 2.25 ... 357.8 358.9axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_eastarray([  0.   ,   1.125,   2.25 , ..., 356.625, 357.75 , 358.875])time(time)datetime64[ns]2015-01-16T12:00:00 ... 2100-12-...axis :Tbounds :time_bndslong_name :timestandard_name :timearray(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', ..., '2100-10-16T12:00:00.000000000',\n       '2100-11-16T00:00:00.000000000', '2100-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398,\n       ...\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152],\n      dtype='float64', name='lat', length=160))lonPandasIndexPandasIndex(Index([    0.0,   1.125,    2.25,   3.375,     4.5,   5.625,    6.75,   7.875,\n           9.0,  10.125,\n       ...\n        348.75, 349.875,   351.0, 352.125,  353.25, 354.375,   355.5, 356.625,\n        357.75, 358.875],\n      dtype='float64', name='lon', length=320))timePandasIndexPandasIndex(DatetimeIndex(['2015-01-16 12:00:00', '2015-02-15 00:00:00',\n               '2015-03-16 12:00:00', '2015-04-16 00:00:00',\n               '2015-05-16 12:00:00', '2015-06-16 00:00:00',\n               '2015-07-16 12:00:00', '2015-08-16 12:00:00',\n               '2015-09-16 00:00:00', '2015-10-16 12:00:00',\n               ...\n               '2100-03-16 12:00:00', '2100-04-16 00:00:00',\n               '2100-05-16 12:00:00', '2100-06-16 00:00:00',\n               '2100-07-16 12:00:00', '2100-08-16 12:00:00',\n               '2100-09-16 00:00:00', '2100-10-16 12:00:00',\n               '2100-11-16 00:00:00', '2100-12-16 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=1032, freq=None))Attributes: (8)cell_measures :area: areacellacell_methods :area: time: meancomment :Total cloud area fraction for the whole atmospheric column, as seen from the surface or the top of the atmosphere. Includes both large-scale and convective cloud.history :2019-02-20T14:25:47Z altered by CMOR: replaced missing value flag (-9.99e+33) with standard missing value (1e+20).long_name :Total Cloud Cover Percentageoriginal_name :TCLOUDstandard_name :cloud_area_fractionunits :%\n\n\n\n%%time\ntime_series = ds.time.dt.year.isin([2015+i for i in range(5)])\noneframe = ds.clt.isel(time=time_series).sel(lon=slice(220,340), lat=slice(-60,60)).mean(dim='time')\n\nCPU times: user 15.4 ms, sys: 2.15 ms, total: 17.5 ms\nWall time: 18.8 ms\n\n\nBasic plot of the Americas - average cloud cover percentage over a 5 year period (2015-2020)\n\n# Generate a plain image to show\nplt.imshow(oneframe.compute(), origin='lower',cmap='Blues') # We need the 'lower' option to flip the image because the latitude coordinates for this dataset start from -90.\nplt.colorbar(label=f'{ds.clt.long_name} ({ds.clt.units})')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\nText(0, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\nThe above image well represents the underlying data in terms of arrangement and resolution, and we can even see the outlines of continents where the land-sea border has an effect on the cloud cover percentage (clt).\nIf we want to have the coastlines added, plus a different projection method for the map, we can use the Geoviews package to arrange the data in this new way.\nNote: Using different projections may add some apparent artifacts to the data, as we can see below. This is probably due to the data conversion to Geoviews and is a good example of why using multiple plots is a valuable filter for unexpected results.\n\n# generate the image to show\nfigdata = gv.Dataset(oneframe.compute(), [\"lon\", \"lat\"], crs=crs.PlateCarree())\nimages = figdata.to(gv.Image)\n\n# Plot\nimages.opts(cmap=\"viridis\", colorbar=True, width=600, height=500) * gf.coastline"
  },
  {
    "objectID": "science/3_Use_Cases.html#plot-some-assembled-data",
    "href": "science/3_Use_Cases.html#plot-some-assembled-data",
    "title": "Use Cases for Climate Datasets",
    "section": "3. Plot some assembled data",
    "text": "3. Plot some assembled data\nHere we plot an example set of data for the years 2251 to 2311 within this model forecase for the Precipitation Flux (pr) variable. We are specifically looking at the mean rainfall in the summer months June-August across the globe. We can select these data from the Xarray dataset, combine them into an array of mean values and plot\n\n# Define our filter based on the desired months.\nsummer = data.time.dt.month.isin([6,7,8])\n\nyears  = []\nvalues = []\nfor i in range(60): # Arbitrary 60-year time-series.\n\n    # Define a filter for each year and use to calculate the mean value.\n    yr_filter = data.time.dt.year.isin([2251+i])\n    value = data.pr.isel(time=summer&yr_filter).mean().compute()\n\n    # Concatenate values\n    values.append(value)\n    years.append(2251+i)\n\nNote: When plotting data in this fashion, it is best to retrieve all the data you’re definitely going to need, using the .compute() function. This should only be done at the point where all the data will be required so you can take advantage of the lazy loading as far as possible, but by fetching the data slightly sooner, you are avoiding the potential for duplicate retrievals. In our example above, if the data chunks each contain multiple years, then our data requests in each loop will likely try to fetch the same data multiple times over the network.\nWe can already guess the expected results but we can see it clearly from the values parameter: There isn’t a lot of variation in Precipitation for this set of time periods expected within this data, the average is fairly constant.\n\nnp.array(values)\n\narray([3.2855816e-05, 3.2685162e-05, 3.3101434e-05, 3.2580436e-05,\n       3.3071603e-05, 3.2749525e-05, 3.2923213e-05, 3.2849730e-05,\n       3.2942939e-05, 3.2836724e-05, 3.2885007e-05, 3.3062988e-05,\n       3.2747223e-05, 3.2915803e-05, 3.2871350e-05, 3.2934004e-05,\n       3.2730630e-05, 3.2895732e-05, 3.2411441e-05, 3.2801654e-05,\n       3.2587293e-05, 3.3185788e-05, 3.2788066e-05, 3.3151795e-05,\n       3.3164430e-05, 3.2921696e-05, 3.3123262e-05, 3.2765842e-05,\n       3.2568005e-05, 3.2715379e-05, 3.3246262e-05, 3.2860520e-05,\n       3.2898555e-05, 3.3144406e-05, 3.2595581e-05, 3.2835953e-05,\n       3.2745345e-05, 3.2944168e-05, 3.2687018e-05, 3.2905038e-05,\n       3.2768705e-05, 3.2982960e-05, 3.2923665e-05, 3.2810131e-05,\n       3.2835127e-05, 3.3216886e-05, 3.2803429e-05, 3.2939348e-05,\n       3.2507061e-05, 3.2745895e-05,           nan,           nan,\n                 nan,           nan,           nan,           nan,\n                 nan,           nan,           nan,           nan],\n      dtype=float32)\n\n\nFinally we can plot this data as a simple scatter plot.\n\n%matplotlib inline\n\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.ylim(0,4e-5)\nplt.ylabel(f'{data.pr.long_name} JJA Avg ({data.pr.units})')\nplt.xlabel('Years')\nplt.scatter(np.array(years),np.array(values))"
  },
  {
    "objectID": "science/3_Use_Cases.html#animations-across-a-time-series",
    "href": "science/3_Use_Cases.html#animations-across-a-time-series",
    "title": "Use Cases for Climate Datasets",
    "section": "4. Animations across a time-series",
    "text": "4. Animations across a time-series\nIt would be useful if a user was able to move through the data and understand the changes in the supplied data as the time periods change. This is where Geoviews really helps.\n\n%%time\n# Load the data selection in Xarray\ndata2 = data.isel(time=slice(0, 50)).sel(lat=slice(-30,30)).compute()\n\nCPU times: user 552 ms, sys: 89.7 ms, total: 642 ms\nWall time: 499 ms\n\n\nNote: It is best to load the data into geoviews at this point (losing the lazy loading) because the output generation takes considerably longer than the data loading, and this causes more frequent connection issues when awaiting http-requested chunks.\nIn this case, because we are using a Cloud-based dataset which retrieves data by request, it is best to load our data into an array at this stage, rather than pass the unloaded dataset to Geoviews. This notebook has been written in this way because it was noted that Geoviews is more prone to request-based failures to fetch data, most likely because in converting the data it attempts to access data selections multiple times, which triggers multiple requests for the same portion of data.\n\n%%time\n# Create a Geoviews dataset and convert to the required format for visualisation\ndataset = gv.Dataset(data2)\nensemble = dataset.to(gv.Image, [\"lon\", \"lat\"], \"pr\")\n\nCPU times: user 221 ms, sys: 2.56 ms, total: 223 ms\nWall time: 222 ms\n\n\nWe can see the above two sections - where we have retrieved the data and assembled a Geoviews dataset - have short durations in terms of computation time. The cell below generates the interactive animation, which even for 50 selections of pre-loaded data takes significantly longer to assemble.\n\n%%time\n# Generate the interactive annimation\ngv.output(\n    ensemble.opts(cmap=\"viridis\", colorbar=True, fig_size=120, backend=\"matplotlib\")\n    * gf.coastline(),\n    backend=\"matplotlib\",\n    widget_location=\"top\",\n    max_frames=200,\n)\n\n\n\n\n\n  \n\n\n\n\nCPU times: user 3min 2s, sys: 53.1 s, total: 3min 55s\nWall time: 2min 30s\n\n\nUse the time slider above the figure to step through the spatial dataset held within this dataset. The Geoviews package and Holoviews suite of tools are very powerful when building interactive components to help scientists and users understand these complex datasets. If the colormap doesn’t work for you alternatibves can be found here.\n\n# # Close the dataset to free resources\ndata.close()"
  },
  {
    "objectID": "workflows/2_EOAP.html",
    "href": "workflows/2_EOAP.html",
    "title": "EOAPs",
    "section": "",
    "text": "Description & purpose: This Notebook introduces the concept of Earth Observation Application Packages (EOAPs).\nAuthor(s): Alastair Graham\nDate created: 2024-11-08\nDate last modified: 2024-12-19\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\n Copyright (c) , All rights reserved.\n Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nWhat are Earth Observation Application Packages?\nEOAPs are self contained amalgamations of tools and code that allow an algorithm, or set of algorithms, to be shared between and run on different data processing platforms. The core concepts behind this are that: * Algorithms and processing methods are not tied to a specific platform * The underlying infrastructure is not important as long as a suitable workflow runner exists on that infrastructure * The workflows preferentially create data services\nThe EOAP is the complete unit of code that is submitted to the workflow runner and comprises: * A CWL workflow file, itself made up of a descriptive component and a series of command line tools * A Dockerfile or Docker image * An optional application file that is usually required for any but the most simple workflows\nA functioning and stable EOAP is challenging to create even for experienced developers and generally the creation of EOAPs is limited to specialist users with some knowledge of dev-ops or with higher programming skills.\n\n\nHow do I create a workflow?\nThe guidance around EOAPs is that handcrafting a package is possible but not recommended. These workflows can become very complex very quickly and it is recommended that some form of tooling is used to automate or semi-automate the generation of an EOAP. As part of the Pathfinder phase of the project, the EODH (via a 3rd party contractor, Oxidian) has created eoap-gen to help specialist technicians to create service workflows.\n\n\nOther information\nAdditional information on the EOAP specification and construction is available here: https://eoap.github.io/mastering-app-package/. This is a comprehensive tutorial that introduces the user to different tools (some of which are mentioned elsewhere in these training materials). Much of the tutorial uses hand-crafted code which is generally not recommended as a method of generation (see notes on the EOAP Generator for further details) but in this case it does enable detailed assesment of the different components.\nA full set of resources are available from the official EOAP Github repository: https://github.com/eoap.\nFurther to this, a related ‘hands on’ tutorial for the creation and use of an EOAp is available here.\n\n\nLinks\nThe following bullet points direct the reader at a set of further reading around EOAPs and the standards they relate to.\n\nEO Application Package example: Waterbodies\nOGC best practices: EO Application packages\nCWL for EO tutorials: Adding conditions\nTerradue training: hands-on (there do seem to be some typographic errors in the scripts)\nEO Application Packages: overview, examples",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "EO App Packages"
    ]
  },
  {
    "objectID": "workflows/4_UserExample1.html",
    "href": "workflows/4_UserExample1.html",
    "title": "User Example: SAR Coherence",
    "section": "",
    "text": "Description & purpose: This Notebook outlines a use case for generating Sentinel 1 SAR (Synthetic Aperture Radar) coherence imagery. It walks the user through the science and outlines how the eoap-gen tool has been used to creat a workflow that is usable on the EODH.\nAuthor(s): Alastair Graham, Dusan Figala\nDate created: 2024-11-08\nDate last modified: 2024-11-08\nLicence: This file is licensed under Creative Commons Attribution-ShareAlike 4.0 International. Any included code is released using the BSD-2-Clause license.\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nWIP Icon Attribution",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: coherence"
    ]
  },
  {
    "objectID": "workflows/4_UserExample1.html#scientific-method",
    "href": "workflows/4_UserExample1.html#scientific-method",
    "title": "User Example: SAR Coherence",
    "section": "Scientific method",
    "text": "Scientific method\nWe start with searching the data on Copernicus Data Space Ecosystem.\nNeed to create an access point to do the search.\n\ndag = EODataAccessGateway()\n# make sure cop_dataspace will be used\ndag.set_preferred_provider(\"cop_dataspace\")\nlogging.basicConfig(level=logging.INFO)\n\nINFO:eodag.config:Loading user configuration from: /home/al/.config/eodag/eodag.yml\nINFO:eodag.core:usgs: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:aws_eos: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:meteoblue: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:hydroweb_next: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:wekeo: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:creodias_s3: provider needing auth for search has been pruned because no crendentials could be found\nINFO:eodag.core:Locations configuration loaded from /home/al/.config/eodag/locations.yml\n\n\n\n# Run search using the bbox set earlier\n\nsearch_criteria = {\n    \"productType\": \"S1_SAR_SLC\",\n    \"start\": \"2023-09-03\",\n    \"end\": \"2023-09-17\",\n    \"geom\": shp,\n}\n\nresults, _ = dag.search(**search_criteria)\n\nINFO:eodag.core:Searching product type 'S1_SAR_SLC' on provider: cop_dataspace\nINFO:eodag.search.qssearch:Sending search request: http://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2023-09-03&completionDate=2023-09-17&geometry=POLYGON ((0.1085 52.5485, 0.6857 52.5580, 0.6778 52.4539, 0.1057 52.4464, 0.1085 52.5485))&productType=SLC&maxRecords=20&page=1&exactCount=1\nINFO:eodag.core:Found 5 result(s) on provider 'cop_dataspace'\n\n\nIf we want to we can display all of the returned image file outlines on a map\n\neo.util.explore_products(results, shp)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nTo be added * Using pyeodh to find the images\nWe are only interested in image pairs. The S1 SLC file name doesn’t make it clear which these are so the following code finds image footprints with 98% overlap\n\n# Find overlaps\ndata = []\nfor item in results:\n    id = item.properties[\"id\"]\n    geom = shape(item.geometry)\n    data.append({\"id\": id, \"geometry\": geom})\n\ngdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")  # Assuming WGS84\n\n# 98% overlap\nthreshold = 0.98\n\noverlaps = []\nfor (idx1, row1), (idx2, row2) in combinations(gdf.iterrows(), 2):\n    intersection = row1[\"geometry\"].intersection(row2[\"geometry\"])\n    if not intersection.is_empty:\n        # Calculate overlap ratio as the area of intersection divided by the area of the smaller polygon\n        overlap_ratio = intersection.area / min(\n            row1[\"geometry\"].area, row2[\"geometry\"].area\n        )\n        if overlap_ratio &gt;= threshold:\n            overlaps.append((row1[\"id\"], row2[\"id\"], overlap_ratio))\n\noverlap_ids = [entry[:-1] for entry in overlaps]\noverlap_ids\n\n[('S1A_IW_SLC__1SDV_20230904T174209_20230904T174236_050181_060A2D_E88F',\n  'S1A_IW_SLC__1SDV_20230916T174209_20230916T174236_050356_061016_8033')]\n\n\nThe data files are large (8GB). when running this locally use a tool such as wget to download the Sentinel 1 image pair that you want\n\n# Set download dirs\nids = [\n    \"S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C\",\n    \"S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046\"\n]\n    #\"S1A_IW_SLC__1SDV_20230904T174209_20230904T174236_050181_060A2D_E88F\",\n    #\"S1A_IW_SLC__1SDV_20230916T174209_20230916T174236_050356_061016_8033\",\n#]\n\nprimary_dir = f\"{data_dir}/{ids[0]}.zip\"\nsecondary_dir = f\"{data_dir}/{ids[1]}.zip\"\noutputs_prefix = f\"{data_dir}/res/test-full-processor\"\n\nThe next cell runs eo-tools on the SAR SLC data pair. There are a multitude of configuration parameters but the most important one for this application is to ensure that write_coherence is set to True. This will generate an image of coherence in the project path set earlier in the Notebook. To effectively set the remainder of the parameters you will need some level of competence in SAR interferometry. For the purposes of this demonstration we leave them on the default values.\n\nout_dir = process_insar(\n    dir_prm=primary_dir,\n    dir_sec=secondary_dir,\n    outputs_prefix=outputs_prefix,\n    aoi_name=None,\n    shp=shp,\n    pol=\"vv\",\n    subswaths=[\"IW1\", \"IW2\", \"IW3\"],\n    write_coherence=True,\n    write_interferogram=True,\n    write_primary_amplitude=False,\n    write_secondary_amplitude=False,\n    apply_fast_esd=True,\n    dem_upsampling=1.8,\n    dem_force_download=False,\n    dem_buffer_arc_sec=40,\n    boxcar_coherence=[3, 3],\n    filter_ifg=True,\n    multilook=[1, 4],\n    warp_kernel=\"bicubic\",\n    clip_to_shape=True,\n)\n\nINFO:eo_tools.S1.process:---- Processing subswath IW1 in VV polarization\nINFO:eo_tools.S1.core:S1IWSwath Initialization:\nINFO:eo_tools.S1.core:- Read metadata file /home/al/Downloads/eotools/S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C.zip/S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C.SAFE/annotation/s1a-iw1-slc-vv-20241016t174207-20241016t174232-056131-06de72-004.xml\nINFO:eo_tools.S1.core:- Read calibration file /home/al/Downloads/eotools/S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C.zip/S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C.SAFE/annotation/calibration/calibration-s1a-iw1-slc-vv-20241016t174207-20241016t174232-056131-06de72-004.xml\nINFO:eo_tools.S1.core:- Set up raster path zip:///home/al/Downloads/eotools/S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C.zip/S1A_IW_SLC__1SDV_20241016T174206_20241016T174233_056131_06DE72_1B0C.SAFE/measurement/s1a-iw1-slc-vv-20241016t174207-20241016t174232-056131-06de72-004.tiff\nINFO:eo_tools.S1.core:- Look for available OSV (Orbit State Vectors)\nINFO:eo_tools.S1.core:-- Precise orbit found\nINFO:eo_tools.S1.core:S1IWSwath Initialization:\nINFO:eo_tools.S1.core:- Read metadata file /home/al/Downloads/eotools/S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046.zip/S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046.SAFE/annotation/s1a-iw1-slc-vv-20241028t174207-20241028t174232-056306-06e564-004.xml\nINFO:eo_tools.S1.core:- Read calibration file /home/al/Downloads/eotools/S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046.zip/S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046.SAFE/annotation/calibration/calibration-s1a-iw1-slc-vv-20241028t174207-20241028t174232-056306-06e564-004.xml\nINFO:eo_tools.S1.core:- Set up raster path zip:///home/al/Downloads/eotools/S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046.zip/S1A_IW_SLC__1SDV_20241028T174206_20241028T174233_056306_06E564_F046.SAFE/measurement/s1a-iw1-slc-vv-20241028t174207-20241028t174232-056306-06e564-004.tiff\nINFO:eo_tools.S1.core:- Look for available OSV (Orbit State Vectors)\nINFO:eo_tools.S1.core:-- Precise orbit found\nINFO:eo_tools.S1.core:--DEM already on disk\nINFO:eo_tools.S1.process:---- Processing burst 3 ----\nINFO:eo_tools.S1.core:Extract DEM coordinates\nINFO:eo_tools.S1.core:Convert latitude, longitude & altitude to ECEF x, y & z\nINFO:eo_tools.S1.core:Interpolate orbit\nINFO:eo_tools.S1.core:Range-Doppler terrain correction (LUT computation)\nINFO:eo_tools.S1.core:Extract DEM coordinates\nINFO:eo_tools.S1.core:Convert latitude, longitude & altitude to ECEF x, y & z\nINFO:eo_tools.S1.core:Interpolate orbit\nINFO:eo_tools.S1.core:Range-Doppler terrain correction (LUT computation)\nINFO:eo_tools.S1.core:Compute beta nought calibration factor.\nINFO:eo_tools.S1.core:Compute beta nought calibration factor.\nINFO:eo_tools.S1.process:Apply calibration factor\nINFO:eo_tools.S1.core:Compute TOPS deramping phase\nINFO:eo_tools.S1.process:Apply phase deramping\nINFO:eo_tools.S1.core:Project secondary coordinates to primary grid.\nINFO:eo_tools.S1.core:Warp secondary to primary geometry.\nINFO:eo_tools.S1.core:Warp secondary to primary geometry.\nINFO:eo_tools.S1.process:Apply phase reramping\nINFO:eo_tools.S1.core:Compute topographic phase\nINFO:eo_tools.S1.core:Compute topographic phase\nINFO:eo_tools.S1.process:Apply topographic phase removal\nINFO:eo_tools.auxils:Removing /home/al/Downloads/eotools/res/test-full-processor/S1_InSAR_2024-10-16-174206__2024-10-28-174206/sar/dem_burst.vrt\nINFO:eo_tools.S1.process:Cleaning temporary files\nINFO:eo_tools.S1.process:---- Interferometric outputs for VV IW1\nINFO:eo_tools.S1.process:Compute coherence & interferogram\nINFO:eo_tools.S1.process:---- Interferometric outputs for VV IW2\nINFO:eo_tools.S1.process:Compute coherence & interferogram\nINFO:eo_tools.S1.process:Geocode file coh_vv_iw1.tif.\nINFO:eo_tools.S1.process:Project image with the lookup table.\nINFO:eo_tools.S1.process:Geocode file coh_vv_iw2.tif.\nINFO:eo_tools.S1.process:Project image with the lookup table.\nINFO:eo_tools.S1.process:Merge file coh_vv.tif\nINFO:eo_tools.auxils:Removing /home/al/Downloads/eotools/res/test-full-processor/S1_InSAR_2024-10-16-174206__2024-10-28-174206/sar/coh_vv_iw1_geo.tif\nINFO:eo_tools.auxils:Removing /home/al/Downloads/eotools/res/test-full-processor/S1_InSAR_2024-10-16-174206__2024-10-28-174206/sar/coh_vv_iw2_geo.tif\nINFO:eo_tools.S1.process:Geocode file ifg_vv_iw1.tif.\nINFO:eo_tools.S1.process:Project image with the lookup table.\nINFO:eo_tools.S1.process:Geocode file ifg_vv_iw2.tif.\nINFO:eo_tools.S1.process:Project image with the lookup table.\nINFO:eo_tools.S1.process:Merge file phi_vv.tif\nINFO:eo_tools.auxils:Removing /home/al/Downloads/eotools/res/test-full-processor/S1_InSAR_2024-10-16-174206__2024-10-28-174206/sar/phi_vv_iw1_geo.tif\nINFO:eo_tools.auxils:Removing /home/al/Downloads/eotools/res/test-full-processor/S1_InSAR_2024-10-16-174206__2024-10-28-174206/sar/phi_vv_iw2_geo.tif",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: coherence"
    ]
  },
  {
    "objectID": "workflows/4_UserExample1.html#outputs",
    "href": "workflows/4_UserExample1.html#outputs",
    "title": "User Example: SAR Coherence",
    "section": "Outputs",
    "text": "Outputs\nThe following figures show, in order: * The AOI * This covers an area between the Fenland town of March to the west and Thetford Forest to the east. The 100 Foot Drain complex is covered in the western portion of the AOI * The full coherence image * Low coherence values are represented in dark blue and high values in yellow, with a gradient between them. * A portion of the coherence image near the town of March\n\n\n\nAOI\n\n\n\n\n\nCoherence\n\n\n\n\n\nClose up of coherence",
    "crumbs": [
      "Creating Workflows - eoap-gen",
      "User Example: coherence"
    ]
  }
]